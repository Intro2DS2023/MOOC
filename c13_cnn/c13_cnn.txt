=== 1. מה היה לפני דיפ לרנינג לתמונות? ===

ביחידה זו נלמד על רשתות קונבולוציה שמתאימות במיוחד לבניית מודלים לחיזוי על תמונות. רשתות קונבולוציה הן כנראה המודל המודרני ביותר שנלמד בקורס זה, והן הבסיס להבנה של מודלים מתחום הראייה הממוחשבת. מודלים שמאפשרים למכוניות לנסוע ללא נהג, וללקוחות לצאת מסופר מבלי לעבור בקופה.

:::

אבל זה לא שרשתות קונבולוציה צמחו בואקום. חשוב לפחות להבין מה היה לפניהן.

:::

ניזכר מהו הייצוג של תמונה אצלנו במחשב. כאן אני קורא את הלוגו של הקורס עם המודול image של ספריית matplotlib.

מה קיבלתי?

:::

קיבלתי מערך של numpy array.

המימדים שלו הם 1400 על 1400 על 3 שכבות צבע: אדום, ירוק וכחול, או RGB.

אם ניקח את השכבה הראשונה ונבקש לראות איזשהו ריבוע קטן במרכז התמונה, נראה פשוט מספרים, כלומר התמונה שלנו היא מערך תלת מימדי של מספרים שלמים.

המספרים נעים מ0 עד 255, כלומר 256 אפשרויות לכל פיקסל בכל שכבת צבע, כאן אין פיקסלים עם רמת צבע פחות מ8.

כשאני מבקש את האטריביוט דיטייפ של המערך הוא uint8, כלומר כל מספר מיוצג על-ידי בייט אחד. האטריביוט size מחזיר את מספר המספרים במערך והאטריביוט itemsize מחזיר את מספר הבייטים לכל מספר. יש לנו 1400 כפול 1400 כפול 3 מספרים, כפול בייט 1. כלומר התמונה שלנו צורכת בזיכרון כמעט 6 מגה-בייט.

:::

אז אם נתייחס לכל פיקסל ופיקסל כפיצ'ר או משתנה, כמה משתנים יהיו לנו? כמעט 6 מיליון!

אז רשתות קונבולוציה החלו בערך ב-1998 כאשר יאן לקון הציג את הlenet-5 לזיהוי אוטומטי של ספרות. אתם יכולים לחפש ביוטיוב סרטים ישנים שלו מציג במעבדה על מחשב ישן את הפלא הזה. וב2012 הגיע מה שמכונה רגע האימג'נט, שבו אלכס קריז'בסקי, איליה סוצקבר וג'פרי הינטון פרסמו רשת בשם אלכסנט והביאו ליכולות חיזוי שטרם נראו, בתחרות חיזוי מפורסמת על מאגר תמונות שנקרא imagenet. האתגר היה פשוט: יש מיליון תמונות ששייכות לאלף קטגוריות שונות. האם אתם יכולים לבנות מודל שיסווג נכון תמונות שהוא לא ראה?

אבל עד אז, התייחסו לתמונה כעוד בעיה עם מימד מאוד גבוה, כמו 6 מיליון משתנים. והתמקדו ברעיונות שונים לפיצ'רים על סמך הבנה של הפיסיקה של העצם הזה, שנקרא תמונה. לדוגמא פיצ'רים שמסתכלים על ההיסטוגרמה של כמות האדום בתמונה, ומתארים את הממוצע, החציון, והסקיונס שלה, או כל רעיון יצירתי אחר שעלה במוחם של החוקרים, ועל זה לעשות את המודלים שלמדנו, לדוגמא בוסטינג.

=== 2. שכבת קונבולוציה ===

אז מהי שכבת הקונבולוציה שעשתה כזאת מהפכה?

:::

קונבולוציה היא לא דבר חדש. קונבולוציה היא מושג ידוע ממתמטיקה. מכפלה של שתי פונקציות f ו-g, ולקיחת אינטגרל או סיכום של ערך המכפלה על פני טווח מסוים.

כאן אנחנו רואים קונבולוציה בין פונקציה g באדום לבין פונקציה f בכחול. תוצאת הקונבולוציה מסומנת בשחור, ואפשר לראות שהיא פונקציה נוספת של פרמטר t. אנחנו מזיזים את g כמו חלון נע, ובכל נקודה בודקים מהו סך שטח החפיפה בינה לבין f, או אינטגרל, ומבטאים את השטח באמצעות פונקציה חדשה.

במובנים מסוימים הפונקציה החדשה מאוד מזכירה קורלציה בין שתי הפונקציות, איזושהי פונקצית מתאם שבודקת עד כמה f דומה לg. אם למשל f היתה זהה לריבוע של g באיזו נקודה, הן היו חופפות לחלוטין והאינטגרל היה 1, שטח ריבוע. כאן זה לא קורה.

זוכרים איפה ראינו כבר קונבולוציה? כשדיברנו על kernel density estimation, ואיך יוצרים תרשים צפיפות של התפלגות, שם קראנו לחלון הנע הזה w(x).

בכל מקרה לא מוכרחים לדעת מהי קונבולוציה כדי להבין איך שכבת קונבולוציה עובדת, זה פשוט מדהים לראות איך הכרה של מושג מתחום המתמטיקה סייעה כל כך לעשות מהפכה בתחום הראייה הממוחשבת.

:::

אז מהי שכבת קונבולוציה?

שכבת הקונבולוציה הראשונה מורכבת מ-K על K נוירונים, שלכל אחד מהם יש "שדה רצפטיבי". הוא לא "מסתכל" על כל הפיקסלים בתמונה שמתחתיו. הוא מסתכל רק על ריבוע של 2X2 או 3X3.

בשכבה הבאה, כל נוירון ונוירון מסתכל על השכבה שמתחתיו, שאפשר לראות בה תמונה חדשה, גם כאן הוא לא מסתכל על כל התמונה, אלא רק על שדה רצפטיבי, וכך בשכבת הקונבולוציה הבאה, והבאה וכולי.

:::

אם רוצים להגדיר את זה יותר מדויק, אפשר לחשוב על פרמטרים f_h ו-f_w שמגדירים את גודל השדה הרצפטיבי. ואז נוירון i, j בשכבת קונבולוציה יסתכל על המלבן שנוצר משורות i עד f_h - 1, ומעמודות j עד f_w - 1.

כאן אפשר לראות למשל את הריבוע שהנוירון האדום מסתכל עליו בשכבה שמתחתיו. ואז נצעד צעד ימינה ונראה את הריבוע שהנוירון הכחול מסתכל עליו בשכבה שמתחתיו. באופן כזה נסרקת כל השכבה, אבל במעין חלון נע כזה שמתפקס כל פעם על פאץ' אחר.

עכשיו חדי העין מביניכם ודאי הבינו שאם לא יהיה שום שינוי שכבת הנוירונים מעל התמונה כבר לא תוכל להישאר בדיוק באותו גודל. באופן ספציפי אם השדה הרצפטיבי הוא כמו כאן 3 על 3, השכבה או התמונה הבאה תהיה "חסרה" פיקסל אחד מכל צד. וכדי שזה לא יקרה, נהוג לעשות גם zero padding, כלומר לעבות את התמונה, את הקלט, מסביב עם אפסים, כמה שצריך. כאן למשל צריך לעבות את התמונה עם שכבה אחת של אפסים.

:::

אבל מה זה כל נוירון "מסתכל", מה הוא עושה עם השדה הרצפטיבי הזה?

כל הנוירונים בשכבה לומדים איזשהו פילטר, או פיצ'ר, או קרנל, בגודל השדה הרצפטיבי שלהם. אם השדה הרצפטיבי שלהם למשל הוא 3 על 3 כמו שאמרנו אז בסוף הם ילמדו איזה פילטר W בגודל 3 על 3.

לדוגמא הפילטר הזה שכאן, W, שיש לו עמודה של אחדות באמצע ואפסים משני צדדיה.

למה אנחנו מסמנים את הפילטר בW? הW הזה הוא בעצם משקולות הרשת, זהו סט הפרמטרים שהיא לומדת, שהיא מעדכנת באמצעות גרדינט דיסנט לאור איזושהי פונקצית לוס.

הנוירונים בשכבת קונבולוציה יכפילו כל פאץ' שהם מסתכלים עליו עם הפילטר הזה ויסכמו למספר אחד - זה בדיוק לעשות קונבולוציה. בואו נראה דוגמא פשוטה.

:::

כאן יש לנו תמונה בגודל 5 על 5, מעין סמיילי. בשכבה הבאה Z, כל נוירון i, j, יהיה שווה להכפלה וסיכום של הפאץ' 3 על 3 שהוא מסתכל עליו עם הפילטר W.

וחוץ מהפילטר W, שנדגיש שוב כל הנוירונים בשכבה אחת משתמשים באותו אחד, נלמד גם חותך, או בשפה של רשתות נוירונים bias, מסומן כאן כb.

:::

אז הנה התמונה, הסמיילי. ונניח שאנחנו עושים לה zero padding (להדגים).

והנה הפילטר W בנקודה הזאת. איך תיראה השכבה Z שמעל? לדוגמא הנוירון בקצה השמאלי העליון הוא תוצאה של מכפלה של W בפאץ' הכי שמאלי עליון (להדגים) ומתקבל אפס. וככה הפילטר שלנו או הקרנל הזה סורק את כל התמונה ומאחסן את ה"מסקנה" שלו בכל נוירון ונוירון בשכבה Z החדשה.

אם נסתכל על Z, התמונה החדשה שנוצרה, מתי היא הכי חיובית? מה הכי הדליק את הנוירון? אנחנו רואים שהערכים הכי גבוהים הם איפה שהיו העיניים של הסמיילי. כלומר הפילטר שלנו, שיש בו בעצם קו אנכי באמצע, "מגיב" הכי הרבה לקווים אנכיים, הוא סורק את התמונה ומחזיר ערכים גבוהים איפה שהוא מוצא מתאם עם הפרט המאוד מאוד עדין הזה.

:::

והנה אותה תמונה של סמיילי עם פילטר W אחר, נסו להבין מה הפילטר הזה מחפש, למה הוא מגיב?

הפילטר הזה מגיב הכי הרבה לקווים אופקיים בתחתית הפאץ', כאן הוא נדלק ונותן ערך הכי גבוה 3, כשהוא מוצא מתאם מלא עם הפה של הסמיילי בתחתית התמונה.

:::

כעת כהרגלנו בקודש יכולנו לממש שכבת קונבולוציה בעצמנו. אבל הפעולה הזאת כל כך נפוצה שאפשר פשוט להשתמש בפונקציה conv2d ממודול tf.nn של טנסורפלואו.

אנחנו קוראים תמונה של שמי ניו יורק, הופכים אותה לשכבה אחת של שחור ולבן, ומוסיפים לה מימד אחד כי זה מה שהפונקציה רוצה לקבל. אנחנו לוקחים את הW האחרון שלנו שמחפש קווים אופקיים בתחתית הפאץ', מוסיפים לו 2 מימדים כי זה מה שהפונקציה רוצה לקבל.

ואז אנחנו מבקשים לעשות קונבולוציה עם הW שלנו על התמונה של ניו-יורק. אנחנו מזינים padding=same מה שאומר לטנסורפלואו להוסיף אפסים כדי לשמור על גודל התמונה, ומבקשים strides=1, שהפילטר שלנו ילך פיקסל פיקסל. האוביקט הסופי ny_convolved הוא בעצם שכבת הZ, התמונה החדשה אחרי קונבולוציה.

:::

כאן אני מציג את התמונה המקורית בצד שמאל מול התמונה לאחר קונבולוציה מצד ימין. אני לא יודע אם רואים את זה דרך הוידאו, מוטב לעבור למצגת המקורית ולהשוות בין תמונות כמה שיותר גדולות. בבניין במרכז התמונה ניתן להראות את ההבדל בצורה הברורה ביותר, הפילטר שלנו עם הקו האופקי אכן מטשטש כלומר לא שם לב לקווים האנכיים של הבניין, והרבה יותר מדגיש את הקווים האופקיים.

=== 3. רשתות קונבולוציה ===

עכשיו שלמדנו מה עושה שכבת קונבלוציה אחת, בואו נראה כיצד נראית רשת קונבולוציה.

:::

אחת האפשרויות שעומדות ברשותנו בכל שכבת קונבולוציה, היא לשנות את פרמטר הstrides. לפעמים זה מאוד מועיל לעשות קונבולוציה על תמונה לא צעד, צעד או פיקסל פיקסל. אלא לבקש צעדים בגודל 2 כמו שאנחנו רואים כאן. כשאנחנו מבצעים צעדים בגודל 2 השכבה שמעל תיעשה קטנה יותר, ותיכף נדבר על אילו יתרונות יכולים להיות לזה, מלבד בעצם להפחית את כמות הפרמטרים בשכבת הקונבולוציה הבאה.

:::

עכשיו אולי זה הציק לכם שעד עכשיו דיברתי על תמונה כאילו היא שכבה אחת של פיקסלים, כשאנחנו כל הזמן אומרים שתמונה סטנדרטית מורכבת מ3 שכבות.

אז כן, פישטנו את העניינים כדי לא לסבך, אבל האמת היא ששכבת קונבולוציה מסתכלת על כל שלוש השכבות של התמונה, אדום ירוק וכחול. והיא לא לומדת פילטר W של 3 על 3 ועוד bias. היא לומדת פילטר W שהוא "קוביה" של 3 על 3 על 3, ועוד bias (להדגים). כלומר לא 9 פרמטרים ועוד חותך אלא 27 פרמטרים ועוד חותך.

עכשיו זה מה שנקרא feature map אחת. שכבת קונבולוציה היא בעצם מערום או stacking, של כמה feature maps שכאלה, כל אחת תלמד פילטר או קרנל אחר בגודל נאמר 3 על 3 על 3 ועוד חותך. שוב -- כל feature map לומדת את הקרנל שלה שמשותף לכל הנוירונים בfeature map!

בתמונה כאן ניתן לראות איך הפיצ'ר מאפ הראשון לומד קווים אופקיים על רקע כחול, הפיצ'ר מאפ השני לומד להגיב דווקא לקווים אנכיים על רקע אדום.

ועל מה תסתכל שכבת הקונבולוציה הבאה? היא תסתכל על שכבת הקונבולוציה הקודמת, הפעם אין שם 3 שכבות אלא הרבה שכבות בדרך כלל, אז הפילטר או קרנל שהיא תלמד הוא כבר לא קוביה הוא מעין תיבה ארוכה. שכבה זו בעצם מסתכלת על נוירונים שמייצגים פיצ'רים מאוד לואו-לבל כמו קו אופקי, קו אנכי, ומשלבת ביניהם, לדוגמא כאן פיצ'ר מאפ שמגיבה לריבוע אדום על רקע כחול.

ככל שנלך במעלה הרשת אנחנו לומדים פיצ'רים יותר ויותר מורכבים, זה אחד האלמנטים שמאפשרים לרשתות קונבלוציה ללמוד דפוסים מורכבים כל כך כמו עין, אוזן של כלב, וכולי.

:::

ועדיין, גם כשהוספנו מימד ואנחנו מתחשבים בכל שלוש השכבות של תמונה, מדובר בפעולה פשוטה של הכפלה וסכימה, לקרנל שלנו בעצם נוסף מימד f_n, ואנחנו סוכמים גם עליו.

:::

אז אמרנו שהפרמטרים של הרשת הם כל הפילטרים או הקרנלים האלה, התיבות הארוכות של W. ואיך הרשת לומדת אותם?

בדיוק כמו הרשת הפשוטה שדיברנו עליה ביחידה הקודמת: יש לנו איזושהי פונקצית לוס של רגרסיה או קלסיפיקציה לדוגמא, והרשת מבצעת forward propagation ו-backward propagation, עם סטוכסטיק גרדיאנט דיסנט, על הפרמטרים של הקרנלים, שוב ושוב כדי להוריד את הלוס.

:::

אנחנו מתחילים להבין כמה כוח יש לרשת כזאת, אבל יש סיבה שהיא לא היתה בשימוש נפוץ כל-כך עד לפני 10-15 שנה.

תחשבו על תמונה אחת בגודל סביר 100 על 100 פיקסלים. ועל התמונה אני בונה שכבת קונבולוציה אחת עם 100 פיצ'ר מאפס ופילטר 3 על 3. אז הפילטר הוא בעצם קוביה 3 על 3 על 3 ועוד חותך, משותפת לכל הנוירונים בשכבה, כפול 100 שכבות. זה 2800 פרמטרים, נורא מעט לעומת המספרים המפלצתיים שהגענו אליהם ביחידה הקודמת.

אבל מה עם האחסון בזיכרון? אני צריך לאחסן, עבור שכבה אחת, רק על התמונה הזאת, 100 תמונות שונות! זה 100 כפול 100 כפול 100, מיליון מספרים בfloat, כל אחד לוקח 4 בייט, זה יוצא 4 מגה-בייט לתמונה אחת לשכבה אחת. ואני אוסיף עוד עשרות אלפי תמונות ואולי עשרות של שכבות, זה מתחיל להיראות מודל כבד מאוד.

ואין לנו רק בעיה של אחסון, יש לנו גם בעיה חישובית -- איך הגעתי לכל מיליון המספרים האלה בשכבה אחת? עשיתי הכפלה של הפילטר בתמונה נאמר, זה בעצם סכום של 27 הכפלות כדי להגיע לכל מספר יחיד, כלומר לשכבה אחת על תמונה אחת עשיתי 27 מיליון הכפלות מספרים.

לכן בלי מימוש חכם, שנוגע גם לחישוב וגם לאחסון, רשתות כאלה נשארו בגדר רעיון בלבד או שהיו נחלתם הבלעדית של חוקרים עם מחשבי על.

:::

דבר אחד שאנחנו יכולים לעשות גם כדי להקל עלינו חישובית וגם כדי לעזור לרשת ללמוד טוב יותר, הוא pooling layers. שכבת פולינג מסכמת או ממצעת שכבת קונבולוציה שבאה לפניה, על-ידי לקיחת מקסימום או ממוצע של איזשהו שדה רצפטיבי, בדיוק כמו השדה הרצפטיבי שדיברנו עליו קודם. כאן בתמונה הנוירון האדום יסתכל על איזשהו פאץ' בגודל 2 על 2, וייקח ממנו רק את המקסימום 5. לאחר מכן נלך stride של 2, והנוירון הכחול יסכם כבר פאץ' אחר.

ושימו לב שבשכבת פולינג אין פרמטרים! כלומר היא לא משתתפת במשחק הגרדיאנט דיסנט, היא לא מעיקה עלינו חישובית.

:::

אם נחזור לסמיילי שלנו, סיימנו במה קונבולוציה עם קרנל שכזה תעשה עליו. כעת נעשה max pooling, ונגיע לתמונה קטנה הרבה יותר. 
(להדגים)

בדרך כלל גם לא נטרח לעשות כאן zero padding, מה שאומר שאולי לא נכסה את קצוות התמונה.

:::

אבל אנחנו מפסידים המון מידע לא? אנחנו לוקחים תמונה עשירה בפיקסלים ועושים לה רדוקציה, יותר ויותר.

למעשה בשכבת max pooling עם פילטר 2 על 2, בכמה אנחנו מקטינים את השכבה הקודמת? ב75 אחוזים!

אבל אולי זה דבר טוב? להתעלם מכמה נוירונים, או להתייחס אל כל אחד ואחד בערבון מוגבל, לקחת רק איזשהו מיצוע שלהם? אנחנו יודעים כבר שמיצוע עוזר מאוד לחיזוי בכך שהוא מקטין אוברפיטינג. ולא רק זה, במקרה של תמונה, ראינו שלהתרחק ממנה, ולהסתכל עליה יותר בהיי-לבל ויותר בהיי-לבל, מאפשר לרשת ללמוד פיצ'רים יותר ויותר מורכבים. לבסוף, מה לקיחת מקסימום מאפשרת לי? היא מאפשרת לדעת אם פיצ'ר מסוים קיים או לא בשכבה שמתחתי, בלי להתחשב או להיות רגיש במיקום של הפיצ'ר הזה! הרשת לא תהיה רגישה רק לאוזניים של כלב שראתה בתחתית התמונה מצד שמאל, היא תדע לזהות אותן בכל מקום. אנחנו קוראים לתכונה כזאת אינווריאנטיות.

שאלה אחרונה לפני שממשיכים: האם שכבת הקונבולוציה היא פעולה ליניארית?

האמת היא, שלפי מה שסיפרתי לכם עד עכשיו, היא יכולה להיות. מה ראינו? הכפלה, סכום ואז פולינג על-ידי לקיחת ממוצע שזו גם פעולה ליניארית. אם תחקרו עוד ברשת תראו איך אפשר עם פרמטריזציה נכונה לנסח את כל הפעולות האלה כפעולה ליניארית. אבל בפועל אנחנו כמעט תמיד נוסיף איזושהי אקטיבציה לא-ליניארית בכל שכבת קונבולוציה, מסוג האקטיבציות שדיברנו עליהן ביחידה הקודמת, למשל relu. כך שבשורה התחתונה נישאר עם פעולה לא-ליניארית.

:::

וככה נראית רשת טיפוסית. תמונה נכנסת ועוברת שכבת קונבלוציה ראשונה עם מספר פיצ'ר מאפס, כל אחת תלמד פיצ'ר לואו-לבל אחר. לאחר מכן פולינג ייקח את המקסימום, יעשה את הפיצ'ר מאפס קטנות יותר. ושוב שכבת קונבולוציה, ושוב פולינג. בסופו של דבר נעשה שיטוח, פלטנינג לנוירונים שנשארנו איתם ונחבר אותם לשכבה fully connected או dense כמו שקוראים לה בקראס, ואז למשל נוירון בודד עם פונקצית אקטיבציה זיגמויד בשביל קלאסיפיקציה או בלי אקטיבציה בשביל רגרסיה.

:::

ויש עוד אלמנט שיכול לפעמים להעניק עוד כוח עצום לרשתות קונבולוציה לעיבוד תמונות. כשמדובר בנתוני טבלה, על אנשים או מוצרים, קשה לעבות את הטבלה בעוד נתונים, לעשות לה אוגמנטציה. בתמונות הדבר שונה. הרי רשת שצריכה לזהות האם יש בתמונה כלב או חתול, זה לא צריך לשנות לה אם הכלב נמצא בצד ימין של התמונה או הוזז לצד שמאל.

אז יש מספר טרנספורמציות על תמונות שאפשר לעשות וכך להגדיל את גודל המדגם ולגוון אותו באופן שמאפשר לרשת קונבולוציה להכליל טוב יותר. אפשר לבצע טרנזליישן, העתקה, להזיז קצת את התמונה ממקום למקום. רוטציה של התמונה, סיבוב קל שלה. ריסקיילינג, הכוונה להגדלה והקטנה של התמונה. פליפינג, היפוך שלה על פני איזשהו ציר אנכי בדרך כלל. וסטרצ'ינג, הכוונה למתיחה שלה גם בכיוונים אלכסוניים.

כל זה טוב ונהדר אבל צריך להיזהר. אתם יכולים לחשוב על יישומים שבהם לא הייתם רוצים לבצע אימג' אוגמנטיישן, שזה פשוט לא נכון לעשות? אני יכול לחשוב למשל על יישומים רפואיים, כמו רשתות שיקראו צילומי רנטגן. לא רק שלא הייתי רוצה להזיז סתם כך גידול ממקום למקום, גם לא ברור מה הטעם, הרי הרשת תיבחן רק על צילומי רנטגן מיושרים ממכונת רנטגן. אפשרות אחרת שצריך להיזהר איתה זה בדוגמת הציורים שלנו! לסובב קצת ציור ימינה או שמאלה אולי זה רעיון סביר אבל למתוח אותו בכיוון אלכסוני? אולי זה מעוות אותו שלא לצורך ומזיק לרשת.

:::

בקראס ברור שיש מימוש נוח לעשות את זה בצורה אוטומטית ותוך כדי אימון. כאן יש את הקלאס imagadatagenerator, שמג'נרט תמונות חדשות כל הזמן עם טרנספורמציות אקראיות מתוך משפחה שאתם מגדירים מראש. כאן למשל אני מגדיר את הזווית המירבית של סיבוב תמונה שאני מוכן שהגנרטור יבצע.

בכל מקרה אפשר להכניס לתוך הגנרטור סט של תמונות, ולקרוא כל פעם למתודה next שלו, ולקבל עוד באץ' ועוד באץ' של תמונות. כאן אני עושה את זה עם התמונה של שמי ניו יורק.

:::

התוצאה היא אותה תמונה שכל פעם עוברת איזה עיוות קטן אבל עדיין ברור מכולם שאלה הם שמי ניו יורק.

:::

נסכם מה נותן את כל הכוח הזה לרשתות קונבולוציה:

אנחנו לא מתייחסים לפיקסלים כבלתי תלויים, אנחנו לוקחים בחשבון את התלות המרחבית ביניהם.

הארכיטקטורה של הרשת מאפשרת לשכבות הראשונות ללמוד פיצ'רים מאוד בסיסיים כמו קו אנכי, ולאט לאט לבנות מהם פיצ'רים מורכבים יותר ויותר.

שרד ווייטס - כמו שאמרנו כל פיצ'ר מאפ לומדת סט של משקולות שקראנו לו פילטר או קרנל, אחד! כל הנוירונים בפיצ'ר מאפ חולקים אותו. זה לא רק אומר הרבה פחות פרמטרים לעומת רשת רגילה, זה גם מאפשר לרשת ללמוד פיצ'ר בכל מקום בתמונה.

הפולינג בעצם מונע מאיתנו לעשות אוברפיטינג לכל פיקסל ופיקסל וגם מאפשר לנו אינווריינטיות ללמידת פיצ'רים באמצעות פעולת המקסימום.

אוגמנטציה של תמונות מאפשרת לנו להגדיל את מסד הנתונים שלנו ולרשת להכליל מול קצת רעש -- איפה שזה דבר נכון לעשות.

ולבסוף, לא דיברנו על זה לעומק כאן, אבל מעבדים חזקים יותר כמו GPU, ופיתוח תוכנות כמו טנזורפלואו ופייטורץ' יש להם בהחלט חלק חשוב במעבר של רשתות קונבולוציה מהאקדמיה לתעשיה ומה שמאפשר להן ללמוד ולחזות בצורה מהירה כל כך. 

=== 4. רשת קונבלוציה על התמונות של תאי המלריה ===

בואו נראה את הביצועים של רשת קונבולוציה בפעולה, על תאי המלריה שלנו.

:::

נזכיר שהורדנו את התמונות של תאים נגועים במלריה או לא נגועים, מספריית tensorflow datasets.

קראנו אותו לתוך מערך תלת מימדי X וחילקנו לX_train ו X_test.

לקחנו רק 2500 תמונות שזה בערך 10 אחוז מהתמונות הקיימות רק בשביל הדגמה, 2000 בטריין, ו500 בטסט. וכשהזנו אותן לרשת רגילה היינו צריכים גם לשטח אותן להיות עם 30 אלף פיקסלים ובעצם להתעלם מכך שלפנינו תמונות עם תלות מרחבית. כאן -- אנחנו לא צריכים לשטח כמובן, נשארים עם מערכים תלת מימדים 100 על 100 על 3 שכבות צבע.

:::

בניית רשת קונבולוציה בקראס פשוטה כמו רשת רגילה.

אני עושה אימפורט לשכבות שנחוצות לי כמו Conv2D ו-MaxPooling.

אני מאתחל את הרשת עם הקלאס Sequential ומוסיף לה שכבת קונבולוציה. אני מבקש 32 פילטרים, כלומר 32 feature maps. גודל הקרנל או כל פילטר ופילטר יהיה 3 על 3, זה גודל השדה הרצפטיבי של כל נוירון. אני מגדיר גם שאני רוצה zero padding עם הפרמטר padding = same. כמו שאמרנו על האאוטפוט של כל נוירון תהיה אקטיבצית relu. מספר הstrides בברירת מחדל הוא 1, כלומר השכבה תלך צעד צעד ולא תהיה הקטנת מימד.

אחר כך אני מוסיף שכבת מקס פולינג. אני מבקש pool_size של 2 על 2, ובאופן דיפולטיבי אקבל גם strides מאותו גודל.

ואז עוד שכבת קונבולוציה של 64 פיצ'ר מאפס, עוד מקס פולינג.

לאחר מכן אני משרשר את כל הנוירונים שיצאו לי עם שכבת flatten.

מוסיף איזשהו דרופאאוט עם יחס חצי, ומחבר את כולם לשכבת dense סופית עם נוירון אחד, ואקטיבצית זיגמויד. למה אקטיבצית זיגמויד? כי בסופו של דבר המשימה של הרשת היא קלאסיפיקציה, אנחנו רוצים סקור בין אפס לאחת שיבטא עד כמה הרשת בטוחה שיש תמונה של תא נגוע במלריה.

מבחינת קומפילציה הכל נשאר זהה, מתאים כאן לוס של קלסיפיקציה לשני קלאסים של binary crossentropy, אופטימייזר של אדם ונוסיף גם דיווח על accuracy תוך כדי אימון.

:::

אבל עוד לפני אימון מומלץ מאוד כמו ביחידה הקודמת לעצור רגע ולבקש model.summary כדי לראות כמה פרמטרים יש ברשת ולנסות להבין איך הגענו למספר הזה.

נשים לב כבר שברשת הפשוטה ביחידה הקודמת, למרות שהיא רשת פשוטה, הגענו ללמעלה מתשעה מיליון פרמטרים! וכאן, יש רק 56 אלף, כלומר שני סדרי גודל פחות. אבל איך הגענו לזה?

(הדגמה)

:::

כעת נאמן את הרשת עם קולבק של early stopping שאנחנו מכירים, הוא מסתכל על עשרה אחוז ולידיישן לוס, ובודק שהוא כל הזמן יורד. אם לא ירד 5 איפוקים האימון ייעצר. כאן נאמן עד למקסימום של 50 איפוקים, וגודל באץ' של 100 תמונות כל פעם.

:::

אנחנו מציירים את הלוס והאקיורסי עבור הטריין והולידיישן. עצרנו אחרי כ30 איפוקים כי לא היה שיפור 5 איפוקים בלוס של הולידיישן בירוק.

:::

איך ביצענו? עם רשת פשוטה אחרי הרבה טיוניג ומאמץ הגענו ל69 אחוז אקיורסי על מדגם הטסט.

ניזכר שהרשת מוציאה סקורים בין אפס לאחת, אנחנו מוציאים מזה חיזוי סופי באמצעות השוואה לערך קאטאוף של חצי -- כאן זה נכון לעשות כי הדאטא הוא באלאנסד, חצי מהתאים נגועים במלריה וחצי לא.

ואנחנו כבר ב76 אחוז אקיורסי כשמשווים בין y_test ל-y_pred.

כשמבקשים קונפיוז'ן מטריקס על מדגם הטסט רואים שהיא אכן בריאה, רוב התצפיות הן על האלכסון.

:::

אבל זה רק עם 10 אחוז מהתמונות! נפתח עכשיו גוגל קולאב עם GPU טוב, ונעשה אימון על כל 27 אלף התמונות.

(הדגמה)

אנחנו רואים שדי מהר, הגענו ל94 אחוזי דיוק כשהשתמשנו בכל התמונות עם אותה רשת פשוטה שלא טרחנו אפילו לכוונן, כלומר אולי אפשר להגיע כאן לאחוזי דיוק גבוהים יותר באמצעות טיונינג של מספר השכבות, גודל הקרנל, מספר הפיצ'ר מאפס וכולי. כל מה שדיברנו עליו בהקשר של טיוניג של מודלים של קראס ביחידה הקודמת, תקף.

מכל מקום דמיינו שאתם החוקרים לפני 10-15 שנה, אתם משאירים את המחשב ללילה לעבוד ומגיעים בבוקר לתוצאות האלה. ולחוקרים ברגע האימג'נט לא היה קראס, הם לא מימשו רשת בעשר שורות של קוד. אני בטוח שדבר ראשון הם חשבו שיש להם באג, אחוזי דיוק כאלה לא נראו לפני כן. וזה מסביר למה הרגע הזה נקרא imagenet moment, זה באמת שינה את כל התחום של ראייה ממוחשבת לאחר מכן, וכל מי שעסק בתעשייה של דאטא, שמע עליו. בסופו של דבר חוקרים כמו ג'פרי הינטון ויאן לקון קיבלו על הפיתוחים שלהם בתחום הדיפ לרנינג את מדליית טיורינג.

=== 5. ויזואליזציה של רשתות קונבולוציה ===

ככל שהמודלים שלנו עמוקים ומורכבים יותר הבעיה שהמודל שלנו הוא קופסא שחורה שאנחנו לא מבינים נעשית חמורה יותר. זה לא רק פתח לאוברפיטינג ומודלים לא יעילים, זאת יכולה להיות גם בעיה רצינית לגרום ללקוחות להשתמש בהם, לקוחות כמו רופאים שאמורים להסתמך על בדיקת תאי המלריה שלנו, או גופים ממשלתיים. באופן כללי התחום של explainable AI הוא חם מאוד והוא מתפתח כמו הAI עצמו - איך אנחנו מסבירים מה המודל שלנו עושה?

אז לא ניכנס לעומק התחום, אבל ננסה למשל לעשות ויזואליזציה של מה המודל שלנו ראה.

:::

מבחינת פרמטרים, אנחנו יודעים להשיג אותם. אפשר לבקש את השכבה הראשונה שנקראת conv2d, ולבקש עליה get_weights. נקבל את כל 32 הפילטרים של כל 32 הפיצ'ר מאפס, כל אחד אמרנו בגודל קוביה של 3 על 3 על 3.

המספרים עצמם לא יגידו לנו כלום כמובן, מדובר במשקולות על פיקסלים.

:::

אנחנו יכולים לנסות לצייר אותם. כאן אני מצייר ממש את החתך הראשון של הקובייה הראשונה, הפילטר הראשון, החתך שמתייחס לשכבת האדום. ואז את החתך השני לשכבת הירוק ואת השלישי לשכבת הכחול.

אבל אנחנו יודעים שהשכבה הראשונה לומדת פיצ'רים כל כך לואו-לבל, שקשה לתת למה שאנחנו רואים אינטרפרטציה כמו "קו אופקי" או "קו אנכי". זו ממש קופסה שחורה.

:::

כדי בכל זאת להבין אפשר לקחת תמונה נתונה, כמו כאן, התא הראשון במדגם הטסט, ולהריץ אותה דרך השכבה הראשונה של הקונבולוציה. מה שאנחנו צופים לקבל זה את אותו התא שעבר דרך 32 פיצ'ר מאפס שונים.

:::

כדי לעשות את זה אני מגדיר מודל עם API שלא ראינו שנקרא Model. אני מגדיר שהאינפוט שלו יהיה כמו האינפוט של המודל המקורי שלנו, והאאוטפוט שלו יהיה האאוטפוט של השכבה הראשונה, כלומר ה32 פיצ'ר מאפס.

כשאני מזין למודל הזה את התא הספציפי שלנו באמצעות קריאה לpredict על התא, אני אכן מקבל 32 פיצ'ר מאפס או תמונות, בגודל 100 על 100.

ואת התמונות האלה אפשר להראות כדי להבין מה הדליק כל פיצ'ר מאפ בשכבת הקונבולוציה הראשונה.

:::

אז אם נניח שככל שהתמונה בהירה יותר האיזור חשוב יותר לפיצ'ר מאפ, ניתן לראות פיצ'ר מאפס שמאוד מתמקדים בגבולות של התא -- הם משחירים הכל חוץ מהגבולות. יש פיצ'רמאפס שמבודדים רק איזור פנימי בתוך התא, ויש פיצ'רמאפס שמתמקדים בתוך התא אבל בפריפריה שלו.

באמצעות ויזואליזציות כאלו אנחנו יכולים בצורה זהירה להבין יותר ויותר איך הרשת שלנו מזהה תא שנגוע במלריה. והרבה פעמים נראה כמה היא יצירתית ואילו פיצ'רים מחוכמים היא בעצמה למדה.

=== 6. ארכיטקטורות של רשתות קונבולוציה ===

זו היתה רשת קונבולוציה הכי פשוטה, ברור שמאז עניינים קצת התפתחו ויש ארכיטקטורות שונות ומשונות על בסיס הרעיון הזה.

:::

בגרף הזה ניתן לראות את הביצועים של רשתות בתחרות האימג'נט ככל שעוברות השנים.

על ציר הוואי אנחנו רואים את הטופ-5 ארור, כלומר מה אחוז הטעות בסיווג תמונות לקטגוריות כשאני מתחשב בטופ 5 קטגוריות שהמודל מציע, מתוך 1000.

רגע האימג'נט שלנו נמצא כאן בשנת 2012, ירידה של למעלה מעשרה אחוז בטעות החיזוי. קפיצה מרשימה נוספת אפשר לראות ב2015 עם ארכיטקטורת הרזנט שנגיד עליה כמה מילים, הראשונה שירדה מתחת ל5 אחוז טעות חיזוי.

כיום מודלים עמוקים ומורכבים יורדים כבר למתחת אחוז אחד טעות בטופ-5, ופחות מ10 אחוז טעות בטופ-1.

:::

זו הארכיטקטורה של יאן לקון ב1998, בסוף המאה הקודמת. היא פשוטה יותר מהרשת שאנחנו בנינו, ואתם יכולים לממש אותו באמצעות כמה שורות בקראס:

:::

לפניכם מימוש של LeNet, שימו לב כמה קל קראס גורמת לכל זה להיראות.

:::

האלכסנט שעשתה את רגע האימג'נט מתוארת סכמטית כאן, אתם יכולים לראות שהרשת מתפצלת לשני ענפים של קונבולוציה, אבל הם מדברים במרכאות אחד עם השני, עד חיזוי סופי של 1000 נוירונים ל1000 קלאסים.

:::

נזכיר גם את הרזנט המפורסמת. כאן מתוארת רזנט-32 שבנויה מ-32 בלוקים שבכל אחד שתי שכבות קונבולוציה! והחוקרים השתמשו גם ברזנט-152. איך הם גרמו לזה לעבוד?

:::

בגדול, אפשר לטעון שככל שמודל יהיה עמוק יותר כך הוא יהיה מורכב יותר, ומסוגל ללמוד דפוסים מורכבים יותר ולהגיע לביצועים טובים יותר.

אבל חוקרים תמיד נזהרו לא לעשות מודלים עמוקים מדי בגלל בעית הגרדיאנט הנעלם, הvanishing gradient. הרי אנחנו משתמשים בפונקצית אקטיבציה כמו relu, שמאפסת או מכבה כל אאוטפוט מתחת לאפס. אנחנו עושים את זה כי זה מה שמוסיף אי-ליניאריות לרשת שלנו. אז דמיינו שזה קורה אחרי עוד שכבה ועוד שכבה ועוד שכבה. מה הסיכוי של איזשהו סיגנל לפעפע מהתמונה עד לקצה הרשת, ולא לעבור איפוס מתישהו. כך בעצם אנחנו "מעלימים" את הגרדיאנט ולא מתבצעת למידה. תופעה הפוכה שנזכיר רק בשם היא הגרדיאנט המתפוצץ, exploding gradient.

באמצעות פרדיגמה שנקראת residual learning, אנחנו מסייעים לסיגנל להתקדם. אנחנו יוצרים מעקף של הסיגנל אל קצה השכבה בלי לגעת בו, ומוסיפים אותו רגע לפני האקטיבציית relu הסופית.

באופן כזה הסיגנל יכול לפעפע עמוק מאוד, והבלוק הזה לא לומד איזושהי פונקציה של הסיגנל H(x), אלא את השארית H(x) פחות x. לכן זה נקרא residual learning.

=== 7. שימוש ברשתות קונבולוציה שכבר אומנו ===

לסיום נלמד איך אנחנו משתמשים במודלים המפוארים האלה שאחרים אימנו בשבילנו בדם יזע ודמעות של GPU. בכל זאת, תמונות זה משהו שיש לכל אחד מאיתנו בשפע. האם המודלים האלה יעבדו על התמונות שיש לנו בפלאפון?

:::

בסופו של דבר מהו המודל הזה, מהי הרשת שנלמדה? זה אוסף של משקלות! במקרה של הרשת שלנו זה 56 אלף מספרים, וכבר ראינו כמה קל בקראס לשמור אותם כאיזשהו אוביקט בזיכרון, ולקרוא אותו שוב לצורך חיזוי. מסתבר שכל המשתתפים בתחרות האימג'נט חויבו לשמור את המשקולות של הרשתות שלהם ולספק אותם לציבור בחינם. וקראס שומרים על המשקולות האלה וניתן בשורת קוד אחת להוריד אותם מהאינטרנט ולהשתמש במודלים בעצם. ניתן לראות שכל המודלים המפורסמים שוקלים בסך הכל כמה עשרות מגה בייטים, אפשר לשים אותם גם על הפלאפון.

:::

כאן יש לי תמונה של הכלב שלי, יוהאן. הוא תמיד נראה לי רועה גרמני מעורב. אני יודע שבאימג'נט מתוך 1000 קטגוריות יש הרבה זנים של כלבים, ומעניין אותי לדעת מה הזן של יוהאן שרזנט למשל יחזה.

אני עושה אימפורט לקלאס Resnet50 וככה אני בעצם מוריד את המשקולות שלו. את התמונה של יוהאן אני שומר באובייקט johann.

:::

עכשיו אני משנה את הגודל של התמונה של יוהאן לגודל שרזנט צריך, כל התמונות של אימג'נט היו בגודל 224 על 224. אני מוסיף לה מימד אחד בהתחלה כי רזנט מצפה לקבל באץ' של תמונות, ומכפיל פי 255 כי הוא רוצה את התמונות כמערכים עם מספרים בין 0 ל255.

לבסוף אני משתמש בפונקציה של רזנט preprocess_input כדי לבצע על התמונה מה שהחוקרים רצו לבצע. וכשאני מבקש predict על התמונה של יוהאן, אני מקבל בעצם 1000 סקורים ל1000 הקלאסים של אימג'נט שמסתכמים ב-1. מעין הסתברויות.

הפונקציה של רזנט decode_predictions מאפשרת לי לתת שם לכל אחד מהסקורים, ואני מדפיס את הקטגוריות עם הטופ 5 סקורים.

והתוצאה עושה שכל, הטופ 5 קטגוריות מתוך 1000 הן אכן של זני כלבים, כולל רועה גרמני וזנים קרובים, כמו האסקי סיבירי. מי יודע, אולי המודל רואה משהו שאנחנו לא רואים.

אבל הדגש כאן שלא חיממנו את המחשב שלנו וכתבנו קוד, ושילמנו על מעבדים יקרים. אלא במינימום מאמץ השתמשנו במודל קיים כדי לחזות איזושהי תמונה מהאוסף האישי שלנו. אתם יכולים לארוז מודל כזה ולעשות ממנו אפליקציה על הפלאפון שלכם.

=== 8. מה הלאה בדיפ לרנינג לראייה ממוחשבת? ===

מה הלאה למי שירצה להעמיק בתחום של ראייה ממוחשבת?

טרנספר לרנינג, מאפשרת לכם להקפיא את השכבות הנמוכות של מודלים עמוקים, ולאמן רק את העליונות, כדי להשיג בצורה מהירה מודלים שמתמחים במשימה הספציפית שלכם, עם פחות דאטא. הם פשוט בונים על "ידע" קיים במודלים שאומנו בצורה הרבה יותר מסיבית.

אובג'קט דטקשן, זה לא רק להגיד אם יש כלב או לא, אלא ממש לצייר מלבן סביב איפה הכלב בתמונה.

סגמנטיישן, זה לא להסתפק במלבן, אלא להגדיר ממש אילו פיקסלים בתמונה מייצגים את הכלב או כל עצם אחר, ממש לצבוע אותם.

קפשנינג זה מודל שייקח תמונה כקלט, והפלט שלו יהיה תיאור של התמונה במשפט או שניים. "כלב מסוג רועה גרמני משחק בכדור על הדשא".

אנחנו עושים היום גם רקונסטרוקציה של תמונות מדו מימד לסביבה תלת מימדית.

אנחנו עושים רסטורציה של תמונות, חשבו על מודל שמקבל את אחד הציורים שלנו ויכול להגיד איך הוא נראה לפני מאתיים שנה, לפני שדהה, או מודל שלוקח תמונה עם חלקים מוסתרים ויכול לשחזר אותם.

יש לנו מודלים שיודעים לשחזר את הpose שרקדנית נמצאת בה, ואז אולי להלביש את זה על תמונה של כל אדם. יש לנו מודלים שיודעים לג'נרט תמונות שהם לא ראו, למשל פרצופים שלא באמת קיימים אבל מאוד קשה להבחין בזה.

ותחום אחר פעיל הוא איך אנחנו עושים את המודלים האלה כמה שיותר מהירים, על וידאו, ומגיבים בריל-טיים למה שהם רואים, במכוניות אוטומטית או אפילו בפילטר האהוב עליכם באינסטגרם.

בקורס זה אנחנו לא מתיימרים להיכנס לעובי הקורה של אף אחד מאלה, אני מקווה שקיבלתם טעימה מעולם הדיפ לרנינג, שתסקרן אתכם ללמוד עוד. בכל תחום מחקר כיום ממדעי הרוח ועד הנדסה יש מקום לדיפ לרנינג או אם תרצו AI, לסייע לחוקרים.
:::
