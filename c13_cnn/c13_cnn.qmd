---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Convolutional Neural Networks"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Convolutional Neural Networks - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ביחידה זו נלמד על רשתות קונבולוציה שמתאימות במיוחד לבניית מודלים לחיזוי על תמונות. רשתות קונבולוציה הן כנראה המודל המודרני ביותר שנלמד בקורס זה, והן הבסיס להבנה של מודלים מתחום הראייה הממוחשבת. מודלים שמאפשרים למכוניות לנסוע ללא נהג, וללקוחות לצאת מסופר מבלי לעבור בקופה.
:::
:::

---

## Before CNN {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל זה לא שרשתות קונבולוציה צמחו בואקום. חשוב לפחות להבין מה היה לפניהן.
:::
:::

---

### Reminder: What is an image?

```{python}
#| echo: false

import os
os.environ['CUDA_VISIBLE_DEVICES'] = "-1" # disabling GPU as CUDA causes probs with tf.nn.conv2d()

```

```{python}
import matplotlib.pyplot as plt
from matplotlib.image import imread

logo = imread('../Intro2DS_logo.jpg')

plt.imshow(logo)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר מהו הייצוג של תמונה אצלנו במחשב. כאן אני קורא את הלוגו של הקורס עם המודול image של ספריית matplotlib.

מה קיבלתי?
:::
:::

---

```{python}
import numpy as np

print(type(logo))
```

```{python}
print(logo.shape)
```

```{python}
print(logo[:4, :4, 0])
```

```{python}
print(logo.min(), logo.max())
```

```{python}
print(logo.dtype, logo.size * logo.itemsize)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קיבלתי מערך של numpy array.

המימדים שלו הם 1400 על 1400 על 3 שכבות צבע: אדום, ירוק וכחול, או RGB.

אם ניקח את השכבה הראשונה ונבקש לראות את הפינה השמאלית העליונה, נראה פשוט מספרים, כלומר התמונה שלנו היא מערך תלת מימדי של מספרים שלמים.

המספרים נעים מ0 עד 255, כלומר 256 אפשרויות בכל שכבת צבע, כאן אין פיקסלים עם רמת צבע פחות מ8.

כשאני מבקש את האטריביוט דיטייפ של המערך הוא uint8, כלומר כל מספר מיוצג על-ידי בייט אחד. האטריביוט size מחזיר את מספר המספרים במערך והאטריביוט itemsize מחזיר את מספר הבייטים לכל מספר. יש לנו 1400 כפול 1400 כפול 3 מספרים, כפול בייט 1. כלומר התמונה שלנו צורכת בזיכרון כמעט 6 מגה-בייט.
:::
:::

---

### Until Convolutional Networks

So how many features is that?

::: {.fragment}
- In 1998 LeCun et. al. published [LeNet-5](https://ieeexplore.ieee.org/abstract/document/726791) for digit recognition
- But it wasn't until 2012 when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton published [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) when the Deep Learning mania really took off.
:::

::: {.fragment}
Until then:

1. Treat it as a regular high-dimensional ML problem
2. Image feature engineering
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אם נתייחס לכל פיקסל ופיקסל כפיצ'ר או משתנה, כמה משתנים יהיו לנו? כמעט 6 מיליון!

אז רשתות קונבולוציה החלו בערך ב-1998 כאשר יאן לקון הציג את הlenet-5 לזיהוי אוטומטי של ספרות. אתם יכולים לחפש ביוטיוב סרטים ישנים שלו מציג במעבדה על מחשב ישן את הפלא הזה. וב2012 הגיע מה שמכונה רגע האימג'נט, שבו אלכס קריז'בסקי, איליה סוצקבר וג'פרי הינטון פרסמו רשת בשם אלכסנט והביאו ליכולות חיזוי שטרם נראו.

אבל עד אז, התייחסו לתמונה כעוד בעיה עם מימד מאוד גבוה, כמו 6 מיליון משתנים. והתמקדו ברעיונות שונים לפיצ'רים על סמך הבנה של הפיסיקה של העצם הזה, שנקרא תמונה. לדוגמא פיצ'רים שמסתכלים על ההיסטוגרמה של כמות האדום בתמונה, ומתארים את הממוצע, החציון, הסקיונס שלה.
:::
:::

---

## Convolutional Layer (2D) {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מהי שכבת הקונבולוציה שעשתה כזאת מהפכה?
:::
:::

---

### What is Convolution?

::: {.fragment}
"the integral of the product of the two functions after one is reflected about the y-axis and shifted"

$$(f \ast g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau)d\tau$$
:::

::: {.fragment}
![](images/Convolution_of_spiky_function_with_box2.gif){fig-align="center"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קונבולוציה היא לא דבר חדש. קונבולוציה היא מושג ידוע ממתמטיקה. מכפלה של שתי פונקציות f ו-g, ולקיחת אינטגרל או סיכום של ערך המכפלה על פני כל טווח מסוים.

כאן אנחנו רואים קונבולוציה בין פונקציה g באדום לבין פונקציה f בכחול. תוצאת הקונבולוציה מסומנת בשחור, ואפשר לראות שהיא פונקציה נוספת של פרמטר t. אנחנו מזיזים את g כמו חלון נע, ובכל נקודה בודקים מהו סך שטח החפיפה בינה לבין f, או אינטגרל, ומבטאים את השטח באמצעות פונקציה חדשה.

במובנים מסוימים הפונקציה החדשה מאוד מזכירה קורלציה בין שתי הפונקציות, איזושהי פונקצית מתאם שבודקת עד כמה f דומה לg. אם למשל f היתה זהה לריבוע של g באיזו נקודה, הן היו חופפות לחלוטין והאינטגרל היה 1, שטח ריבוע. כאן זה לא קורה.

זוכרים איפה ראינו כבר קונבולוציה? כשדיברנו על kernel density estimation, ואיך יוצרים תרשים צפיפות של התפלגות, שם קראנו לחלון הנע הזה w(x).

בכל מקרה לא מוכרחים לדעת מהי קונבולוציה כדי להבין איך שכבת קונבולוציה עובדת, זה פשוט מדהים לראות איך הכרה של מושג מתחום המתמטיקה סייעה כל כך לעשות מהפכה בתחום הראייה הממוחשבת.
:::
:::

---

### The Convolutional Layer

![](images/conv01.png)

Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- First layer: every neuron has a "receptive field", it is focused on a specific rectangle of the image, usually 2x2, 3x3
- Second layer: every neuron has a receptive field in the first layer
- Etc.

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מהי שכבת קונבולוציה?

שכבת הקונבולוציה הראשונה מורכבת מ-K על K נוירונים, שלכל אחד מהם יש "שדה רצפטיבי". הוא לא "מסתכל" על כל הפיקסלים בתמונה שמתחתיו. הוא מסתכל רק על ריבוע של 2X2 או 3X3.

בשכבה הבאה, כל נוירון ונוירון מסתכל על השכבה שמתחתיו, שאפשר לראות בה תמונה חדשה, גם כאן הוא לא מסתכל על כל התמונה, אלא רק על שדה רצפטיבי, וכך בשכבת הקונבולוציה הבאה, והבאה וכולי.
:::
:::

---

### More specifically

![](images/conv02.png)

Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- The $[i,j]$ neuron looks at the rectangle at rows $i$ to $i + f_h - 1$, columns $j$ to $j + f_w - 1$
- Zero Padding: adds 0s around the image to make the next layer the same dimensions

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם רוצים להגדיר את זה יותר מדויק, אפשר לחשוב על פרמטרים f_h ו-f_w שמגדירים את גודל השדה הרצפטיבי. ואז נוירון i, j בשכבת קונבולוציה יסתכל על המלבן שנוצר משורות i עד f_h - 1, ומעמודות j עד f_w - 1.

כאן אפשר לראות למשל את הריבוע שהנוירון האדום מסתכל עליו בשכבה שמתחתיו. ואז נצעד צעד ימינה ונראה את הריבוע שהנוירון הכחול מסתכל עליו בשכבה שמתחתיו. באופן כזה נסרקת כל השכבה, אבל במעין חלון נע כזה שמתפקס כל פעם על פאץ' אחר.

עכשיו חדי העין מביניכם ודאי הבינו שאם לא יהיה שום שינוי שכבת הנוירונים מעל התמונה כבר לא תוכל להישאר בדיוק באותו גודל. באופן ספציפי אם השדה הרצפטיבי הוא כמו כאן 3 על 3, השכבה או התמונה הבאה תהיה "חסרה" פיקסל אחד מכל צד. וכדי שזה לא יקרה, נהוג לעשות גם zero padding, כלומר להוסיף מסביב לעבות את התמונה מסביב עם אפסים, כמה שצריך. כאן למשל צריך לעבות את התמונה עם שכבה אחת של אפסים.
:::
:::

---

### Filters/Features/Kernels

But what does the neuron actually *do*?

All neurons in a layer learn a single *filter* the size of their receptive field, the $f_h, f_w$ rectangle.

Suppose $f_h=f_w=3$ and the first layer learned the $W_{3x3}$ filter:

```{python}
W = np.array(
  [
    [0, 1, 0],
    [0, 1, 0],
    [0, 1, 0]
  ]
)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל מה זה כל נוירון "מסתכל", מה הוא עושה עם השדה הרצפטיבי הזה?

כל הנוירונים בשכבה לומדים איזשהו פילטר, או פיצ'ר, או קרנל, בגודל השדה הרצפטיבי שלהם. אם השדה הרצפטיבי שלהם למשל הוא 3 על 3 כמו שאמרנו אז בסוף הם ילמדו איזה פילטר W בגודל 3 על 3.

לדוגמא הפילטר הזה שכאן, W, שיש לו עמודה של אחדות באמצע ואפסים משני צדדיה.

למה אנחנו מסמנים את הפילטר בW? הW הזה הוא בעצם משקולות הרשת, זהו סט הפרמטרים שהיא לומדת, שהיא מעדכנת באמצעות גרדינט דיסנט לאור איזושהי פונקצית לוס.

הנוירונים בשכבת קונבולוציה יכפילו כל פאץ' שהם מסתכלים עליו עם הפילטר הזה ויסכמו למספר אחד - זה בדיוק לעשות קונבולוציה. בואו נראה דוגמא פשוטה.
:::
:::

---

$X$ is the 5x5 image, suppose it has a single color channel (i.e. grayscale), sort of a smiley:

```{python}
X = np.array(
  [
    [0,   1,  0,  1,  0],
    [0,   1,  0,  1,  0],
    [0,   0,  0,  0,  0],
    [1,   0,  0,  0,  1],
    [0,   1,  1,  1,  0]
  ]
)
```

Each nuron in the new layer $Z$ would be the sum of elementwise multiplication of all 3x3 pixels/neurons in its receptive field with $W_{3x3}$:

$Z_{i,j} = b + \sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}X_{i+u,j+v}W_{u,v}$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן יש לנו תמונה בגודל 5 על 5, מעין סמיילי. בשכבה הבאה Z, כל נוירון i, j, יהיה שווה להכפלה וסיכום של הפאץ' 3 על 3 שהוא מסתכל עליו עם הפילטר W.

וחוץ מהפילטר W, שנדגיש שוב כל הנוירונים בשכבה אחת משתמשים באותו אחד, נלמד גם חותך, או בשפה של רשתות נוירונים bias, מסומן כאן כb.
:::
:::

---

### What turns on this filter?

::: {.columns}
::: {.column width="30%"}
```{python}
X = np.array(
  [
    [0,   1,  0,  1,  0],
    [0,   1,  0,  1,  0],
    [0,   0,  0,  0,  0],
    [1,   0,  0,  0,  1],
    [0,   1,  1,  1,  0]
  ]
)
```
:::
::: {.column width="20%"}
```{python}
W = np.array(
  [
    [0, 1, 0],
    [0, 1, 0],
    [0, 1, 0]
  ]
)
```

:::
::: {.column width="30%"}
```{python}
Z = np.array(
  [
    [0,   2,  0,  2,  0],
    [0,   2,  0,  2,  0],
    [1,   1,  0,  1,  1],
    [1,   1,  1,  1,  1],
    [1,   1,  1,  1,  1]
  ]
)
```
:::

::: {.column width="20%"}
```{python}
#| echo: false

print()
```
:::

:::

::: {.fragment}
What low-level feature did this layer learn to look for? What pattern will make its neurons most positive (i.e. will "turn it on")?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז הנה התמונה, הסמיילי. ונניח שאנחנו עושים לה zero padding (להדגים).

והנה הפילטר W בנקודה הזאת. איך תיראה השכבה Z שמעל? לדוגמא הנוירון בקצה השמאלי העליון הוא תוצאה של מכפלה של W בפאץ' הכי שמאלי עליון (להדגים) ומתקבל אפס. וככה הפילטר שלנו או הקרנל הזה סורק את כל התמונה ומאחסן את ה"מסקנה" שלו בכל נוירון ונוירון בשכבה Z החדשה.

אם נסתכל על Z, התמונה החדשה שנוצרה, מתי היא הכי חיובית? מה הכי הדליק את הנוירון? אנחנו רואים שהערכים הכי גבוהים הם איפה שהיו העיניים של הסמיילי. כלומר הפילטר שלנו, שיש בו בעצם קו אנכי באמצע, "מגיב" הכי הרבה לקווים אנכיים, הוא סורק את התמונה ומחזיר ערכים גבוהים איפה שהוא מוצא מתאם עם הפרט המאוד מאוד עדין הזה.
:::
:::

---

### Another filter

::: {.columns}
::: {.column width="30%"}
```{python}
X = np.array(
  [
    [0,   1,  0,  1,  0],
    [0,   1,  0,  1,  0],
    [0,   0,  0,  0,  0],
    [1,   0,  0,  0,  1],
    [0,   1,  1,  1,  0]
  ]
)
```
:::
::: {.column width="20%"}
```{python}
W = np.array(
  [
    [0, 0, 0],
    [0, 0, 0],
    [1, 1, 1]
  ]
)
```

:::
::: {.column width="30%"}
```{python}
Z = np.array(
  [
    [1,   1,  2,  1,  1],
    [0,   0,  0,  0,  0],
    [1,   1,  0,  1,  1],
    [1,   2,  3,  2,  1],
    [0,   0,  0,  0,  0]
  ]
)
```
:::

::: {.column width="20%"}
```{python}
#| echo: false

print()
```
:::

:::

::: {.fragment}
What low-level feature did this layer learn to look for? What pattern will make its neurons most positive (i.e. will "turn it on")?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
והנה אותה תמונה של סמיילי עם פילטר W אחר, נסו להבין מה הפילטר הזה מחפש, למה הוא מגיב?

הפילטר הזה מגיב הכי הרבה לקווים אופקיים בתחתית הפאץ', כאן הוא נדלק ונותן ערך הכי גבוה 3, כשיש הוא מוצא מתאם מלא עם הפה של הסמיילי בתחתית התמונה.
:::
:::

---

### Convolving with Tensorflow

```{python}
import tensorflow as tf

ny = imread('images/new_york.jpg').mean(axis=2)
ny4D = np.array([ny.reshape(ny.shape[0], ny.shape[1], 1)])
W4d = W.reshape(W.shape[0], W.shape[0], 1, 1)

ny_convolved = tf.nn.conv2d(ny4D, W4d, strides=1, padding='SAME')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת כהרגלנו בקודש יכולנו לממש שכבת קונבולוציה בעצמנו. אבל הפעולה הזאת כל כך נפוצה שאפשר פשוט להשתמש בפונקציה conv2d ממודול tf.nn של טנסורפלואו.

אנחנו קוראים תמונה של שמי ניו יורק, הופכים אותה לשכבה אחת של שחור ולבן, ומוסיפים לה מימד אחד כי זה מה שהפונקציה רוצה לקבל. אנחנו לוקחים את הW האחרון שלנו שמחפש קווים אופקיים בתחתית הפאץ', מוסיפים לו 2 מימדים כי זה מה שהפונקציה רוצה לקבל.

ואז אנחנו מבקשים לעשות קונבולוציה עם הW שלנו על התמונה של ניו-יורק. אנחנו מזינים padding=same מה שאומר לטנסורפלואו להוסיף אפסים כדי לשמור על גודל התמונה, ומבקשים strides=1, שהפילטר שלנו ילך פיקסל פיקסל. האוביקט הסופי ny_convolved הוא בעצם שכבת הZ, התמונה החדשה אחרי קונבולוציה.
:::
:::

---

::: {.columns}
::: {.column width="50%"}

```{python}
plt.imshow(
  ny,
  cmap = 'gray')
plt.show()
```
:::

::: {.column width="50%"}
```{python}
plt.imshow(
  ny_convolved[0, :, :, 0],
  cmap = 'gray')
plt.show()
```
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אני מציג את התמונה המקורית בצד שמאל מול התמונה לאחר קונבולוציה מצד ימין. אני לא יודע אם רואים את זה דרך הוידאו, מוטב לעבור למצגת המקורית ולהשוות בין תמונות כמה שיותר גדולות. בבניין במרכז התמונה ניתן להראות את ההבדל בצורה הברורה ביותר, הפילטר שלנו עם הקו האנכי אכן מטשטש כלומר לא שם לב לקווים האנכיים של הבניין, והרבה יותר מדגיש את הקווים האנכיים.
:::
:::

---

## Making CNN Work {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Strides

In many CNN architectures layers tend to get smaller and smaller using *strides*:

![](images/conv05.png)

- The $[i,j]$ neuron looks at the rectangle at rows $i \cdot s_h$ to $i \cdot s_h + f_h - 1$, columns $j \cdot s_w$ to $j \cdot s_w + f_w - 1$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Stacking Feature Maps

A convolutional layer is actually a 3D stack of a few of the 2D layers we described (a.k.a *Feature Maps*), each learns a single filter.

![](images/conv06.png)

Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

- Feature map 1 learns horizontal lines
- Feature map 2 learns vertical lines
- Feature map 3 learns diagonal lines
- Etc.

And each such feature map takes as inputs all feature maps (or color channels) in the previous layer, and sums:

$Z_{i,j,k} = b_k + \sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}\sum_{k'=0}^{f_n-1}X_{i \cdot s_h+u,j \cdot s_w+v, k'} \cdot W_{u,v,k', k}$

Where $f_n$ is the number of feature maps (or color channels) in the previous layer.

- Feature map 1 with $f_h \cdot f_w$ filter $\cdot f_n$ color channels + 1 bias term (it's a filter *cube*!)
- Feature map 2 with $f_h \cdot f_w$ filter $\cdot f_n$ color channels + 1 bias term
- Etc.

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### And how does the network *learn* these filters?

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Too early to rejoice

That's still quite a lot.

- 100 x 100 RGB image
- 100 feature maps in first layer of filter 3x3
- (3 x 3 x 3 + 1) x 100 = 2800 params, not too bad
- But 100 x 100 numbers in each feature map (x 100) = 1M numbers, each say takes 4B, that's 4MB for 1 image for 1 layer
- Each number is a weighted sum of 3 x 3 x 3 = 27 numbers, so that's 1M x 27 = 27M multiplications for 1 layer...

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Pooling Layers

A pooling layer "sums up" a convolutional layer, by taking the max or mean of the receptive field.

No params!

![](images/conv07.png)

Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

Usually with strides $s_w=s_h=f_w=f_h$ and no padding (a.k.a `VALID`):

::: {.columns}
::: {.column width="30%"}
```{python}
X = np.array(
  [
    [0,   1,  0,  1,  0],
    [0,   1,  0,  1,  0],
    [0,   0,  0,  0,  0],
    [1,   0,  0,  0,  1],
    [0,   1,  1,  1,  0]
  ]
)
```

:::
::: {.column width="20%"}
```{python}
W = np.array(
  [
    [0, 0, 0],
    [0, 0, 0],
    [1, 1, 1]
  ]
)
```

:::
::: {.column width="30%"}
```{python}
Z = np.array(
  [
    [1,   1,  2,  1,  1],
    [0,   0,  0,  0,  0],
    [1,   1,  0,  1,  1],
    [1,   2,  3,  2,  1],
    [0,   0,  0,  0,  0]
  ]
)
```

:::

::: {.column width="20%"}
```{python}
Z2 = np.array(
  [
    [1, 2],
    [2, 3]
  ]
)
```

:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

Clearly we're losing info.

::: {.incremental}
- A 2x2 max pooling layer would reduce the size of the previous layer by how many percents?

- But maybe sometimes it's a *good* thing to ignore some neurons?

- Is CNN a linear operator?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Typical CNN

![](images/Typical_cnn.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Image Data Augmentation

It's very hard to "augment" data about people, products, clicks.

::: {.fragment}
Images are different.

- Translation
- Rotation
- Rescaling
- Flipping
- Stretching
:::
::: {.fragment}
Are there some images or applications you wouldn't want augmentations or at least should go about it very carefully?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

In Keras you can augment "on the fly" as you train your model, here I'm just showing you this works:

```{python}
from tensorflow.keras.preprocessing.image import ImageDataGenerator

idg = ImageDataGenerator(
	rotation_range=30,
	zoom_range=0.15,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.15,
	horizontal_flip=True,
	fill_mode='nearest'
)

ny_generator = idg.flow(ny4D, batch_size=1)

nys = [ny_generator.next() for i in range(10)]
```

See the [ImageDataGenerator](https://keras.io/api/preprocessing/image/) for more.

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| echo: false

width = 5
height = 2
i = 1
_ = plt.figure(figsize=(20, 10))
for _ in range(height):
	for _ in range(width):
		ax = plt.subplot(height, width, i)
		_ = ax.set_xticks([])
		_ = ax.set_yticks([])
		_ = plt.imshow(nys[i - 1][0, :, :, 0], cmap='gray')
		i += 1
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Why do CNN work?

- No longer a long 1D column of neurons, but 2D, taking into account spatial relations between pixels/neurons
- First layer learns very low-level features, second layer learns higher level features, etc.
- Shared weights --> learn feature in one area of the image, generalize it to the entire image
- Less weights --> smaller size, more feasible model, less prone to overfitting
- Less overfitting by Pooling
- Invariance by Max Pooling
- Hardware/Software advancements enable processing of huge amounts of images

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Back to Malaria! {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Reminder

```{python}
import tensorflow_datasets as tfds
from skimage.transform import resize
from sklearn.model_selection import train_test_split

malaria, info = tfds.load('malaria', split='train', with_info=True)

images = []
labels = []
for example in tfds.as_numpy(malaria):
  images.append(resize(example['image'], (100, 100)).astype(np.float32))
  labels.append(example['label'])
  if len(images) == 2500:
    break
  
X = np.array(images)
y = np.array(labels)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

print(X_train.shape)
print(X_test.shape)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| code-line-numbers: "|2|6-7|8|9|10|11|"
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential()
model.add(Conv2D(filters=32, input_shape=(X_train.shape[1:]),
  kernel_size=(3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
  optimizer='adam', metrics=['accuracy'])
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
model.summary()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| eval: false

callbacks = [EarlyStopping(monitor='val_loss', patience=5)]
history = model.fit(X_train, y_train, batch_size=100, epochs=50,
  validation_split=0.1, callbacks=callbacks)
```

```{python}
#| eval: false
#| echo: false
import pandas as pd

pd.DataFrame(history.history).to_csv('malaria_cnn_history.csv', index=False)

y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).reshape(y_test.shape)
np.save('y_pred.npy', y_pred)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| eval: false

import pandas as pd

pd.read_csv(history.history).plot(figsize=(10, 6))
plt.grid(True)
plt.show()
```

```{python}
#| echo: false

import pandas as pd

pd.read_csv('malaria_cnn_history.csv').plot(figsize=(10, 6))
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Last time we got to 69% accuracy after tuning...

```{python}
#| echo: false

from sklearn.metrics import confusion_matrix

y_pred = np.load('y_pred.npy')
```

```{python}
#| eval: false

from sklearn.metrics import confusion_matrix

y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).reshape(y_test.shape)
```

```{python}
np.mean(y_pred == y_test)
```

```{python}
pd.DataFrame(
  confusion_matrix(y_test, y_pred), 
  index=['true:yes', 'true:no'], 
  columns=['pred:yes', 'pred:no']
)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### And this is just 10% of the data.

Watch me reach ~94% accuracy with 100% of the data in [Google Colab](https://colab.research.google.com/drive/1i39eWL8e5Gl3rjoulh5WJMVOUgvM8h6_?usp=sharing).

::: {.fragment}
![](images/malaria_cnn_history_100pct.png){width="60%"}
:::
::: {.fragment}
Think about the researchers seeing this for the first time in 2012...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Visualizing CNN {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Visualizing filters/kernels/features/weights

```{python}
#| echo: false

W = np.load('W.npy')
```

```{python}
#| eval: false

W, b = model.get_layer('conv2d').get_weights()
```

```{python}
W.shape
```


```{python}
W[:, :, 0, 0]
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

First filter of 32:

```{python}
plt.subplot(1,3,1)
plt.imshow(W[:, :, 0, 0], cmap='gray')
plt.subplot(1,3,2)
plt.imshow(W[:, :, 1, 0], cmap='gray')
plt.subplot(1,3,3)
plt.imshow(W[:, :, 2, 0], cmap='gray')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Visualizing Feature Maps

```{python}
cell = X_test[0, :, :, :].reshape(1, 100, 100, 3)

plt.imshow(cell[0])
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| eval: false

from tensorflow.keras import Model

model_1layer = Model(inputs=model.inputs,
  outputs=model.layers[0].output)

feature_maps = model_1layer.predict(cell)

feature_maps.shape
```

```{python}
#| echo: false

feature_maps = np.load('feature_maps.npy')

print(feature_maps.shape)
```

```{python}
#| eval: false

width = 8
height = 4
ix = 1
_ = plt.figure(figsize = (10, 10))
for _ in range(height):
	for _ in range(width):
		ax = plt.subplot(height, width, ix)
		_ = ax.set_xticks([])
		_ = ax.set_yticks([])
		_ = plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')
		ix += 1
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| echo: false

width = 8
height = 4
ix = 1
_ = plt.figure(figsize = (10, 10))
for _ in range(height):
	for _ in range(width):
		ax = plt.subplot(height, width, ix)
		_ = ax.set_xticks([])
		_ = ax.set_yticks([])
		_ = plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')
		ix += 1
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## CNN Architectures {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### ImageNet

![](images/ImageNet.png)

Source: [paperswithcode.com](https://paperswithcode.com/sota/image-classification-on-imagenet)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LeNet-5 (1998)

![](images/lenet-5.png)

Source: [LeCun et. al. 1998](http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf)

You know you can implement this in just a few lines of Keras...

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| eval: false

from tensorflow.keras.layers import Conv2D, AveragePooling2D,
  Flatten, Dense
from tensorflow.keras import Sequential

model = keras.Sequential()

model.add(Conv2D(filters=6, kernel_size=(3, 3),
  activation='relu', input_shape=(32,32,1)))
model.add(AveragePooling2D())
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(120, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation = 'softmax'))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### AlexNet (2012)

![](images/alexnet.png)

Source: [Krizhevsky et. al. 2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

See a Keras implementation e.g. [here](https://medium.datadriveninvestor.com/alexnet-implementation-using-keras-7c10d1bb6715)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### ResNet (2015)

::: {.columns}

::: {.column width="50%"}
![](images/resnet.png){width="20%"}
:::

::: {.column width="50%"}
- Source: [He et. al.](https://arxiv.org/pdf/1512.03385.pdf)
- This is ResNet-34 (34 layers)
- ResNet-152 (152 layers!) achieved 4.5% Top-5 error rate with single model
:::

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Residual Learning

![](images/ru.png){width="20%"}

- "Is learning better networks as easy as stacking more layers?" Yes, but:
- the vanishing/exploding gradients problem
- With *Residual Learning* the activation from a previous layer is being added to the activation of a deeper layer in the network
- The signal is allowed to propagate through the layers

::: {.fragment}
- The model learning $H(X)$ will be forced to learn $H(X) - X$, hence *residual* learning.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Using Pre-trained Models {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What *is* a trained CNN?

![](images/keras_applications.png)

Look up [keras.io/api/applications/](https://keras.io/api/applications/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### ResNet on your local machine

```{python}
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from skimage.transform import resize

model = ResNet50(weights='imagenet')

johann = imread('images/johann.jpg')

fig, ax = plt.subplots(figsize=(7, 4))
_ = ax.imshow(johann)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
johann = resize(johann, (224, 224))
johann = johann.reshape(1, 224, 224, 3) * 255
johann = preprocess_input(johann)

johann_pred = model.predict(johann, verbose=0)

print(johann_pred.shape)
```

```{python}
#| output-location: fragment

johann_breed = decode_predictions(johann_pred, top=5)

for _, breed, score in johann_breed[0]:
  print('%s: %.2f' % (breed, score))
```

::: {.fragment}
Look mom, no training!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What's Next?

This was simple image classification. Furthermore you'd see:

- Transfer Learning
- Object Detection
- Segmentation
- Captioning
- 3D Reconstruction
- Restoration
- Pose Estimation
- Generative models (e.g. GANs)
- Doing it all on Video
- And doing it all in real time, on your phone or in your car &#x1F600;

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
