---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Ensemble Methods: Random Forest and Boosting"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Ensemble Methods: Random Forest and Boosting - Class 11

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Random Forest {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ensemble methods: using trees as subroutines

Instead of a single tree being a model, combine many trees into a model:

1. Bagging and Random Forest: Fit different trees to the data and average them
2. Boosting: Adaptively build a model from adding more and more trees

::: {.fragment}
- We will focus now on Random Forest (also Bagging), later discuss boosting

- Main idea of Random Forest: Take advantage of the instability and high variance of the trees

- Trees are unstable and greedy: if we change the data a little bit, the tree can change a lot

- Now we intentionally change (randomize) the data to get a different tree every time, and average them
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ביחידה הזאת נלמד על שיטות אנסמבל, שעושות קומבינציה של מודלים חלשים רבים, למודל חזק במיוחד. אמרנו כבר כשלמדנו על עצי החלטה שהם מודלים מוגבלים עם שונות גבוהה, ושאלנו את עצמנו -- למה עץ אחד? למה לא יער?

נראה היום שתי שיטות אנסמבל מבוססות עצים. הראשונה רנדום פורסט, שמבוססת על מיצוע של הרבה עצים שונים, והשנייה בוסטינג שבה אנחנו בונים עץ אחרי עץ בצורה אדפטיבית.

נתחיל ברנדום פורסט. במקום להתאים עץ אחד לנתונים, אנחנו נתאים הרבה. אבל לא נתאים אותם לאותם הנתונים, אחרת אין הבדל. נתאים אותם כל פעם על דאטא קצת אחר, דאטא שעבר רנדומיזציה, בשתי דרכים שונות. לבסוף נמצע את העצים -- החיזוי לכל תצפית יהיה ממוצע שלה על פני הרבה עצים, ונראה שכך נטפל באופן ישיר בבעיות של העץ היחיד.
:::
:::

---

### Reminder: the value of averaging

- This is captured through different things we learned: CLT, LLN, variance of the average...

- Assume $z_i \sim F$ has some distribution with mean $\mu$ and variance $\sigma^2$

- If $z_1,\dots,z_m \sim F$ are independent, then $Var(\bar{z}) = \sigma^2 / m$, so $\bar{z}$ is close to $\mu$ for large $m$ 

::: {.incremental}
- What if $z_1,\dots,z_m$ are dependent?

- Slightly more complex setting: assume $z_1,\dots,z_m$ are *somewhat* dependent $Cov(z_i,z_j) = \rho \sigma^2,\;\rho<1$

- Now we still get some variance reduction from averaging: 
$$Var(\bar{z}) \approx \rho\sigma^2 + (1-\rho)\sigma^2/ m$$

- This is exactly the intuition behind Random Forest
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר בערך המיצוע. מה מיצוע נותן לנו? נתקלנו בזה בעבר, כשדיברנו על משפט הגבול המרכזי. אם משתנה מקרי Z_i, מתפלג לפי איזושהי התפלגות F עם תוחלת מיו ושונות סיגמא בריבוע, ואני לוקח m תצפיות כאלה בלתי תלויות, אז השונות של ממוצע המדגם המקרי קטנה פי m. כלומר ככל שm גדול כך הפיזור סביב הממוצע קטן והוא מתקרב לתוחלת האמיתית מיו.

ונניח שהתצפיות הן לא בלתי תלויות. לא רק שהן לא בלתי תלויות, הן תלויות לחלוטין, הן אותה תצפית בדיוק, שחוזרת על עצמה m פעמים. מה יהיה אז הממוצע? התצפית עצמה כמובן. והאם הקטנו את השונות של ההתפלגות המקורית? בכלל לא, נישאר עם השונות המקורית סיגמא בריבוע. כלומר יש כאן איזשהו טווח מתצפיות בלתי תלויות לחלוטין ועד תצפיות תלויות לחלוטין, וההקטנה של סיגמא בריבוע בהתאם.

נסתכל על מצב ביניים, שהתצפיות לא בלתי תלויות לחלוטין אבל גם לא חוזרות על עצמן, המתאם בין זוג תצפיות הוא איזשהו רו שקטן מ1, כלומר הקווריאנס יהיה סיגמא בריבוע כפול רו.

אפשר לראות שכעת שונות ממוצע המדגם היא בקירוב רו סיגמא בריבוע, ועוד 1 מינוס רו כפול סיגמא בריבוע חלקי m. זאת אומרת כשרו שווה ל1, תלות מושלמת, אנחנו נשארים עם סיגמא בריבוע השונות המקורית, וכשרו שווה לאפס, שזה אומר תצפיות בלתי תלויות, נקבל את סיגמא בריבוע חלקי m, שונות מדגם מקרי המוכרת לנו.

זו האינטואיציה מאחורי למה רנדום פורסט עובד. אם אצליח לקחת עוד ועוד דגימות עם כמה שפחות תלות - במקרה שלנו עוד ועוד עצים, נקטין את השונות המקורית של כל אחת מהן עד פי m. אם הדגימות שלי תלויות חזק אחת בשניה, הרווח שלי מוגבל מפעולת המיצוע. נרצה אם ככה לייצר עצים שיהיו שונים כמה שיותר אחד מהשני כך שנרוויח מהמיצוע שלהם.

:::
:::

---

### Random forest algorithm

- Repeat many times: 
1. Randomize the data (by taking a subsample or a **bootstrap** sample)
2. Build a tree on the randomized data, also randomize tree building (e.g. by randomly choosing variables to consider at each node)

::: {.fragment}
- To predict at new $x_0$, apply each tree and average their predictions
:::
::: {.fragment}
- Intuition: trees are different because of randomization, they are like $z_1,...z_n \stackrel{\cdot}{\sim} P(y|x_0)$
1. Related ($\rho > 0$) because it's the same training set $T$
2. Still different from each other ($\rho < 1$) because of randomization and instability of trees
:::
::: {.fragment}
- Hence we expect (and indeed see!) that Random Forest gives more accurate predictions of $E(y|x)$ or $P(y=1|x)$ than single trees
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך נשיג את היער עצים הזה ששונים זה מזה כמה שיותר?

נזריק רנדומיזציה לתהליך: כל עץ יראה דאטא קצת אחר, נהוג לקחת רק חלק מהנתונים, subsample, או מדגם בוטסטראפ, שזה מדגם בגודל m המקורי, עם החזרה. דבר שני שנעשה, תוך כדי בנית העצים על הדאטא הזה, זה בכל צומת נגריל מספר מסוים של משתנים שיהיו מועמדים לפיצול. כלומר אם בעץ המקורי בכל צומת הוא מתחשב בכל המשתנים האפשריים, העצים שלנו עשויים לראות בכל צומת משתנים אחרים לחלוטין.

כעת מגיעה תצפית חדשה לחיזוי. מה זה אומר למצע עצים? זה אומר שנריץ אותה בכל העצים, והחיזוי הסופי שלה יהיה הממוצע שלהם.

על האינטואיציה דיברנו בהרחבה, העצים הם כמו תצפיות ממדגם. הם לא יכולים להיות לגמרי בלתי תלויים כי הם בכל זאת מבוססים על אותו דאטא. אבל נדאג שיהיו כמה שפחות תלויים אחד בשני, וככה נרוויח מהמיצוע שלהם.

אילו עצים נגדל? עמוקים או שטוחים? עמוקים כמובן! עצים עמוקים שמסוגלים לתאר יחסים מורכבים כמה שניתן. לעצים כאלה תהיה שונות גבוהה שנקטין עם המיצוע. אם נבחר בעצים שטוחים יותר, נתחיל אולי בטעות פחות גבוהה אבל גם לא נרוויח מספיק מהמיצוע. למה שלא נראה את רנדום פורסט בפעולה על הנתונים שלנו.
:::
:::

---

## RF in Action {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### RF for Netflix (Regression)

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
from sklearn.model_selection import train_test_split

ratings = pd.read_csv('../datasets/netflix/train_ratings_all.csv', header = None)
miss_cong = pd.read_csv('../datasets/netflix/train_y_rating.csv', header = None, names = ['score'])
movies = pd.read_csv('../datasets/netflix/movie_titles.csv', header = None, names = ['year', 'title'])

netflix_X = ratings.iloc[:, :14]
netflix_X.columns = movies['title'][:14]
netflix_Y = miss_cong.iloc[:, 0]

NE_Xtr, NE_Xte, NE_Ytr, NE_Yte = train_test_split(netflix_X, netflix_Y, test_size=0.2, random_state=42)
```
```{python}
#| code-line-numbers: "|1|7|10-12|13-14|15-16|"

from sklearn.ensemble import RandomForestRegressor

ntr = NE_Xtr.shape[0]
nte = NE_Xte.shape[0]
tr_err = []
te_err = []
ntrees = [1, 10, 50, 100, 500, 1000]

for ntree in ntrees: 
    RF = RandomForestRegressor(n_estimators=ntree, min_samples_split=2,
        min_samples_leaf=1, max_features=0.33,  bootstrap=True)
    RF = RF.fit(NE_Xtr, NE_Ytr)
    yhat_tr = RF.predict(NE_Xtr)
    yhat = RF.predict(NE_Xte)
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat - NE_Yte)**2) / nte))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה את רנדום פורסט על נתוני נטפליקס. הקלאס בשביל רגרסיה נקרא RandomForestRegressor. נגדיל את מספר העצים מאחד ועד אלף. בכל איטרציה נגדל ntree עצים, מאוד עמוקים. נדאג שיהיו מאוד עמוקים במיוחד על-ידי זה שנפרט עם הפרמטר min_samples_split שהמינימום תצפיות לביצוע ספליט הוא קטן, אפשר לבצע ספליט אפילו על שתי תצפיות. עוד פרמטר שכדאי להזכיר הוא הmax_features, ששולט על כמה משתנים יהיו מועמדים בכל פיצול, כאן אנחנו מזינים 0.33 כלומר שליש מהמשתנים, אפשר גם להזין מספר שלם כלשהו, ברירת המחדל הוא לקחת את שורש מספר המשתנים. לבסוף נחזה על מדגם הטריין והטסט ונשרשר את הRMSE לרשימה.
:::
:::

---

```{python}
fig = plt.figure(figsize=(4, 4))
ax = fig.add_subplot(1,1,1)
ax.set_xscale('log')
plt.plot(ntrees, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(ntrees, te_err, color='navy', lw=2, label='test')
plt.ylim([0.3, 1.3])
plt.xlabel('Number trees')
plt.ylabel('RMSE')
plt.title('RF on Netflix')
plt.legend(loc="upper right")
plt.show() 
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מציירים את הטריין והטסט ארור כנגד מספר העצים אנחנו רואים שמעט עצים עמוקים מגיעים לשגיאה די גבוהה על הטסט, יותר מ1. אלף עצים לעומת זאת מגיעים כבר לאיזור ה0.8. הדבר הכי מעניין אולי בגרף הזה זה ששגיאת החיזוי לא עולה שוב, היא לא מתדרדרת, ואנחנו יודעים בדיוק למה, השונות יכולה רק לקטון. כך שעוד יתרון של רנדום פורסט הוא שאין התלבטות לגבי מספר העצים, אנחנו מוגבלים רק על ידי כוח החישוב שלנו וגודל המודל הסופי על הדיסק, ככל שנאפשר יותר עצים נצפה לשגיאה קטנה יותר. ברור שמאיזשהו מספר עצים כמו שרואים כאן לא בטוח שיש לאן לרדת.
:::
:::

---

#### Let's try it bravely on the full 99 dimensions! 

```{python}
from sklearn.tree import DecisionTreeRegressor

ratings[np.isnan(ratings)] = 0
NE_Xtr_noNAN, NE_Xte_noNAN = train_test_split(ratings, test_size=0.2, random_state=42)
```

A single tree can only get you so far:

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ננסה לעבוד עם כל הנתונים שבידינו, כל 99 הסרטים כך שתצפית חסרה היא בעצם דירוג אפס.

נראה קודם מה נותן עץ יחיד.
:::
:::

---

```{python}
tr_err = []
te_err = []
ds = [2, 3, 5, 7, 10, 15]

for depth in ds:
    Netree = DecisionTreeRegressor(max_depth=depth)
    Netree = Netree.fit(NE_Xtr_noNAN, NE_Ytr)
    yhat_tr = Netree.predict(NE_Xtr_noNAN)
    yhat = Netree.predict(NE_Xte_noNAN)
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat - NE_Yte)**2) / nte))
```
```{python}
#| echo: false

plt.figure(figsize=(4, 4))
plt.plot(ds, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(ds, te_err, color='navy', lw=2, label='test')
plt.ylim([0.3,1.3])
plt.xlabel('depth')
plt.ylabel('RMSE')
plt.title('Trees on Netflix -- 99 features')
plt.legend(loc="upper left")
plt.show()
```
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו משתמשים בעץ יחיד ופשוט משנים את הmax_depth שלו כמו שעשינו. עם עץ בודד ו14 סרטים הגענו לRMSE של 0.85, כאן אנחנו רואים כבר RMSE של 0.82.
:::
:::

---

```{python}
tr_err = []
te_err = []
ntrees = [1, 10, 50, 100, 500, 1000]

for ntree in ntrees: 
    RF = RandomForestRegressor(n_estimators=ntree, min_samples_split=5,
        min_samples_leaf=2, max_features=10, bootstrap=True)
    RF = RF.fit(NE_Xtr_noNAN, NE_Ytr)
    yhat_tr = RF.predict(NE_Xtr_noNAN)
    yhat = RF.predict(NE_Xte_noNAN)
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat-NE_Yte)**2) / nte))
```

```{python}
#| echo: false

fig = plt.figure(figsize=(4, 4))
ax = fig.add_subplot(1,1,1)
ax.set_xscale('log')
plt.plot(ntrees, tr_err, color='darkorange', lw=2, label='train')
plt.plot(ntrees, te_err, color='navy', lw=2, label='test')
plt.ylim([0.3,1.3])
plt.xlabel('Number trees')
plt.ylabel('RMSE')
plt.title('RF on Netflix -- 99 features')
plt.legend(loc="upper right")
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו עושים רנדום פורסט, השגיאה של RMSE על הטסט סט, יורדת כבר לאיזור ה0.78! ושוב אם נסתכל על עץ בודד עמוק, השגיאה שלו גבוהה מאוד, זה רק המיצוע של עצים כאלה שמביא אותנו לתוצאה איכותית.
:::
:::

---

### Summary of Random Forest 

- Uses advantages of trees, mitigates their shortcomings

- RF trees should be as different as possible from each other: 
    1. Uses the high-variance property of trees
    2. Add randomization: subsampling of training data for each tree; randomizations in tree splitting

- Add diversity by making trees bigger, control variance by averaging, therefore: 
    1. Trees should be as big as possible
    2. Should build and average as many of them as computationally possible

::: {.fragment}
- Great advantages for "big data": highly parallelizable and (almost) hyperparametr free!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם: שיטת רנדום פורסט משמרת את הגמישות של עצים תוך כדי שהיא מנסה להפחית את החסרון הכי גדול שלם, הנוקשות והשונות הגדולה שלהם.

אנחנו עושים את העצים כמה שיותר שונים זה מזה, על-ידי מדגמי בוטסטראפ ובחירת משתנים שונים כמועמדים לכל פיצול.

ומלבד זה אנחנו דואגים שהעצים יהיו עמוקים כמה שאפשר, כדי שנרוויח כמה שיותר מאפקט המיצוע, מעץ בודד עם איכות חיזוי גרועה להרבה עם איכות חיזוי טובה.

 עקרונית גם אמרנו, שככל שנבנה יותר עצים איכות החיזוי על הטסט סט יכולה רק לקטון, יתרון משמעותי לשיטה, בפועל אנחנו כנראה מוגבלים על-ידי כוח חישוב וגם גודל על הדיסק, כל אחד מהעצים האלה יכול להיות אוביקט די גדול, אלף עצים לשמור על שרתים זה כבר לא סימפטי.

 עוד יתרון שאנחנו פחות עוסקים בקורס הזה אבל הוא קריטי: קל למקבל רנדום פורסט על-פני מספר מחשבים? קל מאוד! כל עץ ברנדום פורסט יכול לגדול באופן בלתי תלוי מהאחרים, לכן אם הנתונים גדולים ועומדת לרשות מדען הנתונים סביבת עבודה מבוזרת, קלאסטר של מספר מחשבים, ניתן להגיע לאימון מהיר מאוד של האלגוריתם. ויתרון אחרון שרמזנו אליו - כמעט בכל שיטה שאנחנו לומדים יש היפרפרמטרים, איזשהם כפתורים שצריך לסובב כדי להתאים את האלגוריתם למקרה שלנו, כמו מספר השכנים בKNN או מטריקת המרחק. בסך הכל ברנדום פורסט אין פרמטרים שיש עליהם סימן שאלה, ברור שאנחנו צריכים כמה שיותר עצים וברור שהם צריכים להיות כמה שיותר עמוקים. זה הופך את רנדום פורסט לאלגוריתם אוף-דה-שלף מאוד פופולרי, כי בלי כיוונון אפשר להגיע מהר לתוצאה מצוינת.
:::
:::

---

## Boosting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נדבר כעת על בוסטינג. בוסטינג הוא גם כן אנסמבל של מודלים פשוטים אבל הוא שונה לחלוטין באופן שבו אנחנו מגדלים את התת-מודלים ובאופן שבו אנחנו ממצעים אותם. בוסטינג לא חייב להיות מבוסס על עצים, אבל בפועל זה המימוש הפופולרי ביותר.
:::
:::

---

### Boosting: intuitive idea

- We gradually and iteratively build the overall model as a sum of smaller models called *weak learners*

- Each weak learner seeks to improve the model we have so far

- Weak learners can be any predictive model, most widely used: trees

- How do we capture the notion of *improve the model we have so far*?

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל ברמה האינטואיטיבית: אנחנו נבנה מודל די מורכב, אבל לא בבת אחת. בהדרגה. נבנה מודל פשוט. ואז נשפר אותו באמצעות מודל פשוט נוסף. ואז נשפר את הביצועים של השניים שיש לנו, עם מודל פשוט שלישי. וכך הלאה. בצורה הזאת יש לי מודל שהוא קומבינציה של הרבה מודלים פשוטים, weak learners, והמודל הזה הוא כבר לא כל כך פשוט.

לדוגמא, עצים. מה הופך עת למודל "פשוט" או weak learner? אם נעשה אותו לא עמוק, עץ יחסית שטוח. עץ בעומק 1 זה בעצם לשאול שאלה אחת על התצפית, עץ בעומק 2 זה אומר לשאול 2 שאלות, לא יכול להיווצר מזה מודל מורכב.

ואיך זה שונה מרנדום פורסט? ברנדום פורסט הסתכלנו על עצים "חזקים" לא "חלשים", וכל עץ באופן בלתי תלוי לחלוטין משאר העצים, אמרנו גם שאפשר לגדל אותם במקביל. כאן אי אפשר לעשות את זה, העץ השני יהיה חייב לדעת מה הביצועים של העץ הראשון כדי לשפר אותו.

השאלה העיקרית היא איך נדאג שהמודל הפשוט הבא, העץ השטוח הבא, ישפר את מה שקדם לו?
:::
:::

---

### Boosting: overall scheme

1. Initialize $F^{(0)}(x) = 0,\; \forall x$
2. At stage $t \geq 1$:<br>    
    a. Calculate $Y^{(t)} = (y_1^{(t)},\ldots,y_n^{(t)})$ capturing what the model $F^{(t-1)}$ **has not yet explained**<br>
    b. Fit a weak learner $\hat{f}^{(t)}$ to $T^{(t)} = (X,Y^{(t)})$<br>
    c. Update $F^{(t)} = F^{(t-1)} + \epsilon \hat{f}^{(t)}$

::: {.fragment}
**Details: How to determine $Y^{(t)}$? Which weak learner to use? What is $\epsilon$?**
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה כעת את האלגוריתם של בוסטינג במבט על.

נתחיל בחיזוי F0 בסיסי זהה לכל התצפיות, אפשר לחזות אפס או ממוצע.

כעת בשלב t, נגדיר את הוקטור y_t להיות לא הוקטור המקורי של התצפיות שלנו, אלא וקטור שתופס באיזשהו מובן את מה שהמודל עד כה לא הצליח לתפוס. איך נעשה את זה עוד לא אמרנו. והמודל עד כה הוא יסומן בF_t-1. בשלב t = 1 זה כמובן המודל הלא אינפורמטיבי F0 שבהגדרה חוזה אפס לכל התצפיות.

עכשיו בשלב הt זהו מדגם הלמידה שלנו, מטריצת הX המקורית, והוקטור y_t שאמרנו שתופס את מה שלא הצלחנו למדל עד כה. ועל הדאטא הזה נבנה את העץ הפשוט, הוויק לרנר, f_hat_t. כלומר בשלב הטי העץ הפשוט ינסה להתאים את עצמו כמו שיותר לy_t, לא לy המקורי.

בשלב האחרון, אנחנו מעדכנים את המודל עד כה עם המודל החדש, f_hat_t.  המודל החדש Ft יהיה המודל עד שלב t-1, ועוד איזשהו קבוע אפסילון קטן כפול המודל החדש f_hat_t. ברגרסיה הכוונה בעצם לקחת את החיזוי של Y לכל תצפית עד כה ולהוסיף את החיזוי החדש כפול משקולת קטנה.

אז זה המודל בהיי-לבל, נשאלת השאלה באיזה מודל פשוט להשתמש (ואמרנו שנשתמש בעץ שטוח), איך לקבוע את הקבוע אפסילון, אבל הכי מעניין איך להחליט מהו וקטור הy_t שאמור לבטא מה שהמודל עד כה לא הצליח לקלוט. ברגרסיה למשל, יש לנו Y מקורי ו-Y חזוי עד כה. איך נבטא את "מה שהמודל לא הצליח לחזות"? באמצעות השארית!
:::
:::

---

### Example: Tree boosting for regression

- Defining $Y^{(t)}$ as $y_i^{(t)} = (y_i - F^{(t-1)}(x_i))$ the current residual (what the model does not explain)

- Weak learner: trees, usually small --- two- or three-level trees as $\hat{f}^{(t)}$

- Make $\epsilon$ as small as possible ($\epsilon$-boosting): tradeoff between accuracy and computation

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז ברגרסיה באמת וקטור y_t שנמדל בכל שלב t יהיה השארית: התצפית הi פחות החיזוי שלה עד כה F_t-1 הi.

העץ שלנו יהיה בדרך כלל בעומק 2-3, תיכף נדגים כמה זה חשוב.

והאפסילון, נרצה שיהיה כמה שיותר קטן. מדוע? אפשר לראות באפסילון קצב הלמידה שלנו. למדתי מודל, ואני לוקח אותו בערבון מוגבל, אני מוסיף אותו לממוצע עם משקולת קטנה וממשיך הלאה. תיכף נעשה את זה ברור יותר אני מקווה.
:::
:::

---

#### Boosted trees: depth 2

```{python}
#| code-line-numbers: "|6|7-9|11|12-13|14-15|16-17|20|"

ntr = NE_Xtr_noNAN.shape[0]
nte = NE_Xte_noNAN.shape[0]
tr_err = []
te_err = []

Ytr_now = NE_Ytr
yhat_tr = np.zeros(ntr)
yhat_te = np.zeros(nte)
eps = 0.05

for iter_num in range(200):
    tree = DecisionTreeRegressor(max_depth = 2)
    tree.fit(NE_Xtr_noNAN, Ytr_now)
    yhat_tr_now = tree.predict(NE_Xtr_noNAN)
    yhat_te_now = tree.predict(NE_Xte_noNAN)
    yhat_tr += eps * yhat_tr_now
    yhat_te += eps * yhat_te_now    
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat_te - NE_Yte)**2) / nte))
    Ytr_now = NE_Ytr - yhat_tr
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ננסה לממש בוסטינג של עצים לרגרסיה בעצמנו על הנתונים של נטפליקס עם כל 99 הסרטים, סך הכל מדובר במתכון די פשוט. אחר כך נראה את הקלאס הרלוונטי מsklearn.

נקבע את Ytr_now להיות Y הנוכחי.

בתור הוקטור yhat_tr ו-yhat_te נשים אפסים, זה מה שאנחנו חוזים לכל התצפיות. נאתחל גם את אפסילון להיות ערך קטן 0.05.

נחזור על הפרוצדורה שלנו 200 פעם כלומר נבנה 200 עצים.

כל פעם נבנה עץ שטוח בעומק 2, ונמדל לא את Y המקורי אלא את Ytr_now.

נחזה על הטריין ועל הטסט, ואת החיזוי שלנו נוסיף כפול אפסילון לוקטורי החיזוי עד כה.

מה נשאר לנו? לעדכן את Ytr_now, זה צריך להיות השארית בין Y המקורי, לבין הY החזוי עד כה, על מדגם הלמידה.
:::
:::

---

```{python}
plt.figure(figsize=(4, 3))
plt.plot(range(200), tr_err, color='darkorange', lw=2, label='train' )
plt.plot(range(200), te_err, color='navy', lw=2, label='test')
plt.ylim([0.3, 1.3])
plt.xlabel('iteration')
plt.ylabel('RMSE')
plt.title('Tree boosting on Netflix - depth 2')
plt.legend(loc="upper right")
plt.show() 

print(f'test RMSE iter 0: {te_err[0]: .2f}, iter 10: {te_err[10]:.2f}, iter 100: {te_err[100]:.2f}, iter 199: {te_err[199]:.2f}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מציירים את טעות החיזוי הRMSE על הטריין ועל הטסט אנחנו רואים כצפוי ירידה ככל שנוספים עוד עצים עד איזושהי אסימפטוטה.

אחרי העץ ה200 כבר רואים RMSE 0.77-0.78 ואנחנו זוכרים שלנתונים האלה זאת תוצאה איכותית מאוד. עוד תופעה מעניינת כאן היא שבניגוד למודלים אחרים שראינו יש הבדל קטן מאוד בין הטריין לטסט בטעות החיזוי. למה זה קורה? זה קורה משום שאנחנו מראש בונים עצים יחסית שטוחים, עצים שלא  עושים overfitting לנתונים, לכן המודל הכללי של בוסטינג נוטה לעשות הרבה פחות overfitting.

נקודה אחרונה שהייתי רוצה להדגיש: הרבה חושבים שהעובדה שאפסילון זהה לכל העצים אומרת שאנחנו נותנים משקל זהה לכולם. קודם כל יש גירסאות לבוסטינג בהן נשתמש באפסילון שונה, כמו בכל אופטימיזציה ניתן לבחור את קצב הלמידה בצורה אדפטיבית. אבל אפילו אם הוא זהה, האם באמת מדובר במשקולת שווה לכל העצים? לא ממש. כי העץ הראשון יחזה כמות יחסית גדולה, את Y המקורי, וכל עץ שיבוא אחריו יחזה שארית שתלך ותיעשה קטנה יותר. אז אולי כל עץ מוכפל פי אפסילון אבל ההשפעה שלהם במודל הגדול שונה לגמרי, העצים הראשונים ישפיעו הרבה יותר על החיזוי מהעץ ה200.
:::
:::

---

#### Boosted trees: depth 3

```{python}
#| code-line-numbers: "|11|"
tr_err = []
te_err = []

Ytr_now = NE_Ytr

yhat_tr = np.zeros(ntr)
yhat_te = np.zeros(nte)
eps = 0.05

for iter_num in range(200):
    tree = DecisionTreeRegressor(max_depth = 3)
    tree.fit(NE_Xtr_noNAN, Ytr_now)
    yhat_tr_now = tree.predict(NE_Xtr_noNAN)
    yhat_te_now = tree.predict(NE_Xte_noNAN)
    yhat_tr += eps * yhat_tr_now
    yhat_te += eps * yhat_te_now    
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat_te - NE_Yte)**2) / nte))
    Ytr_now = NE_Ytr - yhat_tr
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מהו ה"כפתור" העיקרי שלנו כשאנחנו מריצים מודל בוסטינג? עומק העץ. מתבקש לנסות להעמיק כאן את העץ, ההבדל היחיד הוא שאני מנסה עומק מקסימלי של 3. עדיין עצים שטוחים יחסית.
:::
:::

---

```{python}
plt.figure(figsize=(4,3))
plt.plot(range(200), tr_err, color='darkorange', lw=2, label='train' )
plt.plot(range(200), te_err, color='navy', lw=2, label='test')
plt.ylim([0.3, 1.3])
plt.xlabel('iteration')
plt.ylabel('RMSE')
plt.title('Tree boosting on Netflix - depth 3')
plt.legend(loc="upper right")
plt.show() 

print(f'test RMSE iter 0: {te_err[0]: .2f}, iter 10: {te_err[10]:.2f}, iter 100: {te_err[100]:.2f}, iter 199: {te_err[199]:.2f}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
זה נראה בהתחלה שאין הבדל בין עומק 2 לעומק 3, אבל אם תשוו את הפרופילים האלה אחד ליד השני תראו שעקומת הטריין עם עומק 3 שונה הרבה יותר מעקומת הטסט, היא נמוכה יותר, כלומר יש יותר אוברפיטינג. מעניין גם שכאן אנחנו מגיעים לתוצאה האיכותית של RMSE 0.77 כבר אחרי 100 עצים.

אז מה יקרה עם נעשה בוסטינג עם עצים של רנדום פורסט, עצים עמוקים מאוד? ננסה!
:::
:::

---

#### Boosted trees: depth 15 
```{python}
#| code-line-numbers: "|11|"
tr_err = []
te_err = []

Ytr_now = NE_Ytr

yhat_tr = np.zeros(ntr)
yhat_te = np.zeros(nte)
eps = 0.05

for iter_num in range(100):
    tree = DecisionTreeRegressor(max_depth = 15)
    tree.fit(NE_Xtr_noNAN, Ytr_now)
    yhat_tr_now = tree.predict(NE_Xtr_noNAN)
    yhat_te_now = tree.predict(NE_Xte_noNAN)
    yhat_tr += eps * yhat_tr_now
    yhat_te += eps * yhat_te_now    
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat_te - NE_Yte)**2) / nte))
    Ytr_now = NE_Ytr - yhat_tr
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו חוזרים על אותה סימולציה עם עומק מקסימלי 15.
:::
:::

---

```{python}
plt.figure(figsize=(4, 3))
plt.plot(range(100), tr_err, color='darkorange', lw=2, label='train' )
plt.plot(range(100), te_err, color='navy', lw=2, label='test')
plt.ylim([0.3, 1.3])
plt.xlabel('iteration')
plt.ylabel('RMSE')
plt.title('Tree boosting on Netflix - depth 15')
plt.legend(loc="upper right")
plt.show() 

print(f'test RMSE iter 0: {te_err[0]: .2f}, iter 10: {te_err[10]:.2f}, iter 100: {te_err[99]:.2f}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
והנה אנחנו רואים, שבוסטינג עם learners שהם אינם weak לכאורה, זה מודל גרוע. כבר אחרי 100 עצים שגיאת הטסט מגיעה לאסימפטוטה שמייצגת RMSE גרוע, והאוברפיטינג למדגם הטריין הוא פתולוגי. זאת המחשה יפה לתיאוריה שמסבירה למה צריך עצים שטוחים יחסית ולא עמוקים, ואנחנו גם רואים היטב את ההבדל לעומת רנדום פורסט.
:::
:::

---

## Boosting: in Depth {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ננסה להיכנס קצת יותר לעומק המתמטי של בוסטינג. עד עכשיו תיארנו את המודל ונתנו הרבה אינטואיציה.
:::
:::

---

### A more disciplined view

- For regression, taking the residual as $y_i^{(t)}$  makes sense

- What is an analogy for classification? 

- What about a more rigorous mathematical explanation of what we are doing? 

- There are several approaches of varying mathematical complexity for describing and analyzing boosting

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אמרנו שY בזמן t צריך לייצג את מה שלא הסברנו עד עכשיו. וברגרסיה אינטואיטיבית לקחנו את השארית.

ברגע שאנחנו עוברים לקלספיקציה לא ברור מה זה אומר. אולי לשארית יש מובן עמוק יותר?

יש מספר תיאוריות למה בוסטינג עובד, נראה אחת.
:::
:::

---

### The additive model view

- Start with a very large (possibly infinite) set of $q$ candidate "weak learners": $h_1(x), \dots, h_q(x)$

- We are looking for a "linear" model of the form $\hat{f}(x) = \sum_{k=1}^q \hat{\beta}_k h_k(x)$ 

- In boosted trees example the $h_k$'s are *all possible trees of the given depth*

- Since $q$ is huge we cannot directly find a good $\hat{\beta} \in \mathbb{R}^q$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ננסה לא לחשוב על העצים האלה בעומק 2 שאנחנו מתאימים כמודלים. ננסה לחשוב עליהם כעל פיצ'רים! בדומה לרגרסיה ליניארית או לוגיסטית. נניח שיש q עצים אפשריים כאלה כלומר q פיצ'רים, וברור כבר שq יכול להיות גדול מאוד.

המודל הסופי שלנו, מאוד מזכיר רגרסיה, הוא צירוף ליניארי של העצים או הפיצ'רים האלה: כל אחד מהם מקבל משקולת בטא-האט-קיי, והחיזוי הסופי הוא צירוף ליניארי.

מה הבעיה? העצים האלה לא באמת נתונים, אנחנו לא באמת במצב של רגרסיה ליניארית. ו-q הוא עצום. תחשבו כמה עצים בעומק מקסימלי אפשר לבנות לנתונים כמו נטפליקס למשל. כך שאנחנו לא באמת יכולים למצוא את הוקטור בטא-האט הזה באורך q כפי שאנחנו עושים למשל ברגרסיה ליניארית.

הרעיון הוא למצוא אותו בצורה אדיטיבית, גרידית. לחפש כל פעם את ה"משתנה" הבא, במקרה שלנו עץ, ולהוסיף אותו יחד עם משקולת לצירוף הליניארי.
:::
:::

---

### Additive model via boosting

- At each iteration $t$ we find a "good" candidate $h_{k_t}$ and add $\epsilon h_{k_t}$ to the current model

- After $T$ iterations we have a model where $\hat{\beta}_k = \epsilon \times \# \{k_t = k\}$ (the number of times $k$ was chosen)

::: {.incremental}
- How do we define a good $h_{k_t}$ to update its coefficient? 

- One option: given the current model $F^{(t-1)}$, which $h_k$ improves the model fit the *fastest* when we add it to the model? 

- This can be captured by using the derivative of the loss which measures the fit. Derivative of the RSS (squared loss): 
$$\left.\frac{\partial RSS(F^{(t-1)})}{\partial \hat{y}_i}\right|_{\hat{y}_i= F^{(t-1)}(x_i)} = -2 (y_i - F^{(t-1)}(x_i))$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקרה שלנו, בטא כובע הוא האפסילון. כלומר אחרי T איטרציות, בטא-כובע של העץ הספציפי h_k יהיה אפסילון כפול מספר הפעמים שנבחר העץ h_k. אם יש מיליארד עצים אפשריים נניח, רובם לא ייבחרו ויקבלו משקולת בטא-האט אפס, חלקם יקבלו משוקלת אפסילון, ואולי מספר עצים בודדים יקבלו אפסילון כפול 2 או 3 כי הם נבחרו 2 או 3 פעמים. אז ברור שזה לא וקטור הבטא-האט שנותן את הצירוף הליניארי הטוב ביותר שניתן להתאים במרחב העצים, זה וקטור בטא-האט שנבנה בצורה אדיטיבית.

ומהו העץ בכל שלב, שנבחר להצטרף לצירוף הליניארי?

זהו העץ שמשפר את המודל הכי הרבה!

ומה זה מודל שמשפר הכי הרבה? זה מודל שכשאני אוסיף אפסילון קטן כפול החיזוי שלו, הפיט שלי ישתפר הכי הרבה. מה זה ישתפר הכי הרבה? יקטין הכי הרבה את הלוס פאנקשן. באיזו לוס פאנקשן אנחנו משתמשים ברגרסיה? הRSS, סכום השגיאות הריבועיות של Y מyhat.

כלומר אנחנו רוצים להקטין פונקציה כמה שיותר, וזה אנחנו בדרך כלל משיגים על-ידי ירידה של צעד קטן במורד הנגזרת של הפונקציה שלנו, במורד הגרדיאנט.

עכשיו הRSS בכל קואורדינטה i הוא הריבוע y פחות החיזוי שלה בריבוע. והגזרת של זה לפי החיזוי היא מינוס 2, כפול השארית.

המשמעות היא שכל פעם שאנחנו מוסיפים אפסילון כפול המודל שחוזה את השארית הכי טוב, אנחנו בעצם הולכים צעד קטן במורד הגרדיאנט, אנחנו מקטינים את הלוס! זה גם מסביר למה אנחנו קוראים לאפסילון קצב למידה, ולמה אנחנו מעדיפים צעדים קטנים, ככה אנחנו בדרך כלל עושים באופטימיזציה, הולכים בצעדים קטנים.


:::
:::

---

### The Gradient Boosting Paradigm

- Choose a loss function for modeling (like RSS for regression)

- At each iteration: calculate the (negative) gradient of the loss function at the current model, use that as $Y^{(t)}$ for the next weak learner

- Interpretation: trying to find a weak learner $h_{k_t}$ which "behaves like" the negative gradient, which is the direction of *fastest decrease* of the loss

::: {.fragment}
- Can be applied with different loss functions for regression or classification (In the HW -- classification on wikiart paintings)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ואם הבנו את זה אפשר להפעיל את צורת החשיבה הזאת על כל מודל וכל לוס. קח את הלוס שלך ובכל איטרציה תקטין אותו על-ידי שתבחר את הוויק לרנר או העץ שימדל את הגרדיאנט השלילי שלו הכי טוב, ותוסיף אפסילון כפול החיזוי הזה. זה בעצם ללכת צעד אפסילון בכיוון שבו הלוס יורד הכי מהר, כיוון הגרדיאנט.

בקלסיפיקציה אפשר לעשות בדיוק אותו דבר, כל מה שצריך זה לדעת מה הלוס פאנקשן ומה הנגזרת שלה. אם אתם זוכרים שם דיברנו על הנראות כפונקציה שהיינו רוצים לעשות לה מקסימום אז אפשר לחשוב על הנראות השלילית כפונקציה לעשות לה מינימום ולהמשיך משם, עד שמגיעים לביטוי דומה מאוד לשאריות. בpdf המצורף יש הרחבה על הנושא למי שרוצה לקרוא. בכל מקרה, תראו את זה כבר בפעולה בשיעורי הבית על מדגם הציורים שלנו מwikiart.
:::
:::

---

### Boosting for Netflix (Regression)

```{python}
from sklearn.ensemble import GradientBoostingRegressor

GBR = GradientBoostingRegressor(loss='squared_error', learning_rate=0.05,
    n_estimators=200, max_depth=3)
GBR.fit(NE_Xtr_noNAN, NE_Ytr)

yhat_tr = GBR.predict(NE_Xtr_noNAN)
yhat_te = GBR.predict(NE_Xte_noNAN)

RMSE_tr = np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr)
RMSE_te = np.sqrt(np.sum((yhat_te - NE_Yte)**2) / nte)
print(f'200 trees, depth 3: train RMSE: {RMSE_tr: .2f}, test RMSE: {RMSE_te: .2f}')

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשורה התחתונה, אפשר להשתמש במימוש של sklearn עם הקלאס GradientBoostingRegressor. כאן אני מבקש loss של squared_error, מפרט את הלרנינג-רייט הוא האפסילון שלנו. n_estimators זה מספר העצים ופרמטר הmax_depth כרגיל.

אנחנו מקבלים על מדגם הטסט שגיאת חיזוי דומה מאוד לסימולציה שלנו באופן לא מפתיע.
:::
:::

---

### Trees-based Ensembles: RF vs Boosting

::: {.fragment}
RF: Average trees which are *as different as possible*

1. Randomization is key
2. Big, highly variable trees
3. Average effectively: many trees
:::

::: {.fragment}
Boosting: Adaptively build additive model over trees

1. Trees should not be too big
2. Learn slowly: Small $\epsilon$, many trees
:::

::: {.incremental}
- Both of them take a bunch of bad predictive models (trees) and combine them into a good model!
- What does this say in terms of implementation and model-size on disk?
::: 
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם את מודלי האנסמבל שראינו, רנדום פורסט מול בוסטינג.

רנדום פורסט שואף לבנות בבת אחת הרבה עצים שונים זה מזה ככל שניתן, אנחנו רוצים עצים גדולים, כלומר עמוקים שתהיה להם שונות גבוהה, וככל שנאמן יותר עצים אנחנו יכולים רק להקטין את שגיאת החיזוי על נתוני הטסט.

בבוסטינג העצים נבנים אדפטיבית, הם צריכים להיות לא עמוקים, נעדיף למידה איטית ככל האפשר מה שאומר לשמור את אפסילון נמוך, וגם כאן בפועל, למרות שאין תמיד טיעון תיאורטי טוב כמו ברנדום פורסט, בפועל אנחנו רואים שעוד ועוד עצים משפרים את טעות החיזוי של בוסטינג.

שני המודלים לוקחים מודלים מוגבלים כמו עצים ועושים להם קומבינציה למודל טוב ומורכב. שניהם גם מאוד פופולריים בקרב מדעני נתונים כי לא צריך לכוונן יותר מדי ולהתאים אותם ויש להם ביצועים מעולים.

מבחינת מימוש והאפשרות למקבל את האימון בוסטינג כמובן מאתגר יותר, כי כל עץ צריך להתחשב באלה שקדמו לו. מבחינת הגודל על דיסק דווקא בוסטינג יביא למודלים קטנים יותר שאפשר לשים על כמה שרתים בו זמנית, כי הוא משתמש בעצים שטוחים יותר. זו הרבה פעמים סיבה די טובה להעדיף אותו על רנדום פורסט.

עד כאן על עצים ושילוב שלהם. ביחידה הבאה נדבר על מודל גמיש אחר שאין כמעט איש שלא שמע עליו בשנים האחרונות: רשתות נוירונים.
:::
:::
