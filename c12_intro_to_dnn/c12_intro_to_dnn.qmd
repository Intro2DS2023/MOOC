---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Demystifying Neural Networks"
callout-appearance: simple
smaller: true
execute:
  eval: false
  echo: false
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Demystifying Neural Networks {.title-slide}

### Intro to Data Science - Class 12

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מי לא שמע על רשתות נוירונים? רשתות נוירונים עומדות בבסיס מה שרבים מכנים מהפכת האינטליגנציה המלאכותית, הAI. הן עומדות מאחורי כמה ממערכות החיזוי המתקדמות ביותר כגון מכוניות אוטונומיות וצ'טבוטים כגון צ'ט ג'יפיטי של חברת OpenAI.

עם זאת, לרבים המודל של רשתות נוירונים נשמע מרתיע בהתחלה ונטול מוטיבציה.
ביחידה זו ננסה להראות שהשד לא כזה נורא. נחזור לרגרסיה לוגיסטית, ונראה כיצד ניתן לממש אותה כרשת נוירונים. מכאן נוסיף שכבה ועוד שכבה עד שלבסוף נגיע לרשת נוירונים מודרנית.
:::
:::
---

## Logistic Regression with Gradient Descent {.title-slide}

---

### Reminder: Logistic Regression (I)

::: {.incremental}
- Observe $n$ pairs $(x_i, y_i)$ $(i = 1, \dots, n)$
- $y_i \in \{0, 1\}$ binary outcomes
- $x_i \in \mathbb{R}^q$ numeric predictors
- Model: $Y_i|X_i \sim Bernoulli(p_i)$, so: $E(Y_i|X_i = x_i)=P(Y_i = 1|X_i = x_i) = p_i$
- Choose some *link function* $g$ and model *this* transformation of $E(Y_i|X_i = x_i)$
- Typically for this case $g$ is the logit function: $g(E(Y_i|X_i = x_i)) = \text{logit}(p_i) = \log(\frac{p_i}{1-p_i})=x_i'\beta$
- $\beta$ a vector of $q$ params
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר במודל הרגרסיה הלוגיסטית.

אנו צופים בN זוגות של X ו-Y. Y הוא בינארי, לדוגמא 0 או 1. וX הוא וקטור של q משתנים מסבירים.

כעת נמדל את Y בהינתן X כמשתנה ברנולי עם הסתברות p. ומאחר שהתוחלת של משתנה ברנולי עם הסתברות p היא p עצמו זה מה שאנחנו בסופו של דבר ממדלים. הבעיה היא שהסתברות היא כמות בין 0 ל1.

נבחר איזו פונקצית לינק ג'י לתוחלת, לאותה הסתברות, במקרה שלנו הלוג'יט, שהיא לוג יחס הסיכויים, ואת הפונקציה הזאת נמדל באמצעות מודל ליניארי רגיל.

בטא הוא וקטור של q מקדמים, ואותו אנחנו רוצים למצוא.
:::
:::
---

### Reminder: Logistic Regression (II)

- And so we can write:
$E(Y_i|X_i = x_i)= P(Y_i=1|X_i = x_i;\beta) = p_i = g^{-1}(x_i\beta) = \frac{1}{1+e^{-x_i\beta}}$

::: {.incremental}
- Once we get our estimate $\hat\beta$:
1. We could "explain" $Y_i$, the size and direction of each component of $\hat\beta$ indicating the contribution of that predictor to the *log-odds* of $Y_i$ being $1$
2. We could "predict" probability of new observation $x_i$ having $Y_i=1$ by fitting a probability $\hat p_i=\frac{1}{1+e^{-x_i\hat\beta}}$, where typically if $\hat p_i > 0.5$, or $x_i\hat\beta > 0$, we predict $\hat{Y}_i=1$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
איך מוקטור המקדמים נחזור לקבל הסתברות? באמצעות הפונקציה ההופכית g inverse.

כאשר נשיג אומדן לוקטור המקדמים שלנו בטא האט,

1. נוכל להסביר את Y, או יותר נכון להסביר את הסיכוי שY יהיה 1. הכיוון והגודל של כל רכיב בוקטור המקדמים יגידו לנו מהי התרומה של המשתנה המתאים ללוגריתם של יחס הסיכויים של Y.

2. נוכל לחזות עבור תצפית חדשה את ההסתברות שמשתנה הY שלה הוא 1. יש לנו נוסחה לזה, נכפול את הנתונים X בוקטור המקדמים הנאמד בטא האט, ונחשב על זה את הפונקציה ההופכית של g, g inverse. אם נרצה חיזוי סופי נצטרך להשוות את ההסתברות הנחזית p_hat לאיזשהו סף קאטאוף, לדוגמא חצי. אם p_hat גדול מחצי נחזה שY יהיה 1, אחרת נחזה 0.

עד כאן ראינו ברגרסיה לוגיסטית.
:::
:::
---

### Maximum Likelihood

::: {.incremental}

- Under the standard Maximum Likelihood approach we assume $Y_i$ are also *independent* and so their joint "likelihood" is:  
$L(\beta|X, y) = \prod_{i = 1}^n{P(Y_i|X;\beta)} = \prod_{i = 1}^n{P(Y_i = 1|X;\beta)^{y_i}P(Y_i = 0|X;\beta)^{1-y_i}}$

- But what is $P(Y_i = 1|X;\beta)$?  
$L(\beta|X, y) = \prod_{i = 1}^n[g^{-1}(x_i\beta)]^{y_i}[1- g^{-1}(x_i\beta)]^{1-y_i}$

- The $\hat\beta$ we choose is the vector maximizing $L(\beta|X, y)$

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה איך אנחנו מוצאים אומד לוקטור המקדמים בטא.

הגישה הסטנדרטית היא למקסם פונקציה בשם הנראות. פונקציה זו מסומנת בדרך כלל ב-L גדולה, היא פונקציה של בטא בהינתן הנתונים X ו-Y. בהנחת אי-תלות בין התצפיות מדובר במכפלת ההסתברויות לקבל כל תצפית ותצפית, שבמקרה שלנו מקבלת צורה יפה שכזאת: אם Y שווה לאחת נכתוב את ההסתברות בחזקת Y, אם Y שווה אפס נכתוב את ההסתברות בחזקת 1 פחות Y.

ומהי אותה הסתברות לפי המודל? הפונקציה ההופכית של G או אחת פחות הפונקציה ההופכית של G.

הוקטור בטא האט הוא הוקטור שימקסם את הנראות ולכן נקרא אומד נראות מקסימלית, או MLE.
:::
:::

---

### Maximum Likelihood (II)

::: {.incremental}
- Take the log-likelihood which is easier to differentiate:  
$l(\beta|X, y)=\sum_{i=1}^n\ln{P(Y_i|X;\beta)} =$
$\sum_{i=1}^n y_i\ln[g^{-1}(x_i\beta)] + (1-y_i)\ln[1- g^{-1}(x_i\beta)] =$

- This looks Ok but let us improve a bit just for easier differentiation:  
$\sum_{i=1}^n \ln[1- g^{-1}(x_i\beta)] + y_i\ln[\frac{g^{-1}(x_i\beta)}{1- g^{-1}(x_i\beta)}]=$
$\sum_{i=1}^n -\ln[1+ e^{x_i\beta}] + y_ix_i\beta$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
פונקצית הנראות כפי שהוא מנוסחת כרגע היא קשה למיקסום. נהוג לעבוד עם לוג הנראות, כלומר עם סכום הלוגריתמים של ההסתברויות שרשמנו.

נפשט את הפונקציה הזאת אפילו יותר כדי לקבל פונקציה שקל לגזור יותר לפי בטא.
:::
:::


---

### Maximum Likelihood (III)

::: {.incremental}
- Differentiate:  
$\frac{\partial l(\beta|X, y)}{\partial \beta_j} = \sum_{i=1}^n-\frac{1}{1+e^{x_i\beta}}e^{x_i\beta}x_{ij} + y_ix_{ij}=\sum_{i=1}^n x_{ij}(y_i-g^{-1}(x_i\beta))$

- Or in vector notation:  
$\frac{\partial l(\beta|X, y)}{\partial \beta}=X'(y - g^{-1}(X\beta))$,
where $X$ is the $n \times q$ data matrix.


- We would like to equate this with $\vec0$ and get $\hat\beta$ but there's no closed solution.

- At which point usually the Newton-Raphson method comes to the rescue.

- But let's look at simple gradient descent:
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נגזור את הפונקציה לפי אלמנט אחד בוקטור בטא, בטא ג'יי.

נכתוב את הנגזרת לפי כל וקטור בטא בכתיב וקטורי, מה שיקל עלינו במימוש.

את הביטוי הזה היינו רוצים להשוות לוקטור האפס ולקבל את האומד לבטא, אבל לא נקבל פתרון סגור.

בשלב זה נעבור לשיטות של אנליזה כמו ניוטון רפסון, פונקצית לוג הנראות היא קמורה ואיננה נחשבת לפונקציה קשה.

אבל אני רוצה שנסתכל על פתרון של מורד הגרדיאנט, gradient descent.
:::
:::

---

### Gradient Descent

::: {.incremental}
- Instead of maximizing log-likelihood, let's minimize negative log-likelihood $-l(\beta)$ (NLL)
- We'll start with an initial guess $\hat\beta_{t=0}$
- The partial derivatives vector of $-l(\beta)$ at point $\hat\beta_t$ (a.k.a the *gradient* $-\nabla l(\hat\beta_t)$) points to the direction of where $-l(\beta)$ has its steepest descent
- We'll go a small $\alpha$ step down that direction: $\hat\beta_{t+1}=\hat\beta_t -\alpha \cdot[-\nabla l(\hat\beta_t)]$
- We do this for $I$ iterations or until some stopping rule indicating $\hat\beta$ has converged
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקום למקסם את לוג הנראות, נעשה מינימיזציה ללוג הנראות השלילית, ה-NLL.

נתחיל בניחוש ראשוני עבור בטא האט בזמן t = 0.

הגרדיאנט, או וקטור הנגזרות החלקיות של הNLL המחושב בנקודה בטא האט, מצביע לכיוון שיפוע הירידה התלולה ביותר בנקודה זו.

נלך צעד קטן בגודל אלפא בכיוון זה. וקטור בטא האט החדש שלנו יהיה בטא האט הקודם פחות צעד אלפא בכיוון וקטור הגרדיאנט של פונקצית הלוג-נראות השלילית.

נעשה את זה במשך מספר איטרציות עד להתכנסות למינימום מקומי, במקרים מסוימים כגון המקרה שלפנינו אנחנו יכולים לדעת שמדובר בנקודת מינימום גלובלית.

:::
:::

---

#### Let's see that it works ! &#x1F600;

```{python}
import numpy as np
import matplotlib.pyplot as plt

X1 = np.linspace(-4, 4) # for plotting

n = 1000
q = 2
X = np.random.normal(size = n * q).reshape((n, q))
beta = [1.0, 2.0]
p = 1 / (1 + np.exp(-np.dot(X, beta)))
y = np.random.binomial(1, p, size = n)

def plot_sim(plot_beta_hat=True):
  plt.clf()
  plt.scatter(X[:, 0], X[:, 1], c = y)
  plt.plot(X1, -X1 * beta[0]/beta[1], linestyle = '--', color = 'red')
  if plot_beta_hat:
    plt.plot(X1, -X1 * beta_hat[0]/beta_hat[1], linestyle = '--')
  plt.xlabel('X1')
  plt.ylabel('X2')
  if plot_beta_hat:
    title = 'Guess: %.2f * X1 + %.2f * X2 = 0' % (beta_hat[0], beta_hat[1])
  else:
    title = 'Ideal: 1 * X1 + 2 * X2 = 0'
  plt.title(title)
  plt.show()
```

```{python}
#| eval: true
#| echo: true

import numpy as np

n = 1000
q = 2
X = np.random.normal(size = n * q).reshape((n, q))
beta = [1.0, 2.0]
p = 1 / (1 + np.exp(-np.dot(X, beta)))
y = np.random.binomial(1, p, size = n)
```

::: {.callout-note}
See it in python!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה כיצד גרדיאנט דיסנט יעבוד בפייתון. 
בקוד שלפניכם אנחנו מדמים מצב אידיאלי לרגרסיה לוגיסטית עם שני משתנים, וללא חותך. N יהיה שווה 1000 תצפיות ו-q יהיה שווה ל2, כלומר שני משתנים, לכן ב-X יהיו 1000 שורות על שתי עמודות. וקטור המקדמים האמיתי בטא באורך 2 יהיה פשוט 1, 2.

וקטור ההסתברויות האמיתיות p מחושב לפי פונקצית ה-G ההופכית שלנו.

לבסוף Y נדגם מהתפלגות ברנולי להיות 0 או 1, עם ההסתברויות שחישבנו.
:::

:::

```{python}
plot_sim(False)
```

```{python}
# initial guess for beta

beta_hat = np.ones(q) # [1, 1]

plot_sim()
```

```{python}
# let's do 1 iteration

alpha = 0.01

p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))
grad = -np.dot(X.T, (y - p_hat))
beta_hat = beta_hat - alpha * grad

plot_sim()
```

```{python}
# let's do 10 more iterations

for i in range(10):
  p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))
  grad = -np.dot(X.T, (y - p_hat))
  beta_hat = beta_hat - alpha * grad

plot_sim()
```

```{python}
# let's see NLL -l(beta) through iterations

alpha = 0.001
beta_hat = np.array([-2.5, -2.5])
betas = [beta_hat]
ls = []
for i in range(50):
  p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))
  nll = -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))
  ls.append(nll)
  grad = -np.dot(X.T, (y - p_hat))
  beta_hat = beta_hat - alpha * grad
  betas.append(beta_hat)

plt.plot(range(50), ls)
plt.xlabel("Iteration")
plt.ylabel(r"$-l(\beta)$")
plt.show()
```

```{python}
# Even fancier, visualize the actual gradient descent in the beta space:

betas_arr = np.array(betas)
m = 10
beta1 = np.linspace(-3.0, 3.0, m)
beta2 = np.linspace(-3.0, 3.0, m)
B1, B2 = np.meshgrid(beta1, beta2)
L = np.zeros((m, m))
for i in range(m):
  for j in range(m):
    beta_hat = np.array([beta1[i], beta2[j]])
    p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))
    L[i, j] = -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))
fig, ax = plt.subplots(1,1)
cp = ax.contourf(B1, B2, L)
cb = fig.colorbar(cp)
ax.set_title(r'$-l(\beta)$ gradient descent')
ax.set_xlabel(r'$\beta_1$')
ax.set_ylabel(r'$\beta_2$')
ax.plot(betas_arr[:, 0], betas_arr[:, 1], marker='x', color='white')
ax.plot([beta[0]], [beta[1]], marker='x', color='red', markersize=20, markeredgewidth=5)
plt.show()
```

---

## You've already been Neural Network-ing! {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל מה הקשר בין רגרסיה לוגיסטית לרשתות נוירונים? כעת נראה שרגרסיה לוגיסטית היא רשת נוירונים פשוטה למדי.
:::
:::
---

### Call it a Neural Network

::: {.incremental}
1. Call our $-l(\beta)$ "Cross Entropy"
2. Call $g^{-1}(X\beta)$ the "Sigmoid Function"
3. Call computing $\hat p_i$ and $-l(\hat\beta)$ a "Forward Propagation" or "Feed Forward" step
4. Call the differentiation of $-l(\hat\beta)$ a "Backward Propagation" step
5. Call our $\beta$ vector $W_{(q+1)\text{x}1}$, a weight matrix (add intercept, call it "bias")
6. Add *stochastic* gradient descent (SGD)
7. Draw a diagram with circles and arrows, call these "neurons"
:::
::: {.fragment}
And you have a Neural Network*.
:::
::: {.fragment style="font-size: 50%;"}
*Ok, We'll add some stuff later
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נקרא לפונקצית הנראות השלילית cross entropy.

נקרא לפונקציה ההופכית ל-g שמחשבת לנו את ההסתברות החזויה, פונקצית זיגמויד.

לחישוב ההסתברות החזויה ולוג הנראות נקרא פעפוע לפנים, או forward propagation.

לחישוב הגרדיאנט נקרא פעפוע לאחור, או backward propagation.

נסמן את וקטור המקדמים שלנו כמטריצה W שיש לה עמודה אחת, לפעמים נוסיף גם חותך שנקרא לו bias.

במקום להשתמש בגרדיאנט דיסנט על כל הדאטה נשתמש בגרדיאנט דיסנט סטוכסטי או SGD - תיכף נסביר מה זה.

לבסוף נתאר את המודל שלנו בתור דיאגרמה עם קשתות וצמתים, נקרא להם נוירונים.

ויש לנו רשת נוירונים!

טוב, בהמשך נוסיף עוד כמה אלמנטים.
:::
:::

---

### Cross Entropy

::: {.incremental}
- For discrete probability distributions $P(X)$ and $Q(X)$ with the same support $x \in \mathcal X$ Cross Entropy could be seen as a metric of the "distance" between distributions:  
$H(P, Q) = -E_P[\log(Q)] = -\sum _{x\in {\mathcal{X}}}P(X=x)\log[Q(X=x)]$

- In case $X$ has two categories, and $p_1=P(X=x_1)$, $p_2=1-p_1$ and same for $q_1,q_2$:  
$H(P, Q) = -[p_1\log(q_1) + (1-p_1)\log(1-q_1)]$

- If we let $p_1=y_i$ and $q_1=\hat p_i=g^{-1}(x_i\hat\beta)$ we get:  
$H(y_i, \hat p_i) = -[y_i\log(\hat p_i) + (1-y_i)\log(1-\hat p_i)] =$
$-[y_i\ln[g^{-1}(x_i\hat\beta)] + (1-y_i)\ln[1- g^{-1}(x_i\hat\beta)]]$

- Which is exactly the contribution of the $i\text{th}$ observation to the NLL $-l(\hat\beta)$.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ראשית מהו הקרוס אנטרופי? אפשר לראות בקרוס אנטרופי כמדד למרחק בין שתי התפלגויות. 

עבור שתי התפלגויות P ו-Q, קרוס אנטרופי המסומנת בדרך כלל ב-H, היא מינוס התוחלת של לוג ההתפלגות Q לפי התפלגות P.

במקרה שלפנינו למשתנה אקראי X יש שני ערכים אפשריים, והקרוס אנטרופי מקבלת צורה של סכום פשוט של הסתברות לפי התפלגות אחת כפול לוג ההסתברות לפי ההתפלגות האחרת.

אם נחשוב על שתי ההתפלגויות שלנו כפונקצית ההסתברות של Y הבינארי שלנו, ו-Y עצמו, נוכל להוכיח שקרוס אנטרופי היא בדיוק פונקצית לוג-הנראות השלילית שלנו.
:::
:::

---

### Sigmoid Function

If $g(p)$ is the logit function, its inverse would be the sigmoid function:

$g(p) = logit(p) = \log(\frac{p}{1-p}); \space\space g^{-1}(z) = \sigma(z) =\frac{1}{1+e^{-z}}$

So: $g^{-1}(g(p)) = \sigma(logit(p)) = p$

```{python Sigmoid}
#| eval: true

import numpy as np
import matplotlib.pyplot as plt

X1 = np.linspace(-4, 4)
plt.clf()
plt.plot(X1, 1 / (1 + np.exp(-X1)))
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
שינוי נוסף שנעשה הוא פשוט לשנות את שם הפונקציה ההופכית של G לפונקצית זיגמויד. פונקצית זיגמויד המסומנת כאן עם האות סיגמא, מקבלת כל קלט בין מינוס לפלוס אינסוף, וממפה אותו בצורה מונוטונית להיות בין 0 ל-1.
:::
:::

---

### Forward/Backward Propagation

Recall that each iteration of gradient descent included:

::: {.incremental}
1. Forward step: Calculating the NLL loss $-l(\hat\beta)$
2. Backward step: Calculate the gradient $-\nabla l(\hat\beta_t)$
3. Gradient descent: $\hat\beta_{t+1}=\hat\beta_t -\alpha \cdot[-\nabla l(\hat\beta_t)]$
:::

::: {.fragment}
```{python}
#| echo: true
#| code-line-numbers: "|1-3|4-5|6-7"

# forward step
p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))
nll = -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))
# backward step
grad = -np.dot(X.T, (y - p_hat))
# descent
beta_hat = beta_hat - alpha * grad
```
:::
::: {.fragment}
Why "Forward", why "Backward"?...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מהם הפורוורד והבקוורד פרופוגיישן?

אפשר לומר שכל איטרציה בגרדיאנט דיסנט כללה:

שלב פורוורד - חישוב פונקצית הלוג-נראות והחיזוי p_hat

שלב בקוורד - חישוב הגרדיאנט

גרדיאנט דיסנט - הליכה של צעד בגודל אלפא בכיוון הגרדיאנט

בפייתון מימשנו כל שלב באמצעות שורה אחת או שתיים. פורוורד. בקוורד. גרדיאנט דיסנט.

אבל למה אנחנו קוראים לזה פורוורד או לפנים? למה בקוורד או לאחור?
:::
:::

---

### Reminder: Chain Rule

In our case differentiating $l(\beta)$ analytically was manageable.

::: {.incremental}
- As the NN architecture becomes more complex there is need to generalize this, and break down the derivative into (backward) steps.

- Recall that according to the Chain Rule, if $y = y(x) = f(g(h(x)))$ then:
$y'(x)=f'(g(h(x)) \cdot g'(h(x)) \cdot h'(x)$

- Or if you prefer, if $z = z(x); \space u = u(z); \space y = y(u)$ then:
$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dz} \cdot \frac{dz}{dx}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקרה שלנו הגזירה של פונקצית הלוג-נראות השלילית לפי וקטור המקדמים בטא היתה יחסית פשוטה. הגענו לביטוי פשוט גם אם לא לפתרון סגור.

ככל שהארכיטקטורה של רשת הנוירונים תלך ותיעשה מורכבת יותר, נצטרך להכליל את הגזירה הזאת, לצעדים קטנים.

לפי כלל השרשרת, אם Y הוא פונקציה F של פונקציה G של פונקציה H של X - 
אפשר לרשום את הנגזרת של Y לפי X לפי כלל השרשרת. הנגזרת של Y לפי X היא הנגזרת של F לפי G, כפול הנגזרת של G לפי H, כפול הנגזרת של H לפי X.

אפשר לרשום זאת בכתיב דיפרנציאלי, ובכל מקרה נבחין שיש לפנינו מכפלה לאחור של נגזרות.
:::
:::

---

Let's re-write $-l(\beta)$ as a composite function:

::: {.incremental}
- Multiplying $\beta$ by $x_i$ will be $z_i = z(\beta) = x_i\beta$
- Applying the sigmoid $g^{-1}$ will be $p_i = g^{-1}(z_i) = \frac{1}{1 + e^{-z_i}}$
- Calculating the (minus) Cross Entropy will be: $l_i = l(p_i) = y_i\ln(p_i) + (1-y_i)\ln(1 - p_i)$
- So one element of $-l(\beta)$ will be: $l_i(p_i(z_i(\beta)))$
:::

::: {.fragment}
Hence, Forward.
:::

::: {.fragment}
Now $-l(\beta)$ is the sum of (minus) cross entropies: $-l(\beta) = -\sum_i l_i(p_i(z_i(\beta)))$

And we could differentiate using the chain rule like so:

$-\frac{\partial l(\beta)}{\partial \beta_j} = -\sum_i\frac{\partial l_i}{\partial p_i} \cdot \frac{\partial p_i}{\partial z_i} \cdot \frac{\partial z_i}{\partial \beta_j}$
:::
::: {.fragment}
Hence, Backward.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי שנוכל להשתמש בכלל השרשרת, נרשום את פונקצית הלוג-נראות השלילית שלנו כפונקציה מורכבת של בטא:

ההכפלה של האינפוטים X בבטא תהיה פונקציה Z של בטא.

המעבר של Z בפונקצית זיגמויד או הפונקציה ההופכית של G תיקרא פונקציה P של Z.

החישוב הסופי של האלמנט ה-I בתוך הלוג-נראות השלילית מתוך ההסתברויות החזויות P ייקרא פונקציה L של P.

כלומר אלמנט אחד של פונקצית ההפסד שלנו הוא L של P של Z של בטא. ולכן - קדימה, או פורוורד.

וכעת אפשר להפעיל את כלל השרשרת: הנגזרת של פונקצית ההפסד לפי בטא J כלשהו, היא נגזרת של L לפי P, כפול נגזרת של P לפי Z, כפול נגזרת לפי בטא J.
ולכן - אחורה, או בקוורד.
:::
:::

---

Each of these is simpler to calculate:

$\frac{\partial l_i}{\partial p_i}= \frac{y_i - p_i}{p_i(1-p_i)}$

$\frac{\partial p_i}{\partial z_i} = p_i(1-p_i)$

$\frac{\partial z_i}{\partial \beta_j}=x_{ij}$

And so:

$-\frac{\partial l(\beta)}{\partial \beta_j} = - \sum_i \frac{y_i - p_i}{p_i(1-p_i)} \cdot p_i(1-p_i) \cdot x_{ij}$

Which is excatly what we got analytically but now we can write our gradient descent iteration as a list of forward/backward steps.

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כל אחת מהנגזרות האלה קלה לחישוב: הנגזרת של L לפי P, הנגזרת של P לפי Z והנגזרת של Z לפי בטא J.

ואם נכפול את הנגזרות האלה זו בזו לפי כלל השרשרת, נקבל בדיוק את הביטוי האנליטי הפשוט לגרדיאנט שקיבלנו לפני כן.
:::
:::

---

### Implementing NN in python

```{python}
#| echo: true
#| code-line-numbers: "|1-6|8-12|14-15|17-21|"

def forward(X, y, beta_hat):
  z = np.dot(X, beta_hat)
  p_hat = 1 / (1 + np.exp(-z))
  l = y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)
  nll = -np.sum(l)
  return p_hat, nll

def backward(X, y, p_hat):
  dldz = y - p_hat
  dzdb = X.T
  grad = -np.dot(dzdb, dldz)
  return grad

def gradient_descent(alpha, beta_hat, grad):
  return beta_hat - alpha * grad

def optimize(X, y, alpha, beta_hat):
  p_hat, l = forward(X, y, beta_hat)
  grad = backward(X, y, p_hat)
  beta_hat = gradient_descent(alpha, beta_hat, grad)
  return l, beta_hat

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת נממש את הרגרסיה הלוגיסטית לפי הצעדים שחישבנו באמצעות כמה פונקציות פשוטות:

פונקצית הפורוורד מקבלת את X, Y ואת האומדן הנוכחי לוקטור המקדמים בטא. היא מחשבת שלב אחרי שלב את ההסתברויות החזויות ואת לוג הנראות השלילית:
הכפלה של X בבטא האט לתת את Z, זיגמויד על Z לתת את P האט, ולבסוף חישוב הנראות הסופית.

פונקצית הבקוורד תקבל את X ואת ההסתברויות החזויות P האט, ותחשב בשני צעדים לאחור את הנגזרות ותכפיל ביניהן לפי כלל השרשרת להחזיר את הגרדיאנט לפי בטא.

פונקצית הגרדיאנט דיסנט פשוט תקבל את גודל הצעד אלפא, את האומדן הנוכחי לבטא ואת הגרדיאנט, ותחשב אומדן מעודכן לבטא על ידי הליכה בצעד אלפא בכיוון הגרדיאנט.

הפונקציה אופטימייז מהווה איטרציה אחת שכוללת צעד פורוורד, צעד בקוורד וצעד גרדיאנט דיסנט.
:::
:::

---

```{python}
#| echo: true

def lr_nn(X, y, epochs):
  beta_hat = np.array([-2.5, -2.5])
  alpha = 0.001
  for i in range(epochs):
    l, beta_hat = optimize(X, y, alpha, beta_hat)
  return l, beta_hat
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לבסוף הפונקציה lr_nn מקבלת את הדאטא X ו-Y ומספר איטרציות לריצה שנקראות איפוקס.

היא מאתחלת את האומדן בטא-האט לאיזשהם ערכים, כאו מינוס 2.5 לשני הרכיבים של הוקטור.

ואז היא מבצעת את מספר האיפוקס המתבקש, כל אחד מאלה הוא בעצם קריאה לפונקציה אופטימייז, שעושה צעד פורווד, צעד בקוורד וגרדיאנט דיסנט.
:::
:::
---

Adding *stochastic* gradient descent (SGD) on mini-batches:
```{python}
#| echo: true

def lr_nn(X, y, epochs):
  beta_hat = np.random.rand(X.shape[1])
  alpha = 0.001
  batch_size = 100
  n = X.shape[0]
  steps = int(n / batch_size)
  for i in range(epochs):
    print('epoch %d:' % i)
    permute = np.random.permutation(n)
    X_perm = X[permute, :]
    y_perm = y[permute]
    for j in range(steps):
      start = j * batch_size
      l, beta_hat = optimize(X_perm[start:start + batch_size, :],
                            y_perm[start:start + batch_size],
                            alpha, beta_hat)
      print('Trained on %d/%d, loss = %d' % (start + batch_size, n, l))
  return l, beta_hat

l, beta_hat = lr_nn(X, y, 50)
```

::: {.callout-note}
See it in python!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
רגע לפני שנריץ את הפונקציה, אני בכל זאת רוצה שנשנה דבר קטן אחד. נזכיר שרשתות נוירונים יודעות לרוץ על קבצי נתונים גדולים מאוד.

אחד מהאופנים שבהן הן עושות את זה, הוא שהגרדיאנט לא מחושב בכל איטרציה על כל מסד הנתונים. במקרה שלפנינו לא היתה בעיה לעשות את זה גם אם N היה גדול מאוד. אבל ברגע שהרשת תיעשה מורכבת יותר, חישוב הגרדיאנט בכל איטרציה יגזול מאיתנו זמן וכוח חישוב, אם בכלל יהיה אפשרי.

לכן מקובל ברשתות נוירונים לאמן על באצ'ים אקראיים של דאטא. בכל איפוק נחלק את הדאטא למספר תת-מדגמים אקראיים בגודל batch_size, ונזין אותם אחד אחרי השני לרשת, נחשב את הגרדיאנט ונלך צעד בכיוונו, עד שהרשת תראה בסוף האיפוק את כל הדאטה. באיפוק הבא היא תראה באצ'ים אחרים.

האקראיות הזאת שהכנסנו נקראת stochatstic gradient descent, והיא כאמור עקרון מפתח בהפעלת הרשת על נתונים גדולים באמת.

שימו לב שלא כל פונקצית הפסד ניתן בהכרח לפרק לחישוב על מספר באצ'ים. במקרה שלנו ראינו שהלוג-נראות השלילית היא סכום של אלמנטים על כל התצפיות, ולכן היא מתפרקת בפשטות יחסית למספר סכומים, וניתן להראות שזה מספיק כדי לתקף את השינוי הזה.

בואו נראה את המימוש שלנו בפעולה!
:::
:::

---

### Put it in a Neural Network Diagram

Binary Logistic Regression, is in fact a single neuron firing a sigmoid probability-like number between 0 and 1, for each sample:

<img src="images/lr_nn.png" style="width: 80%" />

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה בין מודל הרגרסיה הלוגיסטית לרשת נוירונים? איפה ה"רשת"? נסמן את האינפוט של הרשת, Q פיצ'רים של X ועוד חותך באמצעות צמתים של רשת.

נמתח קשת בין כל אינפוט אל תוך צומת אחד שנקרא לו נוירון. כל אחת מהקשתות מסמלת פרמטר בוקטור המקדמים שלנו בטא, והנוירון כופל את האינפוטים במקדמים וסוכם אותם. למעשה הנוירון מבצע מכפלה פנימית בין שני הוקטורים.

לאחר מכן הכמות הזאת עוברת אקטיבציה, כאן אקטיבצית זיגמויד, כדי לתת חיזוי סופי לתצפית p_hat ואת פונקצית ההפסד בשלב זה.

וכל זה נקרא צעד פורוורד.

לאחר מכן נלך אחור ברשת ונחשב בכל שלב את הנגזרת כדי לתת הגרדיאנט לכל אחד מהמקדמים בוקטור המשקלות בטא. וזה ייקרא צעד בקוורד. לבסוף נלך צעד אלפא בכיוון הגרדיאנט וחוזר חלילה.

אנחנו רואים שניתן לייצג רגרסיה לוגיסטית כנוירון בודד, ברשת שהיא אינה אלא ייצוג למודל המוכר לנו. את הייצוג הזה יהיה קל להכליל למודלים מורכבים יותר ויותר.
:::
:::

---

### LR as NN in `Keras`

```{python}
#| echo: true
#| code-line-numbers: "|1|2|3|5|6-7|8|9|10|"

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

model = Sequential()
model.add(Dense(1, input_shape=(X.shape[1], ),
  activation='sigmoid', use_bias=False))
sgd = SGD(learning_rate=0.1)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X, y, batch_size=100, epochs=50)
```

::: {.callout-note}
See it in python!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מובן שלא נממש בעצמנו רשת נוירונים כל פעם שנצטרך. בקורס זה נשתמש בספריית קראס, שהיא מצוינת למתחילים.

כאן אני מייבא קלאס בשם sequential, קלאס לשכבה dense פשוטה כפי שאנחנו צריכים, וקלאס לגרדיאנט דיסנט סטוכסטי, SGD.

אני מאתחל מודל עם sequential. מוסיף שכבת dense עם נוירון בודד באמצעות הפעולה model.add(), ואקטיבציה sigmoid. נשים לב שאני מבקש מהשכבה לא להשתמש בביאס, הוא החותך.

אני מאתחל קלאס SGD עם איזשהו קצב למידה, זהו האלפא שלנו. מקמפל את המודל באמצעות קריאה לmodel.compile(), שם אני מפרט גם את פונקצית ההפסד שהיא הקרוס אנטרופי ואת האופטימייזר שלנו, הוא הSGD.

לבסוף אני מריץ את המודל על הנתונים באמצעות הקריאה לmodel.fit(), לתוכו אני מזין את X ואז Y, ומפרט גם מה גודל הבאץ' ומספר האיפוקס. 

בואו נראה את זה בפעולה.
:::
:::

```{python}
# See that it makes sense:
beta_hat = model.get_weights() # Note Keras gives a list of weights!
beta_hat
```


```{python}
pred = model.predict(X, verbose=0)
pred[:3]
```


```{python}
pred_manual = 1/(1+np.exp(-np.dot(X, beta_hat[0])))
pred_manual[:3]
```

---

### Is that it?

1. No &#x1F600;
2. > The knee-jerk response from statisticians was "What's the big deal? A neural network is just another nonlinear model, not too different from many other generalizations of linear models". While this may be true, neural networks brought a new energy to the field. They could be scaled up and generalized in a variety of ways... and innovative learning algorithms for massive data sets."

::: {style="font-size: 50%;"}
(*Computer Age Statistical Inference* by Bradley Efron & Trevor Hastie, p. 352)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז האם זה כל מה שיש בה, ברשת הנוירונים? הכללה של רגרסיה לינארית או לוגיסטית לקשר לא-ליניארי בין X ל-Y?

התשובה הקצרה היא: לא. את התשובה הארוכה יותר אני מביא כאן כציטוט מספר מצוין של שני סטטיסטיקאים גדולים, בראדלי אפרון וטרוור הייסטי:

רשתות נוירונים הזריקו אנרגיה חדשה לתחום. אפשר להכליל אותן במגוון דרכים כפי שנראה ולאמן אותם על מסדי נתונים עצומים.
:::
:::

---

## Add Classes {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לאמן רשת על שתי מחלקות או קלאסים בלבד זה מעט משעמם. בואו נכליל את הרשת ל-C קלאסים.
:::
:::

---

### $C$ Neurons for $C$ Classes

::: {.incremental}
- fit a $\beta$ vector for each class (or let's start talking about $W$)
- have $C$ neurons for $C$ classes
- where the output layer is the *Softmax Function*, to make sure the fitted $\hat p$ sum up to 1:  
$\hat p_{i;c} = \text{softmax}(c,W_{(q+1)\text{x}C}, x_i)=\frac{e^{x_iw_c}}{\sum_{c=1}^{C} e^{x_iw_c}}$  
Where $x_i$ is the $i$th row of $X$ as before and $w_c$ is the $c$th row of $W^T$ (or $c$th column of $W$)
:::
::: {.fragment}
::: {.callout-tip}
This would be equivalent to *multinomial logistic regression*!
:::
:::
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתאים וקטור מקדמים בטא לכל אחד מהקלאסים שלנו. נקרא לוקטור כזה כבר W ונדבר על מטריצה של משקלות, עם Q פלוס 1 שורות, ו-C עמודות ל-C קלאסים.

נוסיף בשכבת האאוטפוט C נוירוינים ל-C קלאסים.

כאשר המכפלה הפנימית בכל נוירון כזה תעבור אקטיבציית סופטמקס. סופטמקס היא נוסחה שמטרתה לייצר מ-C נוירונים C הסתברויות שיסתכמו ב-1. הפונקציה לוקחת את האאוטפוט של כל נוירון, מעלה אותו באקספוננט ומחלקת בסך האקספוננטים מכל ה-C נוירונים.

מודל זה, אגב, ניתן לראות כמודל שקול לרגרסיה מולטינומית שהזכרנו, מודל שמכליל רגרסיה לוגיסטית ל-C קלאסים.
:::
:::

---

So the architecture for 2 classes would be:

<img src="images/lr_nn_2neurons.png" style="width: 80%" />

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת נוספים לדיאגרמה שלנו שני נוירונים עבור שני קלאסים שמייצרים שתי הסתברויות שמסתכמות ב1, אחרי אקטיבצית סופטמקס.
:::
:::

---

And in `Keras` we would do:

```{python}
#| echo: true
#| code-line-numbers: "|3|5-6|8|"

from tensorflow.keras.utils import to_categorical

y_categorical = to_categorical(y)
model = Sequential()
model.add(Dense(2, input_shape=(X.shape[1], ),
  activation='softmax', use_bias=False))
sgd = SGD(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
model.fit(X, y_categorical, batch_size=100, epochs=50)
```

::: {.callout-note}
See it in python!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בקראס נעשה מספר שינויים קטן:
ה-Y שלנו בשביל C קלאסים כבר לא יכול להיות וקטור עם 0 או 1. הוא צריך להיות מטריצה, עם N שורות ו-C עמודות, בכל עמודה יש 0 בכל השורה חוץ מהעמודה המתאימה לקלאס שם יש 1. את כל זה ניתן לבצע עם הפונקציה to_categorical.

בשכבת האאוטפוט שלנו יהיו 2 נוירונים, ולא 1, והאקטיבציה תהיה סופטמקס ולא זיגמויד.

לבסוף בזמן קומפילציה של המודל נפרט פונקצית הפסד בשם categorical_crossentropy ולא binary_crossentropy.

בואו נראה את זה בפעולה.
:::
:::

```{python}
# See that it makes sense:
W = model.get_weights()
W
```


```{python}
pred = model.predict(X, verbose=0)
pred[:3]
```


```{python}
Z = np.dot(X, W[0])
Z_exp = np.exp(Z)
Z_exp_sum = Z_exp.sum(axis=1)[:, None]
pred_manual = Z_exp / Z_exp_sum
pred_manual[:3]
```

---

## Add Hidden Layers {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ראינו שקל להכליל את הרשת להוציא C אאוטפוטים ל-C קלאסים. כעת נראה כמה קל להוסיף עוד שכבות ביניים. אנחנו קוראים לשכבות כאלה נסתרות, או hidden.
:::
:::

---

### Adding Hidden Layers

<img src="images/lr_nn_morelayers.png" style="width: 80%" />

Where $g()$ is some non-linear *activation function*, e.g. sigmoid (but not often used).

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הדיאגרמה שלנו כבר נראית כמו רשת נוירונים מורכבת:

Q אינפוטים נכנסים לרשת בתוספת לביאס, ומגיעים לשכבת הביניים שבה יש H נוירונים. הנוירונים מבצעים מכפלה פנימית בין האינפוטים לבין כל אחת מהעמודות של מטריצת משקלות W, ומתבצעת איזושהי אקטיבציה לא-ליניארית G. נשים לב שכל קשת ברשת היא פרמטר, היא אלמנט במטריצת משקולות.

H האינפוטים משכבת הביניים בתוספת לביאס, מגיעים אל שכבת האאוטפוט הסופית. שכבה זה מבצעת את המכפלה הפנימית בין האינפוטים לכל אחת מעמודות מטריצת משקלות W השניה, ומתבצעת אקטיבצית סופטמקס להוצאה של הסתברויות בין 0 ל-1.

כל זה צעד פורוורד של הרשת, וכעת מחשבים את הגרדיאנט באמצעות פעפוע לאחור על כל אחד מהפרמטרים במטריצות המשקלות W1 ו-W2, ומתבצע גרדיאנט דיסנט, בדיוק איך שמימשנו עבור הרשת הפשוטה מקודם שם היה לנו נוירון אחד בלבד.
:::
:::

---

- Notice we are not in Logistic Regression land anymore
- The bias term (intercept) is re-instated

```{python}
#| eval: true

import tensorflow as tf
from tensorflow import keras

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
```
```{python}
#| eval: true
#| echo: true

model = Sequential()
model.add(Dense(4, input_shape=(X.shape[1], ), activation='sigmoid'))
model.add(Dense(2, activation='softmax'))
sgd = SGD(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
```

```{python}
model.fit(X, y_categorical, batch_size=100, epochs=50)
```
:::: {.fragment .fade-in}
::: {.fragment .callout-tip}
This is the MLP (Multi-Layer Perceptron).

Guess how long it's been around.
:::
::::
:::: {.fragment .fade-in}
::: {.fragment .callout-tip}
Even now, the forward step is a simple formula:

$$\hat{p}_{n \times C} = softmax\{[\mathbb{1} \vdots \sigma([\mathbb{1} \vdots X]W^{(1)})]W^{(2)}\}$$
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נשים לב שכבר התרחקנו מרגרסיה לוגיסטית מרחק די רב. הרשת שלפנינו, עם מספיק נוירונים בשכבת הביניים, יכולה כבר למדל יחסים לא-ליניאריים מורכבים ביותר בין X ל-Y.

מבחינת קוד בקראס, כל מה שנוסף כאן הוא עוד קריאה לmodel.add שם אנחנו מוסיפים עוד שכבת dense פשוטה עם 4 נוירונים ואיזושהי אקטיבצית זיגמויד.

המודל הזה נקרא multi layer perceptron או MLP, והוא קיים שנים רבות. המימוש הראשון שלו נצפה כבר ב1958 על-ידי מדען בשם פרנק רוזנבלט. אולם לקח שנים והיווצרות נתונים מספיק גדולים ומחשוב יעיל מספיק כדי להביא מודל זה לשימוש הנרחב שאנו רואים כיום.

ואפילו כעת, נשים לב שיש כאן נוסחה לא מאוד מורכבת. ניקח את מטריצת הנתונים X, נוסיף לה עמודה של אחדים, נכפיל במטריצה W1, נעשה על זה איזושהי אקטיבציה למשל זיגמויד, נוסיף שוב עמודת אחדים לחותך, נכפול במטריצת W2 ונבצע שוב אקטיבצית סופטמקס לקבלת הסתברויות.
:::
:::

---

Call `model.summary()` and see that you can calculate the number of params yourself:

```{python}
#| eval: true
#| echo: true

model.summary()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
המודל שלנו נהפך למורכב יותר ויותר, ואנחנו צריכים לראות כמה פרמטרים הוא כבר כולל.

מומלץ לקרוא לmodel.summary ולנסות לחשב לבד איך הגענו למספר הזה של פרמטרים.

(חישוב ידני על השקף)
:::
:::

```{python}
# See that it makes sense:
pred = model.predict(X, verbose=0)

pred[:3]

```
```{python}
W1, b1, W2, b2 = model.get_weights()

W1.shape # (2, 4)
b1.shape # (4,)
W2.shape # (4, 2)
b2.shape # (2,)

W1 = np.vstack([b1, W1])
W2 = np.vstack([b2, W2])

W1.shape # (3, 4)
W2.shape # (5, 2)

# Get X ready with an intercept column
Xb = np.hstack((np.ones(n).reshape((n, 1)), X))
Xb.shape # (1000, 3)
```

```{python}
Z = 1/(1 + np.exp(-np.dot(Xb, W1)))
Zb = np.hstack((np.ones(n).reshape((n, 1)), Z))
Z2_exp = np.exp(np.dot(Zb, W2))
Z2_exp_sum = Z2_exp.sum(axis=1)[:, None]
pred_manual = Z2_exp / Z2_exp_sum

pred_manual[:3]
```

---

### Activation Functions

::::: {.columns}
:::: {.column width="50%"}
$g(z)=\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$

```{python Tanh}
#| eval: true
#| echo: true
#| code-fold: true

plt.clf()
plt.plot(X1, (np.exp(X1) - np.exp(-X1)) / (np.exp(X1) + np.exp(-X1)))
plt.show()
```
::::

:::: {.column width="50%"}
$g(z)=\text{ReLU}(z)=max(z,0)$

```{python ReLU}
#| eval: true
#| echo: true
#| code-fold: true

plt.clf()
plt.plot(X1, np.maximum(X1, 0))
plt.show()
```

::::
:::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשלב הזה כדאי לומר שכיום אנחנו פחות משתמשים בפונקצית האקטיבציה זיגמויד לשכבות הביניים. שתי פונקציות אקטיבציה לא-ליניאריות נפוצות יותר נמצאות לפנינו: tanh ו-relu.

tanh נמצאת בשימוש נרחב ברשתות לעיבוד שפה, היא לוקחת קלט ממשי כלשהו ודוחסת אותו להיות בין מינוס לפלוס 1.

relu היא למעשה פונקצית ציר, או hinge. היא מאפסת אינפוט שלילי, ומשאירה אינפוט חיובי כפי שהוא.

שתי הפונקציות לא-לינאריות אבל אין להן פרמטרים והגזירה שלהן קלה.
:::
:::

---

### What about linear activations?

See HW.

::: {.callout-note}
See it in python!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האם אנחנו יכולים להשתמש בפונקצית אקטיבציה ליניארית? אם נשתמש בפונקצית אקטיבציה ליניארית, אנחנו נשארים במודל ליניארי! על כך תראו עוד בשיעורי הבית.

בואו נראה כיצד הרשת שלנו עובדת בפייתון.
:::
:::

---

## Add Regularization {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### L1/L2 Regularization

::: {.incremental}
- You might have noticed neural networks intice you to add more and more params.

- Therefore, NN are infamous for overfitting the training data, and some kind of regularization is a must.

- Instead of minimizing some loss $L$ (e.g. Cross Entropy) we add a penalty to the weights: $\min_W{L(y, f(X; W)] + P(W)}$

- Where $P(W)$ would typically be:
  - $P_{L_2}(W)=\lambda \sum_{ijk}(W^{(k)}_{ij})^2$
  - $P_{L_1}(W)=\lambda \sum_{ijk}|W^{(k)}_{ij}|$
  - or both (a.k.a Elastic Net, but not quite): $P_{L1L2}(W) = \lambda_1 \sum_{ijk}(W^{(k)}_{ij})^2 + \lambda_2 \sum_{ijk}|W^{(k)}_{ij}|$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כפי שאולי הבחנתם, זה קל מאוד להוסיף עוד ועוד שכבות ברשת נוירונים ולכן עוד ועוד פרמטרים. רשתות נוירונים ידועות לשמצה ביכולת שלהן לבצע אוברפיטינג לנתונים, ולכן חייבים לבצע איזושהי רגולריזציה על מרחב הפרמטרים.

רגולריזציה יכולה להתבצע למשל באמצעות הוספת איזושהי פונקצית עונש, או פנלטי, על הפרמטרים של המודל, שתתווסף לפונקצית ההפסד. צורה כזו אולי מוכרת לכם ממודלים לרגרסיה ליניארית כמו רידג' או לאסו.

הפנאלטי יכול להיות איזשהו עונש על נורמת L2 של וקטור או מטריצת המקדמים. כאן העונש מתבצע באמצעות פרמטר למדא. ככל שלמדא יהיה גדול יותר העונש על פרמטרים גדולים מדי יהיה גדול יותר והמודל לא ירשה לעצמו להתאים פרמטרים גדולים מדי, הוא יהיה צנוע יותר.

עונשים אחרים יכולים להיות על נורמת L1 או על שילוב של שני אלה.
:::
:::

---

L1/L2 Regularization in `Keras`:

```{python}
#| echo: true
#| code-line-numbers: "|5-6|8|"

from tensorflow.keras import regularizers

model = Sequential()
model.add(Dense(4, input_shape=(X.shape[1], ), activation='relu',
  kernel_regularizer=regularizers.l1(0.01),
  bias_regularizer=regularizers.l2(0.01)))
model.add(Dense(2, activation='softmax',
  kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))
sgd = SGD(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
model.fit(X, y_categorical, batch_size=100, epochs=50, verbose=0)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בקראס ניתן להוסיף רגולריזציה על המשקולות בכל שכבה ושכבה. כאן אנחנו מדגימים את כל אחד מהסוגים שדיברנו עליהם.
:::
:::

---

### Dropout

::: {.incremental}
- How to take neurons with a grain of salt?

- During each epoch, individual neurons are either "dropped out" of the net with probability $1-p$ (i.e. their weight is zero) or kept with probability $p$, so that a reduced network is left.
:::
::: {.fragment}
![](images/dropout.png){width=40%}
:::

::: {.fragment}
::: {.callout-important}
During prediction no Dropout is performed, but neurons output is scaled by $p$ to make it identical to their expected outputs at training time.
:::
:::
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
רגולריזציה יכולה להיות גם לא קשורה הישר לפונקצית ההפסד, היא יכולה להיות גם אלגוריתמית. לדוגמא: דרופאאוט.

כאשר אנחנו מאמנים רשת עם מנגנון דרופאאוט, אנחנו למעשה מכבים נוירונים בשכבה באופן אקראי בכל אפוק או איטרציה. בסיכוי מסוים P אנחנו פשוט מאפסים את האאוטפוט שלהם, ומכאן השם drop out.

בזמן חיזוי על דאטאסט חדש, כל הנוירונים פעילים, והאאוטפוט שלהם מוכפל בP
:::
:::

---

Why does it work?

::: {.incremental}
- You could look at Dropout as an ensemble of neural networks! Each neuron can either count or not at each training step, so after 1K training steps you have virtually trained 1K slightly different models out of $2^N$ possible (where $N$ is no. of neurons).

- Another explanation uses numerical analysis terms: with each epoch we "break" to randomly look at "close" solutions to the current optimal solution, this is increases our chances of reaching a global optimum

- Dropout in `Keras` (the `rate` parameter is the "fraction of the input units to drop"):
:::

::: {.fragment}
```{python}
#| echo: true
#| code-line-numbers: "|5|"

from tensorflow.keras.layers import Dropout

model = Sequential()
model.add(Dense(4, input_shape=(X.shape[1], ), activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
sgd = SGD(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
model.fit(X, y_categorical, batch_size=100, epochs=50, verbose=0)
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
למה דרופאאוט עובד?

אפשר לחשוב על דרופאאוט כאימון על אנסמבל של רשתות בדומה למודל רנדום פורסטס שאנחנו מכירים. בכל איטרציה המודל רואה רשת בארכיטקטורה אחרת והצורה הסופית היא מעין ממוצע של הרבה רשתות כאלה.

אפשר גם להסביר את היעילות של דרופאאוט באמצעות מונחים מתחום האנליזה הנומרית. בכל איטרציה אנחנו שוברים כיוון לעבר פתרון שכן בצורה אקראית ובכך גדל הסיכוי שנכסה את מרחב הפרמטרים ונגיע לנקודת אופטימום גלובלית או טובה יותר.

בקראס אנחנו מוסיפים דרופאאוט כך.
:::
:::

---

### Early Stopping

Since NN are trained iteratively and are particularly useful on large datasets it is common to monitor the model performance using an additional validation set, or some of the training set. If you see no improvement in the model's performance (e.g. decrease in loss) for a few epochs - stop training.

```{python}
#| echo: true
#| code-line-numbers: "|8|11-12|"
 
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential()
model.add(Dense(4, input_shape=(X.shape[1], ), activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
sgd = SGD(learning_rate=0.1)
callbacks = [EarlyStopping(monitor='val_loss', patience=5)]
model.compile(loss='categorical_crossentropy', optimizer=sgd)

model.fit(X, y_categorical, batch_size=100, epochs=50,
  validation_split=0.2, callbacks=callbacks)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
רגולריזציה אלגוריתמית נוספת שנמצאת בשימוש נרחב היא early stopping או עצירה מוקדמת. במהלך האימון שמרו בצד עוד סט אקראי קטן של נתונים עליו המודל לא יתאמן, אלא רק יבדוק אחרי כל איטרציה האם יש אכן ירידה בפונקצית ההפסד, הלוס, על דאטה שהמודל לא ראה.

אם אין ירידה במשך כך וכך צעדים, יפסיק המודל להתאמן, כי אנחנו חושדים שבשלב זה הוא רק עושה אוברפיטינג למדגם הלמידה.

בקראס early stopping הוא קריאה או קולבאק. ניתן לאמן את המודל באמצעות קריאה לכמה קולבקס כאלה וגם לכתוב קולבקס בעצמנו. כאן אנחנו מגדירים שearly stopping ינטר את הולידיישן לוס, ואם תוך 5 צעדים אין שיפור בפונקצית ההפסד על תת מדגם זה, יעצור.
בקריאה לפיט, אנחנו מבקשים שיחלק בתחילת הריצה את מדגם הלמידה ל-20 אחוז ולידיישן עליו המודל לא יתאמן ו-80 אחוז טריינינג, באמצעות הפרמטר validation_split. וכדי לבצע עצירה מוקדמת אנחנו מזינים את הקולבק בתוך רשימה לארגומנט callbacks.
:::
:::

---

## Keras {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Keras is an API

- [Keras](https://keras.io/) is a high-level API "designed for human beings, not machines" developed by [François Chollet](https://twitter.com/fchollet)
- It sits upon a popular DL backends such as [Tensorflow](https://www.tensorflow.org/), also by Google, and [PyTorch](https://pytorch.org/) by Meta

```{python}
#| echo: true

import tensorflow as tf
from tensorflow import keras
```

- "ease of use does not come at the cost of reduced flexibility"
- Seamless integration with the Pandasverse

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת נרחיב קצת יותר על היכולות של קראס, עם נתונים אמיתיים.

קראס הוא בעצמו API, שנכתב על ידי מהנדס מגוגל בשם פרנסואה שולה. קראס עוטפת תוכנות פופולריות ללמידה עמוקה כמו טנסורפלואו, גם כן נכתבה על ידי מהנדסי גוגל, או פייטורץ' שמגיעה מחברת מטא.

קראס מאפשרת כפי שראינו להגדיר רשתות מורכבות מבלי להיכנס לברזלים של תהליך האופטימיזציה, ואם רוצים יותר גמישות בארכיטקטורה תמיד אפשר לעבור לכתוב בטנזורפלואו.

קראס מתכתבת באופן ישיר עם החבילות המוכרות שלנו של נאמפיי ופנדאס.
:::
:::

---

### Malaria!

::: {.incremental}
- The [Malaria](https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html) dataset contains over 27K (processed and segmented) cell images with equal instances of parasitized and uninfected cells, from hundreds of patients in Bangaladesh. The images were taken by a mobile application that runs on a standard Android smartphone attached to a conventional light microscope. The goal is "reduce the burden for microscopists in resource-constrained regions and improve diagnostic accuracy".

- This dataset is part of the [`tensorflow_dataset`](https://www.tensorflow.org/datasets) library which gives you easy access to dozens of varied datasets.

- Here we take only ~10% of the images as a Numpy array and resize them all to 100x100 pixels, for the sake of speed.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסתכל על מסד נתונים מעניין בשם מלריה. במסד נתונים זה יש כ27 אלף תמונות של תאים, שיכולים להיות נגועים במלריה או לא. נרצה לחזות האם תא נגוע במלריה או לא, כלומר בעית קלסיפיקציה של שני קלאסים. המטרה במודל חיזוי לבעיה כזו היא להטמיע אותו בפלאפונים של רופאים שמטפלים בפציינטים באזורים נחשלים לאבחון מהיר של מלריה. נציין עוד כי הדאטה הוא balanced, כלומר חצי מהתאים נגועים במלריה וחצי לא.

מסד הנתונים הזה נלקח ממאגר עצום של נתונים שמתאימים במיוחד ללמידה עמוקה שנקרא tensorflow datasets. מומלץ ללחוץ על הקישור כאן ולעלעל במגוון הנתונים העשיר שניתן להוריד בשורה או שתיים של קוד אל כל מחברת ולהריץ עליהם מודלים.

כאן נשתמש רק בעשרה אחוזים של הנתונים, כלומר 2500 תמונות, ונאחד אותן לגודל של 100 על 100 פיקסלים. מאוחר יותר נראה ביצועים מרשימים אף יותר באמצעות רשתות מתקדמות יותר, על כל סט התמונות.
:::
:::

---

```{python Malaria}
#| echo: true
#| eval: true

import tensorflow_datasets as tfds
from skimage.transform import resize

malaria, info = tfds.load('malaria', split='train', with_info=True)
fig = tfds.show_examples(malaria, info)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בקטע קוד זה אנחנו מייבאים את הספרייה tansorflow datasets, ומורידים את סט הנתונים כאובייקט מיוחד של טנזורפלואו איתו קל לעבוד, ומראים דוגמאות של תאים מתוך המדגם. ניתן לראות שחלק מהתאים מסווגים כparasitized כלומר נגועים במלריה, וחלק כ-uninfected כלומר לא נגועים.
:::
:::

---

```{python}
#| eval: true
#| echo: true
#| code-line-numbers: "|16-17|"

from sklearn.model_selection import train_test_split

images = []
labels = []
for example in tfds.as_numpy(malaria):
  images.append(resize(example['image'], (100, 100)))
  labels.append(example['label'])
  if len(images) == 2500:
    break
  
X = np.array(images)
y = np.array(labels)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

X_train = X_train.flatten().reshape((X_train.shape[0], -1))
X_test = X_test.flatten().reshape((X_test.shape[0], -1))

print(X_train.shape)
print(X_test.shape)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בקטע קוד זה אנחנו הופכים כל תמונה ותמונה להיות מערך של נאמפיי, ועוצרים ב-2500 תמונות. לאחר מכאן אנחנו מחלקים את X ו-Y, התמונות והסיווגים שלהן ל80 אחוז מדגם למידה ו20 אחוז מדגם טסט.

עוד נלמד על תמונות וייצוגן. כרגע מדובר במערך תלת מימדי בגודל 100 על 100 פיקסלים, על 3 שכבות צבע: אדום, ירוק וכחול, או RGB. מודל הMLP שלנו לא יודע להתמודד עם נתונים שאינם טבלאיים ולכן יש צורך לשטח אותם, ונקבל בסופו של דבר מטריצת X_train עם 2000 שורות ו-30 אלף עמודות שמייצגות שרשור של 30 אלף פיקסלים: 100 כפול 100 כפול 3.

במדגם הטסט יש לנו 500 תמונות עם צורה דומה.
:::
:::

---

```{python}
#| eval: true
#| echo: true

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(penalty='none', max_iter=1000, random_state=42)
lr = lr.fit(X_train, y_train)

test_acc = lr.score(X_test, y_test)
print(f'Test accuracy for LR: {test_acc:.3f}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו מבצעים רגרסיה לוגיסטית באמצעות sklearn. המימוש בsklearn יעיל כך שאין לו בעיה להתמודד אפילו עם 30 אלף משתנים שיש כאן. בחיזוי על מדגם הטסט לעומת זאת, כשאנו מבקשים את הscore אנחנו מקבלים את אחוז הדיוק הכללי על הטסט, הaccuracy, והוא די מאכזב. אמנם אין ספק שמודל הרגרסיה הלוגיסטית שהוא מודל ליניארי למד משהו, והדיוק גבוה בהרבה מדיוק אקראי של 50 אחוז, אך דיוק של בין 60 ל-70 אחוזים איננו מרשים.
:::
:::

---

#### The `Sequential` API

```{python}
#| eval: true
#| echo: true

from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(300, input_shape=(30000,), activation='relu', name='my_dense_layer'))
model.add(Dense(100, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```

Alternatively we could:

```{python}
#| eval: true
keras.backend.clear_session()
```

```{python}
#| echo: true

model = Sequential([
  Dense(300, input_shape=(30000,), activation='relu'),
  Dense(100, activation='relu'),
  Dense(50, activation='relu'),
  Dense(1, activation='sigmoid')
])
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בקראס נאמן רשת עמוקה עם 3 שכבות ביניים: שכבה עם 300 נוירונים, שכבה עם 100 נוירונים ושכבה עם 50 נוירונים. לכל אחת מהשכבות אקטיבצית רלו, ולבסוף נוספת שכבת האאוטפוט עם נוירון בודד עם אקטיבצית זיגמויד שתפקידו להחזיר לנו בסוף כמות בין 0 ל-1, בה אפשר לראות כמעין הסתברות שהתא נגוע במלריה.

ניתן כפי שנראה כאן לכתוב את הרשת כרשימה משורשרת של שכבות, ובהמשך נראה דרכים נוספות להגדיר רשתות מורכבות יותר.
:::
:::

---

Make sure you get these numbers:

```{python}
#| eval: true
#| echo: true

model.summary()
```

::: {.callout-tip}
Are you at all worried?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה שוב שאנחנו מבינים כמה פרמטרים יש במודל שלנו, ומאיפה הגיעו המספרים תחת model.summary.

(חישוב ידני)

האם אתם מודאגים? 9 מיליון פרמטרים ברשת שלנו, ועוד לא דיברנו על בעיות אחרות מעצם הגדרתה, והאם מתאים להתייחס לתמונות כמו כאן, כאוסף של פיקסלים בלי תלות ברורה ביניהם.
:::
:::

---

Access layers and their weights:

```{python}
#| eval: true
#| echo: true

model.layers
```

```{python}
#| eval: true
#| echo: true

model.layers[0].name
```

```{python}
#| eval: true
#| echo: true

W1, b1 = model.get_layer('my_dense_layer').get_weights()

print(W1.shape)
W1
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר כעת הרשת מאותחלת בפרמטרים אקראיים, וניתן לגשת אל השכבות עצמן ואל המשקלות שלהן.

באמצעות model.layers ניתן לגשת לרשימה של שכבות, לכל אחת מהן שם, לדוגמא לשכבה הראשונה קוראים my_dense_layer כי ככה הגדרנו.

אם נבקש את השכבה הזאת באמצעות הפונקציה get_layer, ועל השכבה נבקש get_weights, נקבל את מטריצת המשקולות של השכבה ואת וקטור הביאס שלה. כאן אנחנו מדפיסים את מטריצת המשקולות W ואת הגודל שלה לצורך הדגמה ולודא שהבנו.

:::
:::

---

Compiling your model:

```{python}
#| eval: true
#| echo: true

model.compile(loss="binary_crossentropy",
  optimizer="adam",
  metrics=["accuracy"])
```

::: {.fragment .fade-in}
For more initialization schemes, losses, metrics and optimizers:

* [https://keras.io/api/layers/initializers/](https://keras.io/api/layers/initializers/)
* [https://keras.io/api/losses/](https://keras.io/api/losses/)
* [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)
* [https://keras.io/api/metrics/](https://keras.io/api/metrics/)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הקומפילציה של המודל נעשית כרגיל. כאן אנחנו משתמשים בפונקצית הפסד שמתאימה לבעית קלסיפיקציה בינארית כפי שראינו binary_crossentropy.

בנוסף, הפעם אנחנו משתמשים באופטימייזר מעט שונה אך פופולרי שנקרא אדם.

באמצעות הארגומנט metrics נבקש גם מהרשת לדווח לנו בכל איטרציה רשימה של מטריקות נוספות לבחירתנו. כאן אנחנו מבקשים שבצד הלוס הרגיל של הרשת יודפס גם הדיוק, הaccuracy.

לפני שנריץ מומלץ ללחוץ על הלינקים השונים שמופיעים כאן כדי להתרשם ממגוון האתחולים, פונקציות ההפסד, האופטימייזרים והמטריקות שלקראס יש להציע. לכל אחד מאלה ניתן גם לרשום פונקציה שמתאימה לצרכים הספציפיים של הרשת שלנו, וזאת מבלי לכתוב בטנזורפלואו שהוא מעט מורכב יותר לשימוש.
:::
:::

---

Fitting the model:

```{python}
#| echo: true
#| eval: true

from tensorflow.keras.callbacks import EarlyStopping

callbacks = [EarlyStopping(monitor='val_loss', patience=5,
  restore_best_weights=True)]

history = model.fit(X_train, y_train,
  batch_size=100, epochs=50,
  validation_split=0.1, callbacks=callbacks, verbose=0)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת נריץ את המודל עם early stopping שמנטר 10 אחוז מדגם ולידיישן. אם לא יראה שיפור בפונקצית ההפסד על מדגם זה במשך 5 צעדים הלמידה תיעצר. כאן אנחנו גם מבקשים מקראס לשמור את סט המשקולות שמתאים לאיטרציה עם ההפסד הנמוך ביותר שראינו עד כה.

דבר נוסף שאנו עושים זה להחזיר את הקריאה לmodel.fit לתוך אוביקט שנקרא history.
:::
:::

---

See later the `history` object's many fields.

```{python History}
#| eval: true
#| echo: true

import pandas as pd

pd.DataFrame(history.history).plot(figsize=(10, 6))
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לאחר הריצה מומלץ לעטוף את האוביקט history כדאטה פריים של פנדאס ולהדפיס את הלוס לאורך האיטרציות ואולי עוד מטריקות כמו accuracy, על מדגם הטריין ועל מדגם הולידיישן. לראות שהאימון נראה טוב ואין אילו אנומליות שלא צפינו.

כאן ניתן לראות שההפסד, הלוס יורד על מדגם הלמידה ומדגם הולידיישן, וכאשר אין שיפור על מדגם הולידיישן במשך 5 צעדים מופסקת הלמידה. ניתן לראות גם את הaccuracy על שני המדגמים. היא עולה עד גבול מסוים, ובכל מקרה איננה מרשימה.
:::
:::

---

Evaluate on test set:

```{python}
#| eval: true
#| echo: true

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=False)
print(f'Test accuracy for NN: {test_acc:.3f}')
```

```{python}
#| eval: true
#| echo: true
#| code-line-numbers: "|3|5|"

from sklearn.metrics import confusion_matrix

y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).reshape(y_test.shape)
pd.DataFrame(
  confusion_matrix(y_test, y_pred), 
  index=['true:yes', 'true:no'], 
  columns=['pred:yes', 'pred:no']
)
```

::: {.callout-note}
It's OK to be underwhelmed.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי לראות כיצד המודל ביצע על מדגם הטסט שלנו, ניתן להריץ אותו על X_test ו-y_test באמצעות קריאה לevaluate. כפי שניתן לראות אחוז הדיוק של הרשת על תמונות המלריה הוא לא הרבה יותר טוב מאחוז הדיוק של רגרסיה לוגיסטית, ובריצות מסוימות יכול להיות גם נמוך יותר.

ניתן גם לקרוא לmodel.predict, כדי לקבל את הציונים בין 0 ל-1 החזויים בעצמם. את הציונים האלה ניתן להשוות לאיזשהו סף קטאוף כדי לדעת בדיוק אילו תצפיות של תאים נחזות כ1 כלומר יש להן מלריה, ואילו 0. על הוקטור הזה ניתן להפעיל את הפונקציה confusion_matrix המוכרת לנו מsklearn כדי לראות בדיוק איך הגענו לאחוז הדיוק הנוכחי.

שוב נעיר שאחוז הדיוק איננו מרשים וזה בסדר להכיר בכך. בשיעור הבא נלמד על ארכיטקטורה של למידה עמוקה שמתאימה הרבה יותר לנתונים של תמונות, ושם נראה אחוז דיוק מרשים הרבה יותר.
:::
:::

---

Tuning params:

```{python}
#| echo: true
#| code-line-numbers: "|5|"

from tensorflow.keras.layers import InputLayer
from tensorflow.keras.optimizers import SGD
from scikeras.wrappers import KerasClassifier

def malaria_model(n_hidden, n_neurons, lrt):
  model = Sequential()
  model.add(InputLayer(input_shape=(30000, )))
  for layer in range(n_hidden):
    model.add(Dense(n_neurons, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss="binary_crossentropy",
    optimizer=SGD(learning_rate=lrt),
    metrics=["accuracy"])
  return model

keras_clf = KerasClassifier(model=malaria_model, n_hidden=1, n_neurons=30, lrt=3e-3)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר בשלב זה ודאי הבחנתם כמה החלטות אנחנו צריכים לעשות, כמה היפר-פרמטרים עלינו לכוון. כמה שכבות, כמה נוירונים בכל שכבה, מהו קצב הלמידה, באילו פונקציות אקטיבציה להשתמש ובאילו שיטות רגולריזציה.

מומלץ להשתמש בשיטות קיימות לבצע כוונון או טיונינג של פרמטרים. כאן אנחנו משתמשים בספריה שנקראת scikeras, ובה יש קלאס שנקרא KerasClassifier. הקלאס הזה יכול לעטוף פונקציה לבניית מודל כמו שלנו, והפונקציה תקבל כארגומנטים מספר היפר-פרמטרים שאנחנו רוצים לכוונן. כאן למשל אנחנו מזינים לפונקציה את הפרמטרים מספר שכבות, מספר נוירונים בהנחה שיהיה זהה בכל שכבה, וקצב למידה, learning_rate.
:::
:::

---

```{python}
#| echo: true
#| code-line-numbers: "|4-8|10-13|"

from scipy.stats import reciprocal
from sklearn.model_selection import RandomizedSearchCV

params = {
  'n_hidden': [0, 1, 2, 3],
  'n_neurons': np.arange(1, 100),
  'lrt': reciprocal(3e-4, 3e-2)
}

rnd_search_cv = RandomizedSearchCV(keras_clf, params, cv=5,
  n_iter=10)
rnd_search_cv.fit(X_train, y_train, epochs=50,
  validation_split=0.1, callbacks=callbacks)

print(f'Best test accuracy: {rnd_search_cv.best_score_:.2f}')
print(f'Best params: {rnd_search_cv.best_params_}')
```

```{python}
#| eval: true

# as this takes a while...
d = {'lrt': 0.0004918307063493132, 'n_hidden': 3, 'n_neurons': 17}
print(f'Best test accuracy: {0.69:.2f}')
print(f'Best params: {d}')
```

See also sklearn's [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and [KerasTuner](https://keras.io/keras_tuner/) for a more robust solution.

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת ניתן לעטוף את אוביקט הקראס קלסיפייר שלנו בקלאס אחר מsklearn לביצוע חיפוש רנדומלי במרחב הפרמטרים שהזנו, RandomizedSearchCV.

קלאס זה יקבל מילון עם האפשרויות לכל פרמטר, כאן הוא נקרא params.

לאחר מכן נגדיר איך אנחנו רוצים שירוצו כל המודלים האלה. כאן אנחנו מגדירים שאנחנו רוצים 10 איטרציות שבכל אחת נגריל סט של פרמטרים מהמרחב שהגדרנו. נריץ כל מודל על פני חמישה פולדים של דאטה בפרוצדורה שלcross validation.

לבסוף נדפיס את סט הפרמטרים שהביאו לתוצאה הטובה ביותר בממוצע על פני חמשת הפולדים, וזה יהיה הסט בוא נשתמש בריצה על כל מדגם הנתונים לפני חיזוי סופי על הטסט סט.

אנחנו רואים שגם סט הנתונים הטוב ביותר לא הביא לחיזוי על מדגם טסט טוב בהרבה ממה שראינו, כאן הגענו ל69 אחוזים accuracy. לעוד פתרונות מומלץ להסתכל על ספרית KerasTuner שנותנת עוד אפשרויות לכוונון היפר-פרמרטרים.
:::
:::

---

Saving and restoring a model:

```{python}
#| eval: true
#| echo: true

model.save('malaria.h5')
```

Then:

```{python}
#| eval: true
#| echo: true

model = keras.models.load_model('malaria.h5')
model.predict(X_test[:3], verbose=0)
```

The HDF5 model saves the model's architecture and hyperparameters, and all weights matrices and biases.

Also see the `ModelCheckPoint()` callback.

```{python}
#| eval: true

import os
os.remove('malaria.h5')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לפני שנסיים ודאי הבחנתם שמודלים של רשתות נוירונים יכולים לקחת זמן לא מועט לאימון. המודלים הסופיים עצמם יכולים להיות גם די כבדים, כאן ראינו מודל די פשוט אך הוא עדיין מסתכם ב9 מיליון פרמטרים שיש לשמור בזיכרון אם רוצים להריץ את המודל בסביבת פרודקשן, לדוגמא באתר באינטרנט על תמונות נכנסות.

מודלים פשוטים של קראס באים עם מתודה נוחה לשמירת המודל על הדיסק ולהעלאתו: save ו-load_model. כאן אנחנו שומרים מודל לדיסק בתור קובץ מסוג HDF5, וכשאנחנו מעלים אותו שוב עם פונקצית load_model הוא מוכן לחיזוי ואפילו המשך אימון.

:::
:::

---

### Few Excellent Books

:::: {.columns}
::: {.column width="50%"}
<img src = "images/geron_cover.png" style="width: 70%">
:::
::: {.column width="50%"}
<img src = "images/chollet_cover.png" style="width: 70%">
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסיים בהמלצות לספרים מצוינים בנושא, שניהם משתמשים בקראס לאימון רשתות, ואחד מהם נכתב אפילו בידי המחבר של קראס, פרנסואה שולה.
:::
:::
