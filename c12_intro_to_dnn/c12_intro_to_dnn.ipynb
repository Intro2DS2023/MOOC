{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression with manual Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X1 = np.linspace(-4, 4) # for plotting\n",
        "\n",
        "n = 1000\n",
        "q = 2\n",
        "X = np.random.normal(size = n * q).reshape((n, q))\n",
        "beta = [1.0, 2.0]\n",
        "p = 1 / (1 + np.exp(-np.dot(X, beta)))\n",
        "y = np.random.binomial(1, p, size = n)\n",
        "\n",
        "def plot_sim(plot_beta_hat=True):\n",
        "  plt.clf()\n",
        "  plt.scatter(X[:, 0], X[:, 1], c = y)\n",
        "  plt.plot(X1, -X1 * beta[0]/beta[1], linestyle = '--', color = 'red')\n",
        "  if plot_beta_hat:\n",
        "    plt.plot(X1, -X1 * beta_hat[0]/beta_hat[1], linestyle = '--')\n",
        "  plt.xlabel('X1')\n",
        "  plt.ylabel('X2')\n",
        "  if plot_beta_hat:\n",
        "    title = 'Guess: %.2f * X1 + %.2f * X2 = 0' % (beta_hat[0], beta_hat[1])\n",
        "  else:\n",
        "    title = 'Ideal: 1 * X1 + 2 * X2 = 0'\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_sim(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initial guess for beta\n",
        "\n",
        "beta_hat = np.ones(q) # [1, 1]\n",
        "\n",
        "plot_sim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's do 1 iteration\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))\n",
        "grad = -np.dot(X.T, (y - p_hat))\n",
        "beta_hat = beta_hat - alpha * grad\n",
        "\n",
        "plot_sim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's do 10 more iterations\n",
        "\n",
        "for i in range(10):\n",
        "  p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))\n",
        "  grad = -np.dot(X.T, (y - p_hat))\n",
        "  beta_hat = beta_hat - alpha * grad\n",
        "\n",
        "plot_sim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see NLL -l(beta) through iterations\n",
        "\n",
        "alpha = 0.001\n",
        "beta_hat = np.array([-2.5, -2.5])\n",
        "betas = [beta_hat]\n",
        "ls = []\n",
        "for i in range(50):\n",
        "  p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))\n",
        "  nll = -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))\n",
        "  ls.append(nll)\n",
        "  grad = -np.dot(X.T, (y - p_hat))\n",
        "  beta_hat = beta_hat - alpha * grad\n",
        "  betas.append(beta_hat)\n",
        "\n",
        "plt.plot(range(50), ls)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(r\"$-l(\\beta)$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Even fancier, visualize the actual gradient descent in the beta space:\n",
        "\n",
        "betas_arr = np.array(betas)\n",
        "m = 10\n",
        "beta1 = np.linspace(-3.0, 3.0, m)\n",
        "beta2 = np.linspace(-3.0, 3.0, m)\n",
        "B1, B2 = np.meshgrid(beta1, beta2)\n",
        "L = np.zeros((m, m))\n",
        "for i in range(m):\n",
        "  for j in range(m):\n",
        "    beta_hat = np.array([beta1[i], beta2[j]])\n",
        "    p_hat = 1 / (1 + np.exp(-np.dot(X, beta_hat)))\n",
        "    L[i, j] = -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))\n",
        "fig, ax = plt.subplots(1,1)\n",
        "cp = ax.contourf(B1, B2, L)\n",
        "cb = fig.colorbar(cp)\n",
        "ax.set_title(r'$-l(\\beta)$ gradient descent')\n",
        "ax.set_xlabel(r'$\\beta_1$')\n",
        "ax.set_ylabel(r'$\\beta_2$')\n",
        "ax.plot(betas_arr[:, 0], betas_arr[:, 1], marker='x', color='white')\n",
        "ax.plot([beta[0]], [beta[1]], marker='x', color='red', markersize=20, markeredgewidth=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression as NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|1-6|8-12|14-15|17-21|\"\n",
        "\n",
        "def forward(X, y, beta_hat):\n",
        "  z = np.dot(X, beta_hat)\n",
        "  p_hat = 1 / (1 + np.exp(-z))\n",
        "  l = y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)\n",
        "  nll = -np.sum(l)\n",
        "  return p_hat, nll\n",
        "\n",
        "def backward(X, y, p_hat):\n",
        "  dldz = y - p_hat\n",
        "  dzdb = X.T\n",
        "  grad = -np.dot(dzdb, dldz)\n",
        "  return grad\n",
        "\n",
        "def gradient_descent(alpha, beta_hat, grad):\n",
        "  return beta_hat - alpha * grad\n",
        "\n",
        "def optimize(X, y, alpha, beta_hat):\n",
        "  p_hat, l = forward(X, y, beta_hat)\n",
        "  grad = backward(X, y, p_hat)\n",
        "  beta_hat = gradient_descent(alpha, beta_hat, grad)\n",
        "  return l, beta_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "\n",
        "def lr_nn(X, y, epochs):\n",
        "  beta_hat = np.array([-2.5, -2.5])\n",
        "  alpha = 0.001\n",
        "  for i in range(epochs):\n",
        "    l, beta_hat = optimize(X, y, alpha, beta_hat)\n",
        "  return l, beta_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "\n",
        "def lr_nn(X, y, epochs):\n",
        "  beta_hat = np.random.rand(X.shape[1])\n",
        "  alpha = 0.001\n",
        "  batch_size = 100\n",
        "  n = X.shape[0]\n",
        "  steps = int(n / batch_size)\n",
        "  for i in range(epochs):\n",
        "    print('epoch %d:' % i)\n",
        "    permute = np.random.permutation(n)\n",
        "    X_perm = X[permute, :]\n",
        "    y_perm = y[permute]\n",
        "    for j in range(steps):\n",
        "      start = j * batch_size\n",
        "      l, beta_hat = optimize(X_perm[start:start + batch_size, :],\n",
        "                            y_perm[start:start + batch_size],\n",
        "                            alpha, beta_hat)\n",
        "      print('Trained on %d/%d, loss = %d' % (start + batch_size, n, l))\n",
        "  return l, beta_hat\n",
        "\n",
        "l, beta_hat = lr_nn(X, y, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression as NN in `Keras`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|1|2|3|5|6-7|8|9|10|\"\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_shape=(X.shape[1], ),\n",
        "  activation='sigmoid', use_bias=False))\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
        "model.fit(X, y, batch_size=100, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See that it makes sense:\n",
        "beta_hat = model.get_weights() # Note Keras gives a list of weights!\n",
        "beta_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = model.predict(X, verbose=0)\n",
        "pred[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_manual = 1/(1+np.exp(-np.dot(X, beta_hat[0])))\n",
        "pred_manual[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adding C nuerons for C classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|3|5-6|8|\"\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_categorical = to_categorical(y)\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_shape=(X.shape[1], ),\n",
        "  activation='softmax', use_bias=False))\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X, y_categorical, batch_size=100, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See that it makes sense:\n",
        "W = model.get_weights()\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = model.predict(X, verbose=0)\n",
        "pred[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Z = np.dot(X, W[0])\n",
        "Z_exp = np.exp(Z)\n",
        "Z_exp_sum = Z_exp.sum(axis=1)[:, None]\n",
        "pred_manual = Z_exp / Z_exp_sum\n",
        "pred_manual[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adding Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_shape=(X.shape[1], ), activation='sigmoid'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X, y_categorical, batch_size=100, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See that it makes sense:\n",
        "pred = model.predict(X, verbose=0)\n",
        "\n",
        "pred[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W1, b1, W2, b2 = model.get_weights()\n",
        "\n",
        "W1.shape # (2, 4)\n",
        "b1.shape # (4,)\n",
        "W2.shape # (4, 2)\n",
        "b2.shape # (2,)\n",
        "\n",
        "W1 = np.vstack([b1, W1])\n",
        "W2 = np.vstack([b2, W2])\n",
        "\n",
        "W1.shape # (3, 4)\n",
        "W2.shape # (5, 2)\n",
        "\n",
        "# Get X ready with an intercept column\n",
        "Xb = np.hstack((np.ones(n).reshape((n, 1)), X))\n",
        "Xb.shape # (1000, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Z = 1/(1 + np.exp(-np.dot(Xb, W1)))\n",
        "Zb = np.hstack((np.ones(n).reshape((n, 1)), Z))\n",
        "Z2_exp = np.exp(np.dot(Zb, W2))\n",
        "Z2_exp_sum = Z2_exp.sum(axis=1)[:, None]\n",
        "pred_manual = Z2_exp / Z2_exp_sum\n",
        "\n",
        "pred_manual[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Malaria classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: true\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from skimage.transform import resize\n",
        "\n",
        "malaria, info = tfds.load('malaria', split='train', with_info=True)\n",
        "fig = tfds.show_examples(malaria, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| code-line-numbers: \"|16-17|\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "for example in tfds.as_numpy(malaria):\n",
        "  images.append(resize(example['image'], (100, 100)))\n",
        "  labels.append(example['label'])\n",
        "  if len(images) == 2500:\n",
        "    break\n",
        "  \n",
        "X = np.array(images)\n",
        "y = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train = X_train.flatten().reshape((X_train.shape[0], -1))\n",
        "X_test = X_test.flatten().reshape((X_test.shape[0], -1))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(penalty='none', max_iter=1000, random_state=42)\n",
        "lr = lr.fit(X_train, y_train)\n",
        "\n",
        "test_acc = lr.score(X_test, y_test)\n",
        "print(f'Test accuracy for LR: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(300, input_shape=(30000,), activation='relu', name='my_dense_layer'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.layers[0].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "W1, b1 = model.get_layer('my_dense_layer').get_weights()\n",
        "\n",
        "print(W1.shape)\n",
        "W1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "  optimizer=\"adam\",\n",
        "  metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: true\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5,\n",
        "  restore_best_weights=True)]\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "  batch_size=100, epochs=50,\n",
        "  validation_split=0.1, callbacks=callbacks, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(f'Test accuracy for NN: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| code-line-numbers: \"|3|5|\"\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).reshape(y_test.shape)\n",
        "pd.DataFrame(\n",
        "  confusion_matrix(y_test, y_pred), \n",
        "  index=['true:yes', 'true:no'], \n",
        "  columns=['pred:yes', 'pred:no']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|5|\"\n",
        "\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def malaria_model(n_hidden, n_neurons, lrt):\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer(input_shape=(30000, )))\n",
        "  for layer in range(n_hidden):\n",
        "    model.add(Dense(n_neurons, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss=\"binary_crossentropy\",\n",
        "    optimizer=SGD(learning_rate=lrt),\n",
        "    metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "keras_clf = KerasClassifier(model=malaria_model, n_hidden=1, n_neurons=30, lrt=3e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|4-8|10-13|\"\n",
        "\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params = {\n",
        "  'n_hidden': [0, 1, 2, 3],\n",
        "  'n_neurons': np.arange(1, 100),\n",
        "  'lrt': reciprocal(3e-4, 3e-2)\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_clf, params, cv=5,\n",
        "  n_iter=10)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
        "  validation_split=0.1, callbacks=callbacks)\n",
        "\n",
        "print(f'Best test accuracy: {rnd_search_cv.best_score_:.2f}')\n",
        "print(f'Best params: {rnd_search_cv.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model.save('malaria.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "\n",
        "model = keras.models.load_model('malaria.h5')\n",
        "model.predict(X_test[:3], verbose=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
