מי לא שמע על רשתות נוירונים? רשתות נוירונים עומדות בבסיס מה שרבים מכנים מהפכת האינטליגנציה המלאכותית, הAI. הן עומדות מאחורי כמה ממערכות החיזוי המתקדמות ביותר כגון מכוניות אוטונומיות וצ'טבוטים כגון צ'ט ג'יפיטי של חברת OpenAI.

עם זאת, לרבים המודל של רשתות נוירונים נשמע מרתיע בהתחלה ונטול מוטיבציה.
ביחידה זו ננסה להראות שהשד לא כזה נורא. נחזור לרגרסיה לוגיסטית, ונראה כיצד ניתן לממש אותה כרשת נוירונים. מכאן נוסיף שכבה ועוד שכבה עד שלבסוף נגיע לרשת נוירונים מודרנית.

---

ניזכר במודל הרגרסיה הלוגיסטית.

אנו צופים בN זוגות של X ו-Y. Y הוא בינארי, לדוגמא 0 או 1. וX הוא וקטור של q משתנים מסבירים.

כעת נמדל את Y בהינתן X כמשתנה ברנולי עם הסתברות p. ומאחר שהתוחלת של משתנה ברנולי עם הסתברות p היא p עצמו זה מה שאנחנו בסופו של דבר ממדלים. הבעיה היא שהסתברות היא כמות בין 0 ל1.

נבחר איזו פונקצית לינק ג'י לתוחלת, לאותה הסתברות, במקרה שלנו הלוג'יט, שהיא לוג יחס הסיכויים, ואת הפונקציה הזאת נמדל באמצעות מודל ליניארי רגיל.

בטא הוא וקטור של q מקדמים, ואותו אנחנו רוצים למצוא.

---

איך מוקטור המקדמים נחזור לקבל הסתברות? באמצעות הפונקציה ההופכית g inverse.

כאשר נשיג אומדן לוקטור המקדמים שלנו בטא האט,

1. נוכל להסביר את Y, או יותר נכון להסביר את הסיכוי שY יהיה 1. הכיוון והגודל של כל רכיב בוקטור המקדמים יגידו לנו מהי התרומה של המשתנה המתאים ללוגריתם של יחס הסיכויים של Y.

2. נוכל לחזות עבור תצפית חדשה את ההסתברות שמשתנה הY שלה הוא 1. יש לנו נוסחה לזה, נכפול את הנתונים X בוקטור המקדמים הנאמד בטא האט, ונחשב על זה את הפונקציה ההופכית של g, g inverse. אם נרצה חיזוי סופי נצטרך להשוות את ההסתברות הנחזית p_hat לאיזשהו סף קאטאוף, לדוגמא חצי. אם p_hat גדול מחצי נחזה שY יהיה 1, אחרת נחזה 0.

עד כאן ראינו ברגרסיה לוגיסטית.

---

בואו נראה איך אנחנו מוצאים אומד לוקטור המקדמים בטא.

הגישה הסטנדרטית היא למקסם פונקציה בשם הנראות. פונקציה זו מסומנת בדרך כלל ב-L גדולה, היא פונקציה של בטא בהינתן הנתונים X ו-Y. בהנחת אי-תלות בין התצפיות מדובר במכפלת ההסתברויות לקבל כל תצפית ותצפית, שבמקרה שלנו מקבלת צורה יפה שכזאת: אם Y שווה לאחת נכתוב את ההסתברות בחזקת Y, אם Y שווה אפס נכתוב את ההסתברות בחזקת 1 פחות Y.

ומהי אותה הסתברות לפי המודל? הפונקציה ההופכית של G או אחת פחות הפונקציה ההופכית של G.

הוקטור בטא האט הוא הוקטור שימקסם את הנראות ולכן נקרא אומד נראות מקסימלית, או MLE.

---

פונקצית הנראות כפי שהוא מנוסחת כרגע היא קשה למיקסום. נהוג לעבוד עם לוג הנראות, כלומר עם סכום הלוגריתמים של ההסתברויות שרשמנו.

נפשט את הפונקציה הזאת אפילו יותר כדי לקבל פונקציה שקל לגזור יותר לפי בטא.

---

נגזור את הפונקציה לפי אלמנט אחד בוקטור בטא, בטא ג'יי.

נכתוב את הנגזרת לפי כל וקטור בטא בכתיב וקטורי, מה שיקל עלינו במימוש.

את הביטוי הזה היינו רוצים להשוות לוקטור האפס ולקבל את האומד לבטא, אבל לא נקבל פתרון סגור.

בשלב זה נעבור לשיטות של אנליזה כמו ניוטון רפסון, פונקצית לוג הנראות היא קמורה ואיננה נחשבת לפונקציה קשה.

אבל אני רוצה שנסתכל על פתרון של מורד הגרדיאנט, gradient descent.

---

במקום למקסם את לוג הנראות, נעשה מינימיזציה ללוג הנראות השלילית, ה-NLL.

נתחיל בניחוש ראשוני עבור בטא האט בזמן t = 0.

הגרדיאנט, או וקטור הנגזרות החלקיות של הNLL המחושב בנקודה בטא האט, מצביע לכיוון שיפוע הירידה התלולה ביותר בנקודה זו.

נלך צעד קטן בגודל אלפא בכיוון זה. וקטור בטא האט החדש שלנו יהיה בטא האט הקודם פחות צעד אלפא בכיוון וקטור הגרדיאנט של פונקצית הלוג-נראות השלילית.

נעשה את זה במשך מספר איטרציות עד להתכנסות למינימום מקומי, במקרים מסוימים כגון המקרה שלפנינו אנחנו יכולים לדעת שמדובר בנקודת מינימום גלובלית.

---

בואו נראה כיצד גרדיאנט דיסנט יעבוד בפייתון. 
בקוד שלפניכם אנחנו מדמים מצב אידיאלי לרגרסיה לוגיסטית עם שני משתנים, וללא חותך. N יהיה שווה 1000 תצפיות ו-q יהיה שווה ל2, כלומר שני משתנים, לכן ב-X יהיו 1000 שורות על שתי עמודות. וקטור המקדמים האמיתי בטא באורך 2 יהיה פשוט 1, 2.

וקטור ההסתברויות האמיתיות p מחושב לפי פונקצית ה-G ההופכית שלנו.

לבסוף Y נדגם מהתפלגות ברנולי להיות 0 או 1, עם ההסתברויות שחישבנו.

---

אבל מה הקשר בין רגרסיה לוגיסטית לרשתות נוירונים? כעת נראה שרגרסיה לוגיסטית היא רשת נוירונים פשוטה למדי.

---

נקרא לפונקצית הנראות השלילית cross entropy.

נקרא לפונקציה ההופכית ל-g שמחשבת לנו את ההסתברות החזויה, פונקצית זיגמויד.

לחישוב ההסתברות החזויה ולוג הנראות נקרא פעפוע לפנים, או forward propagation.

לחישוב הגרדיאנט נקרא פעפוע לאחור, או backward propagation.

נסמן את וקטור המקדמים שלנו כמטריצה W שיש לה עמודה אחת, לפעמים נוסיף גם חותך שנקרא לו bias.

במקום להשתמש בגרדיאנט דיסנט על כל הדאטה נשתמש בגרדיאנט דיסנט סטוכסטי או SGD - תיכף נסביר מה זה.

לבסוף נתאר את המודל שלנו בתור דיאגרמה עם קשתות וצמתים, נקרא להם נוירונים.

ויש לנו רשת נוירונים!

טוב, בהמשך נוסיף עוד כמה אלמנטים.

---

ראשית מהי האנטרופיה? אפשר לראות באנטרופיה כמדד למרחק בין שתי התפלגויות. 

עבור שתי התפלגויות P ו-Q, אנטרופיה המסומנת בדרך כלל ב-H, היא מינוס התוחלת של לוג ההתפלגות Q לפי התפלגות P.

במקרה שלפנינו למשתנה אקראי X יש שני ערכים אפשריים, והאנטרופיה מקבלת צורה של סכום פשוט של הסתברות לפי התפלגות אחת כפול לוג ההסתברות לפי ההתפלגות האחרת.

אם נחשוב על שתי ההתפלגויות שלנו כפונקצית ההסתברות של Y הבינארי שלנו, ו-Y עצמו, נוכל להוכיח שאנטרופיה היא בדיוק פונקצית לוג-הנראות השלילית שלנו.

---

שינוי נוסף שנעשה הוא פשוט לשנות את שם הפונקציה ההופכית של G לפונקצית זיגמויד. פונקצית זיגמויד המסומנת כאן עם האות סיגמא, מקבלת כל קלט בין מינוס לפלוס אינסוף, וממפה אותו בצורה מונוטונית להיות בין 0 ל-1.

---

מהם הפורוורד והבקוורד פרופוגיישן?

אפשר לומר שכל איטרציה בגרדיאנט דיסנט כללה:

שלב פורוורד - חישוב פונקצית הלוג-נראות והחיזוי p_hat

שלב בקוורד - חישוב הגרדיאנט

גרדיאנט דיסנט - הליכה של צעד בגודל אלפא בכיוון הגרדיאנט

בפייתון מימשנו כל שלב באמצעות שורה אחת או שתיים. פורוורד. בקוורד. גרדיאנט דיסנט.

אבל למה אנחנו קוראים לזה פורוורד או לפנים? למה בקוורד או לאחור?

---

במקרה שלנו הגזירה של פונקצית הלוג-נראות השלילית לפי וקטור המקדמים בטא היתה יחסית פשוטה. הגענו לביטוי פשוט גם אם לא לפתרון סגור.

ככל שהארכיטקטורה של רשת הנוירונים תלך ותיעשה מורכבת יותר, נצטרך להכליל את הגזירה הזאת, לצעדים קטנים.

לפי כלל השרשרת, אם Y הוא פונקציה F של פונקציה G של פונקציה H של X - 
אפשר לרשום את הנגזרת של Y לפי X לפי כלל השרשרת. הנגזרת של Y לפי X היא הנגזרת של F לפי G, כפול הנגזרת של G לפי H, כפול הנגזרת של H לפי X.

אפשר לרשום זאת בכתיב דיפרנציאלי, ובכל מקרה נבחין שיש לפנינו מכפלה לאחור של נגזרות.

---

כדי שנוכל להשתמש בכלל השרשרת, נרשום את פונקצית הלוג-נראות השלילית שלנו כפונקציה מורכבת של בטא:

ההכפלה של האינפוטים X בבטא תהיה פונקציה Z של בטא.

המעבר של Z בפונקצית זיגמויד או הפונקציה ההופכית של G תיקרא פונקציה P של Z.

החישוב הסופי של האלמנט ה-I בתוך הלוג-נראות השלילית מתוך ההסתברויות החזויות P ייקרא פונקציה L של P.

כלומר אלמנט אחד של פונקצית ההפסד שלנו הוא L של P של Z של בטא. ולכן - קדימה, או פורוורד.

וכעת אפשר להפעיל את כלל השרשרת: הנגזרת של פונקצית ההפסד לפי בטא J כלשהו, היא נגזרת של L לפי P, כפול נגזרת של P לפי Z, כפול נגזרת לפי בטא J.
ולכן - אחורה, או בקוורד.

---

כל אחת מהנגזרות האלה קלה לחישוב: הנגזרת של L לפי P, הנגזרת של P לפי Z והנגזרת של Z לפי בטא J.

ואם נכפול את הנגזרות האלה זו בזו לפי כלל השרשרת, נקבל בדיוק את הביטוי האנליטי הפשוט לגרדיאנט שקיבלנו לפני כן.

---

כעת נממש את הרגרסיה הלוגיסטית לפי הצעדים שחישבנו באמצעות כמה פונקציות פשוטות:

פונקצית הפורוורד מקבלת את X, Y ואת האומדן הנוכחי לוקטור המקדמים בטא. היא מחשבת שלב אחרי שלב את ההסתברויות החזויות ואת לוג הנראות השלילית:
הכפלה של X בבטא האט לתת את Z, זיגמויד על Z לתת את P האט, ולבסוף חישוב הנראות הסופית.

פונקצית הבקוורד תקבל את X ואת ההסתברויות החזויות P האט, ותחשב בשני צעדים לאחור את הנגזרות ותכפיל ביניהן לפי כלל השרשרת להחזיר את הגרדיאנט לפי בטא.

פונקצית הגרדיאנט דיסנט פשוט תקבל את גודל הצעד אלפא, את האומדן הנוכחי לבטא ואת הגרדיאנט, ותחשב אומדן מעודכן לבטא על ידי הליכה בצעד אלפא בכיוון הגרדיאנט.

הפונקציה אופטימייז מהווה איטרציה אחת שכוללת צעד פורוורד, צעד בקוורד וצעד גרדיאנט דיסנט.

---

לבסוף הפונקציה lr_nn מקבלת את הדאטא X ו-Y ומספר איטרציות לריצה שנקראות איפוקס.

היא מאתחלת את האומדן בטא-האט לאיזשהם ערכים, כאו מינוס 2.5 לשני הרכיבים של הוקטור.

ואז היא מבצעת את מספר האיפוקס המתבקש, כל אחד מאלה הוא בעצם קריאה לפונקציה אופטימייז, שעושה צעד פורווד, צעד בקוורד וגרדיאנט דיסנט.

---

רגע לפני שנריץ את הפונקציה, אני בכל זאת רוצה שנשנה דבר קטן אחד. נזכיר שרשתות נוירונים יודעות לרוץ על קבצי נתונים גדולים מאוד.

אחד מהאופנים שבהן הן עושות את זה, הוא שהגרדיאנט לא מחושב בכל איטרציה על כל מסד הנתונים. במקרה שלפנינו לא היתה בעיה לעשות את זה גם אם N היה גדול מאוד. אבל ברגע שהרשת תיעשה מורכבת יותר, חישוב הגרדיאנט בכל איטרציה יגזול מאיתנו זמן וכוח חישוב, אם בכלל יהיה אפשרי.

לכן מקובל ברשתות נוירונים לאמן על באצ'ים אקראיים של דאטא. בכל איפוק נחלק את הדאטא למספר תת-מדגמים אקראיים בגודל batch_size, ונזין אותם אחד אחרי השני לרשת, נחשב את הגרדיאנט ונלך צעד בכיוונו, עד שהרשת תראה בסוף האיפוק את כל הדאטה. באיפוק הבא היא תראה באצ'ים אחרים.

האקראיות הזאת שהכנסנו נקראת stochatstic gradient descent, והיא כאמור עקרון מפתח בהפעלת הרשת על נתונים גדולים באמת.

שימו לב שלא כל פונקצית הפסד ניתן בהכרח לפרק לחישוב על מספר באצ'ים. במקרה שלנו ראינו שהלוג-נראות השלילית היא סכום של אלמנטים על כל התצפיות, ולכן היא מתפרקת בפשטות יחסית למספר סכומים, וניתן להראות שזה מספיק כדי לתקף את השינוי הזה.

בואו נראה את המימוש שלנו בפעולה!

---

מה בין מודל הרגרסיה הלוגיסטית לרשת נוירונים? איפה ה"רשת"? נסמן את האינפוט של הרשת, Q פיצ'רים של X ועוד חותך באמצעות צמתים של רשת.

נמתח קשת בין כל אינפוט אל תוך צומת אחד שנקרא לו נוירון. כל אחת מהקשתות מסמלת פרמטר בוקטור המקדמים שלנו בטא, והנוירון כופל את האינפוטים במקדמים וסוכם אותם. למעשה הנוירון מבצע מכפלה פנימית בין שני הוקטורים.

לאחר מכן הכמות הזאת עוברת אקטיבציה, כאן אקטיבצית זיגמויד, כדי לתת חיזוי סופי לתצפית p_hat ואת פונקצית ההפסד בשלב זה.

וכל זה נקרא צעד פורוורד.

לאחר מכן נלך אחור ברשת ונחשב בכל שלב את הנגזרת כדי לתת הגרדיאנט לכל אחד מהמקדמים בוקטור המשקלות בטא. וזה ייקרא צעד בקוורד. לבסוף נלך צעד אלפא בכיוון הגרדיאנט וחוזר חלילה.

אנחנו רואים שניתן לייצג רגרסיה לוגיסטית כנוירון בודד, ברשת שהיא אינה אלא ייצוג למודל המוכר לנו. את הייצוג הזה יהיה קל להכליל למודלים מורכבים יותר ויותר.

---

מובן שלא נממש בעצמנו רשת נוירונים כל פעם שנצטרך. בקורס זה נשתמש בספריית קראס, שהיא מצוינת למתחילים.

כאן אני מייבא קלאס בשם sequential, קלאס לשכבה dense פשוטה כפי שאנחנו צריכים, וקלאס לגרדיאנט דיסנט סטוכסטי, SGD.

אני מאתחל מודל עם sequential. מוסיף שכבת dense עם נוירון בודד באמצעות הפעולה model.add(), ואקטיבציה sigmoid. נשים לב שאני מבקש מהשכבה לא להשתמש בביאס, הוא החותך.

אני מאתחל קלאס SGD עם איזשהו קצב למידה, זהו האלפא שלנו. מקמפל את המודל באמצעות קריאה לmodel.compile(), שם אני מפרט גם את פונקצית ההפסד שהיא האנטרופיה ואת האופטימייזר שלנו, הוא הSGD.

לבסוף אני מריץ את המודל על הנתונים באמצעות הקריאה לmodel.fit(), לתוכו אני מזין את X ואז Y, ומפרט גם מה גודל הבאץ' ומספר האיפוקס. 

בואו נראה את זה בפעולה.

---

אז האם זה כל מה שיש בה, ברשת הנוירונים? הכללה של רגרסיה לינארית או לוגיסטית לקשר לא-ליניארי בין X ל-Y?

התשובה הקצרה היא: לא. את התשובה הארוכה יותר אני מביא כאן כציטוט מספר מצוין של שני סטטיסטיקאים גדולים, בראדלי אפרון וטרוור הייסטי:

רשתות נוירונים הזריקו אנרגיה חדשה לתחום. אפשר להכליל אותן במגוון דרכים כפי שנראה ולאמן אותם על מסדי נתונים עצומים.

---

לאמן רשת על שתי מחלקות או קלאסים בלבד זה מעט משעמם. בואו נכליל את הרשת ל-C קלאסים.

---

נתאים וקטור מקדמים בטא לכל אחד מהקלאסים שלנו. נקרא לוקטור כזה כבר W ונדבר על מטריצה של משקלות, עם Q פלוס 1 שורות, ו-C עמודות ל-C קלאסים.

נוסיף בשכבת האאוטפוט C נוירוינים ל-C קלאסים.

כאשר המכפלה הפנימית בכל נוירון כזה תעבור אקטיבציית סופטמקס. סופטמקס היא נוסחה שמטרתה לייצר מ-C נוירונים C הסתברויות שיסתכמו ב-1. הפונקציה לוקחת את האאוטפוט של כל נוירון, מעלה אותו באקספוננט ומחלקת בסך האקספוננטים מכל ה-C נוירונים.

מודל זה, אגב, ניתן לראות כמודל שקול לרגרסיה מולטינומית שהזכרנו, מודל שמכליל רגרסיה לוגיסטית ל-C קלאסים.

---

כעת נוספים לדיאגרמה שלנו שני נוירונים עבור שני קלאסים שמייצרים שתי הסתברויות שמסתכמות ב1, אחרי אקטיבצית סופטמקס.

---

בקראס נעשה מספר שינויים קטן:
ה-Y שלנו בשביל C קלאסים כבר לא יכול להיות וקטור עם 0 או 1. הוא צריך להיות מטריצה, עם N שורות ו-C עמודות, בכל עמודה יש 0 בכל השורה חוץ מהעמודה המתאימה לקלאס שם יש 1. את כל זה ניתן לבצע עם הפונקציה to_categorical.

בשכבת האאוטפוט שלנו יהיו 2 נוירונים, ולא 1, והאקטיבציה תהיה סופטמקס ולא זיגמויד.

לבסוף בזמן קומפילציה של המודל נפרט פונקצית הפסד בשם categorical_crossentropy ולא binary_crossentropy.

בואו נראה את זה בפעולה.

---

ראינו שקל להכליל את הרשת להוציא C אאוטפוטים ל-C קלאסים. כעת נראה כמה קל להוסיף עוד שכבות ביניים. אנחנו קוראים לשכבות כאלה נסתרות, או hidden.

---

הדיאגרמה שלנו כבר נראית כמו רשת נוירונים מורכבת:

Q אינפוטים נכנסים לרשת בתוספת לביאס, ומגיעים לשכבת הביניים שבה יש H נוירונים. הנוירונים מבצעים מכפלה פנימית בין האינפוטים לבין כל אחת מהעמודות של מטריצת משקלות W, ומתבצעת איזושהי אקטיבציה לא-ליניארית G. נשים לב שכל קשת ברשת היא פרמטר, היא אלמנט במטריצת משקולות.

H האינפוטים משכבת הביניים בתוספת לביאס, מגיעים אל שכבת האאוטפוט הסופית. שכבה זה מבצעת את המכפלה הפנימית בין האינפוטים לכל אחת מעמודות מטריצת משקלות W השניה, ומתבצעת אקטיבצית סופטמקס להוצאה של הסתברויות בין 0 ל-1.

כל זה צעד פורוורד של הרשת, וכעת מחשבים את הגרדיאנט באמצעות פעפוע לאחור על כל אחד מהפרמטרים במטריצות המשקלות W1 ו-W2, ומתבצע גרדיאנט דיסנט, בדיוק איך שמימשנו עבור הרשת הפשוטה מקודם שם היה לנו נוירון אחד בלבד.

---

נשים לב שכבר התרחקנו מרגרסיה לוגיסטית מרחק די רב. הרשת שלפנינו, עם מספיק נוירונים בשכבת הביניים, יכולה כבר למדל יחסים לא-ליניאריים מורכבים ביותר בין X ל-Y.

מבחינת קוד בקראס, כל מה שנוסף כאן הוא עוד קריאה לmodel.add שם אנחנו מוסיפים עוד שכבת dense פשוטה עם 4 נוירונים ואיזושהי אקטיבצית זיגמויד.

המודל הזה נקרא multi layer perceptron או MLP, והוא קיים שנים רבות. המימוש הראשון שלו נצפה כבר ב1958 על-ידי מדען בשם פרנק רוזנבלט. אולם לקח שנים והיווצרות נתונים מספיק גדולים ומחשוב יעיל מספיק כדי להביא מודל זה לשימוש הנרחב שאנו רואים כיום.

ואפילו כעת, נשים לב שיש כאן נוסחה לא מאוד מורכבת. ניקח את מטריצת הנתונים X, נוסיף לה עמודה של אחדים, נכפיל במטריצה W1, נעשה על זה איזושהי אקטיבציה למשל זיגמויד, נוסיף שוב עמודת אחדים לחותך, נכפול במטריצת W2 ונבצע שוב אקטיבצית סופטמקס לקבלת הסתברויות.

---

המודל שלנו נהפך למורכב יותר ויותר, ואנחנו צריכים לראות כמה פרמטרים הוא כבר כולל.

מומלץ לקרוא לmodel.summary ולנסות לחשב לבד איך הגענו למספר הזה של פרמטרים.

(חישוב ידני על השקף)

---

בשלב הזה כדאי לומר שכיום אנחנו פחות משתמשים בפונקצית האקטיבציה זיגמויד לשכבות הביניים. שתי פונקציות אקטיבציה לא-ליניאריות נפוצות יותר נמצאות לפנינו: tanh ו-relu.

tanh נמצאת בשימוש נרחב ברשתות לעיבוד שפה, היא לוקחת קלט ממשי כלשהו ודוחסת אותו להיות בין מינוס לפלוס 1.

relu היא למעשה פונקצית ציר, או hinge. היא מאפסת אינפוט שלילי, ומשאירה אינפוט חיובי כפי שהוא.

שתי הפונקציות לא-לינאריות אבל אין להן פרמטרים והגזירה שלהן קלה.

---

האם אנחנו יכולים להשתמש בפונקצית אקטיבציה ליניארית? אם נשתמש בפונקצית אקטיבציה ליניארית, אנחנו נשארים במודל ליניארי! על כך תראו עוד בשיעורי הבית.

בואו נראה כיצד הרשת שלנו עובדת בפייתון.

---

כפי שאולי הבחנתם, זה קל מאוד להוסיף עוד ועוד שכבות ברשת נוירונים ולכן עוד ועוד פרמטרים. רשתות נוירונים ידועות לשמצה ביכולת שלהן לבצע אוברפיטינג לנתונים, ולכן חייבים לבצע איזושהי רגולריזציה על מרחב הפרמטרים.

רגולריזציה יכולה להתבצע למשל באמצעות הוספת איזושהי פונקצית עונש, או פנלטי, על הפרמטרים של המודל, שתתווסף לפונקצית ההפסד. צורה כזו אולי מוכרת לכם ממודלים לרגרסיה ליניארית כמו רידג' או לאסו.

הפנאלטי יכול להיות איזשהו עונש על נורמת L2 של וקטור או מטריצת המקדמים. כאן העונש מתבצע באמצעות פרמטר למדא. ככל שלמדא יהיה גדול יותר העונש על פרמטרים גדולים מדי יהיה גדול יותר והמודל לא ירשה לעצמו להתאים פרמטרים גדולים מדי, הוא יהיה צנוע יותר.

עונשים אחרים יכולים להיות על נורמת L1 או על שילוב של שני אלה.

---

בקראס ניתן להוסיף רגולריזציה על המשקולות בכל שכבה ושכבה. כאן אנחנו מדגימים את כל אחד מהסוגים שדיברנו עליהם.

---

רגולריזציה יכולה להיות גם לא קשורה הישר לפונקצית ההפסד, היא יכולה להיות גם אלגוריתמית. לדוגמא: דרופאאוט.

כאשר אנחנו מאמנים רשת עם מנגנון דרופאאוט, אנחנו למעשה מכבים נוירונים בשכבה באופן אקראי בכל אפוק או איטרציה. בסיכוי מסוים P אנחנו פשוט מאפסים את האאוטפוט שלהם, ומכאן השם drop out.

בזמן חיזוי על דאטאסט חדש, כל הנוירונים פעילים, והאאוטפוט שלהם מוכפל בP

---

למה דרופאאוט עובד?

אפשר לחשוב על דרופאאוט כאימון על אנסמבל של רשתות בדומה למודל רנדום פורסטס שאנחנו מכירים. בכל איטרציה המודל רואה רשת בארכיטקטורה אחרת והצורה הסופית היא מעין ממוצע של הרבה רשתות כאלה.

אפשר גם להסביר את היעילות של דרופאאוט באמצעות מונחים מתחום האנליזה הנומרית. בכל איטרציה אנחנו שוברים כיוון לעבר פתרון שכן בצורה אקראית ובכך גדל הסיכוי שנכסה את מרחב הפרמטרים ונגיע לנקודת אופטימום גלובלית או טובה יותר.

בקראס אנחנו מוסיפים דרופאאוט כך.

---

רגולריזציה אלגוריתמית נוספת שנמצאת בשימוש נרחב היא early stopping או עצירה מוקדמת. במהלך האימון שמרו בצד עוד סט אקראי קטן של נתונים עליו המודל לא יתאמן, אלא רק יבדוק אחרי כל איטרציה האם יש אכן ירידה בפונקצית ההפסד, הלוס, על דאטה שהמודל לא ראה.

אם אין ירידה במשך כך וכך צעדים, יפסיק המודל להתאמן, כי אנחנו חושדים שבשלב זה הוא רק עושה אוברפיטינג למדגם הלמידה.

בקראס early stopping הוא קריאה או קולבאק. ניתן לאמן את המודל באמצעות קריאה לכמה קולבקס כאלה וגם לכתוב קולבקס בעצמנו. כאן אנחנו מגדירים שearly stopping ינטר את הולידיישן לוס, ואם תוך 5 צעדים אין שיפור בפונקצית ההפסד על תת מדגם זה, יעצור.
בקריאה לפיט, אנחנו מבקשים שיחלק בתחילת הריצה את מדגם הלמידה ל-20 אחוז ולידיישן עליו המודל לא יתאמן ו-80 אחוז טריינינג, באמצעות הפרמטר validation_split. וכדי לבצע עצירה מוקדמת אנחנו מזינים את הקולבק בתוך רשימה לארגומנט callbacks.

---

כעת נרחיב קצת יותר על היכולות של קראס, עם נתונים אמיתיים.

קראס הוא בעצמו API, שנכתב על ידי מהנדס מגוגל בשם פרנסואה שולה. קראס עוטפת תוכנה פופולרית ללמידה עמוקה בשם טנסורפלואו, גם כן נכתבה על ידי מהנדסי גוגל.

קראס מאפשרת כפי שראינו להגדיר רשתות מורכבות מבלי להיכנס לברזלים של תהליך האופטימיזציה, ואם רוצים יותר גמישות בארכיטקטורה תמיד אפשר לעבור לכתוב בטנזורפלואו.

קראס מתכתבת באופן ישיר עם החבילות המוכרות שלנו של נאמפיי ופנדאס. כדאי להכיר גם את המתחרה הגדול של טנזורפלואו, הוא פייטורץ' מבית מטא, שגם לו יש API שמקילים על משתמשים מתחילים לכתוב רשתות מורכבות.

---

נסתכל על מסד נתונים מעניין בשם מלריה. במסד נתונים זה יש כ27 אלף תמונות של תאים, שיכולים להיות נגועים במלריה או לא. נרצה לחזות האם תא נגוע במלריה או לא, כלומר בעית קלסיפיקציה של שני קלאסים. המטרה במודל חיזוי לבעיה כזו היא להטמיע אותו בפלאפונים של רופאים שמטפלים בפציינטים באזורים נחשלים לאבחון מהיר של מלריה. נציין עוד כי הדאטה הוא balanced, כלומר חצי מהתאים נגועים במלריה וחצי לא.

מסד הנתונים הזה נלקח ממאגר עצום של נתונים שמתאימים במיוחד ללמידה עמוקה שנקרא tensorflow datasets. מומלץ ללחוץ על הקישור כאן ולעלעל במגוון הנתונים העשיר שניתן להוריד בשורה או שתיים של קוד אל כל מחברת ולהריץ עליהם מודלים.

כאן נשתמש רק בעשרה אחוזים של הנתונים, כלומר 2500 תמונות, ונאחד אותן לגודל של 100 על 100 פיקסלים. מאוחר יותר נראה ביצועים מרשימים אף יותר באמצעות רשתות מתקדמות יותר, על כל סט התמונות.

---

בקטע קוד זה אנחנו מייבאים את הספרייה tansorflow datasets, ומורידים את סט הנתונים כאובייקט מיוחד של טנזורפלואו איתו קל לעבוד, ומראים דוגמאות של תאים מתוך המדגם. ניתן לראות שחלק מהתאים מסווגים כparasitized כלומר נגועים במלריה, וחלק כ-uninfected כלומר לא נגועים.

---

בקטע קוד זה אנחנו הופכים כל תמונה ותמונה להיות מערך של נאמפיי, ועוצרים ב-2500 תמונות. לאחר מכאן אנחנו מחלקים את X ו-Y, התמונות והסיווגים שלהן ל80 אחוז מדגם למידה ו20 אחוז מדגם טסט.

עוד נלמד על תמונות וייצוגן. כרגע מדובר במערך תלת מימדי בגודל 100 על 100 פיקסלים, על 3 שכבות צבע: אדום, ירוק וכחול, או RGB. מודל הMLP שלנו לא יודע להתמודד עם נתונים שאינם טבלאיים ולכן יש צורך לשטח אותם, ונקבל בסופו של דבר מטריצת X_train עם 2000 שורות ו-30 אלף עמודות שמייצגות שרשור של 30 אלף פיקסלים: 100 כפול 100 כפול 3.

במדגם הטסט יש לנו 500 תמונות עם צורה דומה.

---

כאן אנחנו מבצעים רגרסיה לוגיסטית באמצעות sklearn. המימוש בsklearn יעיל כך שאין לו בעיה להתמודד אפילו עם 30 אלף משתנים שיש כאן. בחיזוי על מדגם הטסט לעומת זאת, כשאנו מבקשים את הscore אנחנו מקבלים את אחוז הדיוק הכללי על הטסט, הaccuracy, והוא די מאכזב. אמנם אין ספק שמודל הרגרסיה הלוגיסטית שהוא מודל ליניארי למד משהו, והדיוק גבוה בהרבה מדיוק אקראי של 50 אחוז, אך דיוק של בין 60 ל-70 אחוזים איננו מרשים.

---

בקראס נאמן רשת עמוקה עם 3 שכבות ביניים: שכבה עם 300 נוירונים, שכבה עם 100 נוירונים ושכבה עם 50 נוירונים. לכל אחת מהשכבות אקטיבצית רלו, ולבסוף נוספת שכבת האאוטפוט עם נוירון בודד עם אקטיבצית זיגמויד שתפקידו להחזיר לנו בסוף כמות בין 0 ל-1, בה אפשר לראות כמעין הסתברות שהתא נגוע במלריה.

ניתן כפי שנראה כאן לכתוב את הרשת כרשימה משורשרת של שכבות, ובהמשך נראה דרכים נוספות להגדיר רשתות מורכבות יותר.

---

בואו נראה שוב שאנחנו מבינים כמה פרמטרים יש במודל שלנו, ומאיפה הגיעו המספרים תחת model.summary.

(חישוב ידני)

האם אתם מודאגים? 9 מיליון פרמטרים ברשת שלנו, ועוד לא דיברנו על בעיות אחרות מעצם הגדרתה, והאם מתאים להתייחס לתמונות כמו כאן, כאוסף של פיקסלים בלי תלות ברורה ביניהם.

---

כבר כעת הרשת מאותחלת בפרמטרים אקראיים, וניתן לגשת אל השכבות עצמן ואל המשקלות שלהן.

באמצעות model.layers ניתן לגשת לרשימה של שכבות, לכל אחת מהן שם, לדוגמא לשכבה הראשונה קוראים my_dense_layer כי ככה הגדרנו.

אם נבקש את השכבה הזאת באמצעות הפונקציה get_layer, ועל השכבה נבקש get_weights, נקבל את מטריצת המשקולות של השכבה ואת וקטור הביאס שלה. כאן אנחנו מדפיסים את מטריצת המשקולות W ואת הגודל שלה לצורך הדגמה ולודא שהבנו.


---

הקומפילציה של המודל נעשית כרגיל. כאן אנחנו משתמשים בפונקצית הפסד שמתאימה לבעית קלסיפיקציה בינארית כפי שראינו binary_crossentropy.

בנוסף, הפעם אנחנו משתמשים באופטימייזר מעט שונה אך פופולרי שנקרא אדם.

באמצעות הארגומנט metrics נבקש גם מהרשת לדווח לנו בכל איטרציה רשימה של מטריקות נוספות לבחירתנו. כאן אנחנו מבקשים שבצד הלוס הרגיל של הרשת יודפס גם הדיוק, הaccuracy.

לפני שנריץ מומלץ ללחוץ על הלינקים השונים שמופיעים כאן כדי להתרשם ממגוון האתחולים, פונקציות ההפסד, האופטימייזרים והמטריקות שלקראס יש להציע. לכל אחד מאלה ניתן גם לרשום פונקציה שמתאימה לצרכים הספציפיים של הרשת שלנו, וזאת מבלי לכתוב בטנזורפלואו שהוא מעט מורכב יותר לשימוש.

---

כעת נריץ את המודל עם early stopping שמנטר 10 אחוז מדגם ולידיישן. אם לא יראה שיפור בפונקצית ההפסד על מדגם זה במשך 5 צעדים הלמידה תיעצר. כאן אנחנו גם מבקשים מקראס לשמור את סט המשקולות שמתאים לאיטרציה עם ההפסד הנמוך ביותר שראינו עד כה.

דבר נוסף שאנו עושים זה להחזיר את הקריאה לmodel.fit לתוך אוביקט שנקרא history.

---

לאחר הריצה מומלץ לעטוף את האוביקט history כדאטה פריים של פנדאס ולהדפיס את הלוס לאורך האיטרציות ואולי עוד מטריקות כמו accuracy, על מדגם הטריין ועל מדגם הולידיישן. לראות שהאימון נראה טוב ואין אילו אנומליות שלא צפינו.

כאן ניתן לראות שההפסד, הלוס יורד על מדגם הלמידה ומדגם הולידיישן, וכאשר אין שיפור על מדגם הולידיישן במשך 5 צעדים מופסקת הלמידה. ניתן לראות גם את הaccuracy על שני המדגמים. היא עולה עד גבול מסוים, ובכל מקרה איננה מרשימה.

---

כדי לראות כיצד המודל ביצע על מדגם הטסט שלנו, ניתן להריץ אותו על X_test ו-y_test באמצעות קריאה לevaluate. כפי שניתן לראות אחוז הדיוק של הרשת על תמונות המלריה הוא לא הרבה יותר טוב מאחוז הדיוק של רגרסיה לוגיסטית, ובריצות מסוימות יכול להיות גם נמוך יותר.

ניתן גם לקרוא לmodel.predict, כדי לקבל את הציונים בין 0 ל-1 החזויים בעצמם. את הציונים האלה ניתן להשוות לאיזשהו סף קטאוף כדי לדעת בדיוק אילו תצפיות של תאים נחזות כ1 כלומר יש להן מלריה, ואילו 0. על הוקטור הזה ניתן להפעיל את הפונקציה confusion_matrix המוכרת לנו מsklearn כדי לראות בדיוק איך הגענו לאחוז הדיוק הנוכחי.

שוב נעיר שאחוז הדיוק איננו מרשים וזה בסדר להכיר בכך. בשיעור הבא נלמד על ארכיטקטורה של למידה עמוקה שמתאימה הרבה יותר לנתונים של תמונות, ושם נראה אחוז דיוק מרשים הרבה יותר.

---

כבר בשלב זה ודאי הבחנתם כמה החלטות אנחנו צריכים לעשות, כמה היפר-פרמטרים עלינו לכוון. כמה שכבות, כמה נוירונים בכל שכבה, מהו קצב הלמידה, באילו פונקציות אקטיבציה להשתמש ובאילו שיטות רגולריזציה.

מומלץ להשתמש בשיטות קיימות לבצע כוונון או טיונינג של פרמטרים. כאן אנחנו משתמשים בספריה שנקראת scikeras, ובה יש קלאס שנקרא KerasClassifier. הקלאס הזה יכול לעטוף פונקציה לבניית מודל כמו שלנו, והפונקציה תקבל כארגומנטים מספר היפר-פרמטרים שאנחנו רוצים לכוונן. כאן למשל אנחנו מזינים לפונקציה את הפרמטרים מספר שכבות, מספר נוירונים בהנחה שיהיה זהה בכל שכבה, וקצב למידה, learning_rate.

---

כעת ניתן לעטוף את אוביקט הקראס קלסיפייר שלנו בקלאס אחר מsklearn לביצוע חיפוש רנדומלי במרחב הפרמטרים שהזנו, RandomizedSearchCV.

קלאס זה יקבל מילון עם האפשרויות לכל פרמטר, כאן הוא נקרא params.

לאחר מכן נגדיר איך אנחנו רוצים שירוצו כל המודלים האלה. כאן אנחנו מגדירים שאנחנו רוצים 10 איטרציות שבכל אחת נגריל סט של פרמטרים מהמרחב שהגדרנו. נריץ כל מודל על פני חמישה פולדים של דאטה בפרוצדורה שלcross validation.

לבסוף נדפיס את סט הפרמטרים שהביאו לתוצאה הטובה ביותר בממוצע על פני חמשת הפולדים, וזה יהיה הסט בוא נשתמש בריצה על כל מדגם הנתונים לפני חיזוי סופי על הטסט סט.

אנחנו רואים שגם סט הנתונים הטוב ביותר לא הביא לחיזוי על מדגם טסט טוב בהרבה ממה שראינו, כאן הגענו ל69 אחוזים accuracy. לעוד פתרונות מומלץ להסתכל על ספרית KerasTuner שנותנת עוד אפשרויות לכוונון היפר-פרמרטרים.

---

לפני שנסיים ודאי הבחנתם שמודלים של רשתות נוירונים יכולים לקחת זמן לא מועט לאימון. המודלים הסופיים עצמם יכולים להיות גם די כבדים, כאן ראינו מודל די פשוט אך הוא עדיין מסתכם ב9 מיליון פרמטרים שיש לשמור בזיכרון אם רוצים להריץ את המודל בסביבת פרודקשן, לדוגמא באתר באינטרנט על תמונות נכנסות.

מודלים פשוטים של קראס באים עם מתודה נוחה לשמירת המודל על הדיסק ולהעלאתו: save ו-load_model. כאן אנחנו שומרים מודל לדיסק בתור קובץ מסוג HDF5, וכשאנחנו מעלים אותו שוב עם פונקצית load_model הוא מוכן לחיזוי ואפילו המשך אימון.


---

נסיים בהמלצות לספרים מצוינים בנושא, שניהם משתמשים בקראס לאימון רשתות, ואחד מהם נכתב אפילו בידי המחבר של קראס, פרנסואה שולה.
