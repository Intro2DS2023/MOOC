<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.433">

  <title>Predictive Modeling Theory</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  
    <link rel="icon" href="../Intro2DS_logo.jpg" type="image/jpg"> 
    <link rel="shortcut icon" href="../Intro2DS_logo.jpg" type="image/jpg">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">
  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="section" class="slide level2 logo-slide">
    <h2></h2>
    </section>
<section id="introduction-to-data-science" class="slide level2 title-slide center">
<h2>Introduction to Data Science</h2>
<h3 id="predictive-modeling-theory---class-14">Predictive Modeling Theory - Class 14</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2ds-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2ds</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predictive-modeling-so-far" class="slide level2 title-slide center">
<h2>Predictive Modeling So Far</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ביחידה הזאת לא נלמד אף מודל חיזוי חדש, אלא נעסוק בתיאוריה של מודלים לחיזוי מעט יותר לעומק. הרציונל מאחורי הבחירה ללמוד תיאוריה לאחר מעשה, הוא שכעת יש לנו הרבה דוגמאות להמחשת התיאוריה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-we-have-learned">What we have learned</h3>
<ul>
<li><p>Want to model and predict <span class="math inline">\(y\)</span> as function of <span class="math inline">\(x\)</span>: regression or classification</p></li>
<li><p>Using training data <span class="math inline">\(Tr\)</span> to learn model, test data <span class="math inline">\(Te\)</span> to evaluate model and emulate actual prediction</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Traditional parametric models: OLS regression, logistic regression</p></li>
<li><p>Non-parametric models based on neighborhoods: Nearest neighbors, trees</p></li>
<li><p>Modern methods based on trees: Random Forest and Boosting</p></li>
<li><p>Neural nets and DL (specifically CNN)</p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז מה עשינו בחצי השני של הקורס?</p>
<p>דיברנו על הסטאפ הבסיסי שבו X הוא וקטור משתנים וY סקלאר שאנחנו מנסים למדל ולחזות, אם Y ממשי קראנו לזה רגרסיה ואם קטגוריאלי קלסיפיקציה.</p>
<p>הפרדיגמה הכללית שלנו היא להשתמש בדאטא נפרד ללמידה שמסומן בTR ודאטא אחר לחיזוי שמסומן בTE, ואמרנו שהרבה פעמים זה אומר בפועל לקחת את הדאטא שיש לנו ולחלק אותו בעצמנו לטריין וטסט. ראינו אינטואיטיבית למה שנעשה דבר כזה, מדגם הטריין לרוב יכול להגיע לאפס אחוז טעות, מה שנקרא אוברפיטינג, ואיכות המודל נשפטת על-פי מדגם הטסט. כאן טמונה בעיה שאולי נחשפתם אליה בתרגול: אם יש לי טסט סט אחד, ועל-פיו החלטתי לקחת לדוגמא עץ בעומק 5 במקום עץ בעומק 4 – אם אני רוצה לשקף מה הביצועים הסופיים של המודל שלי, האם אני יכול לדווח על הביצועים של המודל עם הטסט סט שבו השתמשתי לבחירה בין מודלים? אני מקווה שזה נשמע לכם מעט בעייתי, נדבר על זה היום.</p>
<p>מבחינת שיטות המודלים הפרמטריים שלמדנו היו רגרסיה ליניארית ורגרסיה לוגיסטית. שיטות שאין להן פרמטרים שהן צריכות לאמוד ולכן ניתן לקרוא להן אפרמטריות, שיטות שמבוססות על הגדרת שכונות – היו KNN ועצים.</p>
<p>למדנו גם על שיטות אנסמבל מבוססות עצים - רנדום פורסט ובוסטינג. ולמדנו על רשתות נוירונים מהבסיס, והנציג של למידה עמוקה או דיפ לרנינג שהצגנו היה רשתות קונבולוציה.</p>
<p>ננסה לחשוב על כל השיטות שלמדנו בצורה אחודה, באמצעות חשיבה רוחבית על כמה תכונות של מודל לחיזוי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="model-complexity-or-flexibility">Model complexity or flexibility</h3>
<ul>
<li>Many of the models we have learned have a <em>complexity</em> parameter
<ul>
<li>OLS/logistic: <span class="math inline">\(p\)</span>, number of variables in <span class="math inline">\(x\)</span></li>
<li>k-NN: number of neighbors <span class="math inline">\(k\)</span></li>
<li>Trees: depth of tree <span class="math inline">\(d\)</span></li>
<li>Neural nets: number of edges in <span class="math inline">\(W\)</span></li>
</ul></li>
<li>More complex models are more flexible and “rich”</li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>תכונה אחת מעניינת היא המסובכות או הקומפלקסיטי של השיטות. אני בכוונה לא אומר סיבוכיות כי זה מושג אחר לחלוטין.</p>
<p>ראינו שלכל השיטות שלנו יש פרמטר אחד או יותר, כמו כפתור כזה, שניתן לסובב כדי לנוע בין מודל פשטני מאוד למודל מורכב מאוד אולי מורכב מדי.</p>
<p>ברגרסיה אפשר לחשוב על מספר המשתנים שנכנסים למודל, אפשר הרי לנסח מודל עם מספר מצומצם של משתנים, ואפשר להוסיף עוד משתנים או אפילו להנדס עוד ועוד משתנים למודל.</p>
<p>בKNN יש לנו את מספר השכנים K שאפשר לשנות, בעצים יש את עומק העץ, פרמטר הmax_depth, אבל אמרנו שאפשר לחשוב גם על איזשהו סף לירידה בRSS בכל פיצול. אם הסף יהיה נמוך אני אמשיך לפצל ואגדל עצים מורכבים יותר ויותר, ואם יהיה גבוה יהיו לי מעט פיצולים איכותיים אבל עץ שטוח ולא מורכב.</p>
<p>וברשתות יש הרבה היפרמטרים שצריך לכוונן ויכולים לעשות את הרשת גמישה יותר ויותר, לדוגמא מספר הנוירונים ברשת.</p>
<p>המשותף כאמור לכל אלה, ככל שאני מסובב את הכפתור אני מקבל מודל מורכב ועשיר יותר לתיאור היחס בין המשתנים המסבירים למשתנה התלוי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="model-complexity-or-flexibility-1">Model complexity or flexibility</h3>
<ul>
<li>Typical behavior we have seen: as complexity increases, training error decreases, test/prediction error has U-shape:</li>
</ul>

<img data-src="images/u_shape.png" class="r-stretch"><ul>
<li>Optimal complexity: flexible enough to capture relevant information, not too flexible to cause overfitting</li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הדפוס הדומה לכל השיטות שראינו: על ציר הY טעות החיזוי, למשל RMSE, על ציר הX רמת המורכבות של המודל. וככל שהמודל מורכב וגמיש יותר שגיאת הטריין בכחול יורדת, לפעמים עד לאפס, ואילו לשגיאת הטסט יש איזשהו אופטימום.</p>
<p>לא הוכחנו תיאורטית למה זה קורה אלא רק נתנו אינטואיציה, המודל עם הקומפלקסיטי האופטימלי גמיש מספיק כדי לבטא יחסים מורכבים אמיתיים בין X לY אבל לא גמיש עד כדי כך שהוא מתאים את עצמו לכל רעש קטן במדגם הלמידה ומביא לאוברפיטינג.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="estimation-and-approximation-errors" class="slide level2 title-slide center">
<h2>Estimation and Approximation Errors</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ננסה כעת לשיים את שתי התופעות שאנחנו רואים. נתחיל בעוד קצת אינטואיציה ונגיע אל המושגים המתמטיים פרופר: ביאס, וריאנס, והטריידאוף ביניהן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="intuition-estimation-and-approximation-errors">Intuition: Estimation and approximation errors</h3>
<ul>
<li><p><strong>Approximation error</strong>: how well our model can capture the “true” dependence of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span></p></li>
<li><p><strong>Estimation error</strong>: how well can we estimate our model from our training data <span class="math inline">\(Tr\)</span></p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Simple (low-complexity) model: high approximation error, low estimation error</p></li>
<li><p>Complex model: low approximation error, high estimation error</p></li>
</ul>
<p><img data-src="images/bias_variance_tradeoff.png" style="width:40.0%"></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אנחנו נקרא אפרוקיסמיישן ארור, לטעות שמבטאת עד כמה המודל שלנו מסוגל באמת לתפוס את הקשר האמיתי בין X לY. לדוגמא, בדוגמא עם הסיכוי לחלות במחלת לב, אם הוא מושפע משלושה משתנים, גיל, היסטוריה משפחתית ומשקל, אבל אני מכניס למודל רק שני משתנים, גיל ומשקל, הטעות של המודל – האפרוקסימיישן ארור – תנבע מזה שהוא בכלל לא רואה את משתנה ההיסטוריה המשפחתית, הוא לא מדבר בשפה הנכונה לקלוט את התלות האמיתית של Y בX, חסרים לו משתנים, הוא לא מורכב מספיק, הוא דל. כלומר מודל שתהיה לו אפרוקסימיישן ארור גבוהה הוא מודל עם קומפלקסיטי נמוך – מעט מדי משתנים או עץ לא עמוק מספיק.</p>
<p>אסטימיישן ארור, מבטא עד כמה יש לנו מספיק אינפורמציה ממדגם הלמידה כדי לחזות על נתונים חדשים, כדי להתאים את המודל שלנו. בדוגמא של מחלות לב, נניח ואנחנו כוללים במודל בדיוק את שלושת המשתנים שאנחנו צריכים, המודל שלנו עשיר בדיוק כדי לבטא את התלות של Y בX, ולא תהיה לו אפרוקסימיישן ארור. אבל מה אם אגיד לכם שאני לומד משתי תצפיות בלבד? ברור שאין לי שום יכולת לאמן את המודל הזה, והטעות שתהיה לי לא תהיה מפני שהמודל לא נכון, המודל מצוין, פשוט אין לי מספיק תצפיות לקבל הערכה מדויקת של הפרמטרים, אם מדברים על רגרסיה. ואם אני אשתמש בדאטא אחר סביר שאגיע לפרמטרים אחרים, ההחלטה שלי רועשת.</p>
<p>מצד שני, אולי לא שתי תצפיות כי זה מקרה קיצוני, אבל מעט תצפיות כן יספיקו, כדי להתאים מודל פשוט יותר, למשל מודל רגרסיה עם משתנה אחד, אין צורך במיליון תצפיות כדי לתאר קו ישר. והאסטימיישן ארור, כמה אנחנו מדייקים בשערוך הפרמטרים שלנו, תהיה קטנה. אם אשתמש בדאטא אחר סביר שאגיע לאותם פרמטרים, שיפוע וחותך במקרה של קו ישר.</p>
<p>ואנחנו מגיעים לאיזשהו טריידאוף: מודל פשוט מדי עם קומפלקסיטי נמוך, מדבר בשפה “דלה”, יהיה לו אפרוקסימיישן ארור גבוהה אבל אסטימיישן ארור נמוכה כי יהיו לו מעט פרמטרים שנשערך היטב. מודל מורכב ועמוק, יהיה לו אפרוקסימיישן ארור נמוכה כי הוא יכול לבטא יחסים נורא מסובכים, אבל אסטימיישן ארור גבוהה כי נצטרך המון תצפיות לדייק בשערוך כל הפרמטרים שלו.</p>
<p>מה מעניין אותנו? הטסט ארור, שגיאת החיזוי על נתונים שהמודל לא ראה. הציור שלפנינו מפרק אותה לשני מרכיבים (תיכף נראה שזה עוד לא מדויק): האפרוקסימיישן והאסטימיישן ארורז. אנחנו רואים את הצורת U המוכרת לנו ומבינים סוף סוף, אם כי בנפנוף ידיים, איך היא נוצרת. ככל שהמודל מורכב יותר טעות אפרוקסימיישן יורדת. אבל טעות האסטימיישן עולה, אנחנו מחפשים במרחב גדול הרבה יותר של מודלים וצריכים הרבה יותר דאטא כדי לשערך אותם. והצורת U קורית כי יש בחיבור של הטעויות האלו איזו נקודה אופטימלית של מורכבות המודל, שבה שתיהן כמה שיותר נמוכות.</p>
<p>ושאלה אחרונה לראות שהבנו: אם אגדיל עוד ועוד את הדאטא שיש לי לצורך למידה, על איזו מסוג הטעויות אשפיע, אוריד אותה, אפרוקסימיישן או אסטימיישן? אסטימיישן כמובן. ככל שיש לי יותר דאטא אני יכול ללמוד מודלים מורכבים יותר מדויק יותר. אין לזה השפעה על האפרוקסימיישן ארור, שהרי גם עם שתי תצפיות בלבד ניתן לתאר מודל מורכב עם המון משתנים. הבעיה תהיה באסטימיישן שלו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-bias-variance-tradeoff" class="slide level2 title-slide center">
<h2>The Bias Variance Tradeoff</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ננסה לפרמל את המושגים שאנחנו מבינים אינטואיטיבית, ניתן להם את השמות ביאס ווריאנס, ולאחר מכן נדגים עם סימולציה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="squared-error-decomposition">Squared error decomposition</h3>
<ul>
<li><p>For regression, take the standard model: <span class="math inline">\(y = f(x) + \epsilon\;,\;\epsilon \sim (0,\sigma^2)\)</span></p></li>
<li><p>Modeling approach (e.g.&nbsp;OLS), given training data <span class="math inline">\(Tr\)</span>, gives model <span class="math inline">\(\hat{f}(x)\)</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>Assume want to predict at new point <span class="math inline">\(x_0\)</span>, and understand our expected (squared) prediction error:</li>
</ul>
<p><span class="math inline">\(\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2\)</span></p>
<ul>
<li>Note we treat both the training data <span class="math inline">\(Tr\)</span> (and hence <span class="math inline">\(\hat{f}\)</span>) and the response <span class="math inline">\(y_0\)</span> as random variables in our expectations</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתמקד ברגרסיה, שם אנחנו מניחים שY הוא איזושהי פונקציה f של X ועוד רעש עם תוחלת אפס ושונות סיגמא בריבוע, לאו דוקא מהתפלגות נורמלית. מהו f אם ככה, אם התוחלת של אפסילון היא אפס? f הוא התוחלת המותנית של Y בהינתן הנתונים בX. הערך הנמדד של Y הוא לא בדיוק הערך הצפוי שלו, התוחלת, אלא התוחלת ועוד איזשהו רעש.</p>
<p>כעת אנחנו לוקחים טריינינג דאטא TR וממנו לומדים את f.&nbsp;אנחנו יכולים עכשיו לבדוק את הביצועים של המודל על מדגם הטסט, אבל אני רוצה שנחשוב על כל התהליך הזה של דגימת נתונים, בניית מודל f, למידה שלו מהנתונים - כעל תהליך אקראי. ולהסתכל על התוחלת של הלוס שלנו, לדוגמא ברגרסיה השגיאה הריבועית, על פני ביצוע התהליך הזה הרבה פעמים, אם היה לי טריינינג דאטא קצת שונה כל פעם. רוצה לומר גם f_hat המודל שבנינו הוא משתנה מקרי שמבוסס על מקריות מדגם הלמידה שלנו!</p>
<p>אז תגיע תצפית חדשה X0 וכמו שאמרנו נסתכל על תוחלת הלוס שלה, כאן השגיאה בין Y0 האמיתי שלה, למודל שבנינו f_hat של X0. בהקשר לקו הירוק של פרדיקשן ארור שראינו בשקף הקודם אנחנו בעצם מסתכלים על נקודה בו ועושים לה תוחלת, או מסתכלים על ההתנהגות האופיינית שלה, על פני הרבה מדגמי למידה.</p>
<p>כעת אני רושם את ההפרש כסכום של כמה אלמנטים. כל מה שאני עושה זה לחסר ולהוסיף את f של X0, שהוא התלות האמיתית של Y0 בX0, התוחלת המותנית. וגם מחסר ומוסיף את התוחלת של f_hat. שוב, למה זה דבר שקיים לנו? כי f_hat הוא בעצמו מעין משתנה מקרי שכאילו נדגם מתוך הרבה מודלים סופיים שנובעים ממדגמי למידה שונים.</p>
<p>כעת אנחנו פשוט שמים סוגריים ומסמנים אלמנט A, אלמנט B ואלמנט C. נרצה לפתח את הריבוע ולראות אכן ששגיאת החיזוי שלנו בתוחלת מתפרקת לביטויים מעניינים ולתת להם את השם המתמטי שלהם.</p>
<p>אילו מהגורמים תלויים באקראיות מדגם הלמידה?</p>
<p>הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס.</p>
<p>הגורם B - נראה בהתחלה שתלוי במדגם הלמידה</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Which factors are random variables, dependent on <span class="math inline">\(Tr\)</span></p>
<p><span class="math inline">\(\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2\)</span></p>
<p><span class="math display">\[A = y_0 - f(x_0)\]</span></p>
<p><span class="math display">\[B = f(x_0) - \mathbb{E} (\hat{f}(x_0))\]</span></p>
<p><span class="math display">\[C = \mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\]</span></p>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אילו מהגורמים תלויים באקראיות מדגם הלמידה?</p>
<p>הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס כלומר כן משתנה מקרי אבל לא תלוי במדגם הלמידה.</p>
<p>הגורם B - נראה בהתחלה שתלוי במדגם הלמידה כי יש בו את המודל הנאמד f_hat, אבל יש כאן תוחלת, כלומר זה קבוע. כך שגורם זה הוא מספר, לא משתנה מקרי, ומה היינו מצפים שיהיה המספר הזה? אפס. אנחנו מקווים שהמודל שלנו עשיר מספיק שבסופו של דבר בתוחלת הוא קירוב טוב ליחס האמיתי f.</p>
<p>הגורם C - כאן בעצם יש את הגורם שתלוי במדגם הלמידה - המרחק של f_hat המודל שלמדנו מהתוחלת שלו. אם אני מעלה את זה בריבוע ולוקח תוחלת מה זה? שונות! שונות המודל f_hat.</p>
<p>כך שעוד לפני שהוכחנו, אתם כבר יכולים להבין שגורם A הוא איזושהי טעות שאין לי הרבה מה לעשות לגביה. גורם B הוא בעצם האפרוקסימיישן ארור, כמה המודל שלנו בתוחלת עשיר מספיק כדי לבטא את הקשר האמיתי בין X לY. וגורם C הוא האסטימיישן ארור, האם יש לי מספיק דאטא כדי שאם אחזור על התהליך הזה עם דאטא קצת אחר אקבל מודל דומה, כלומר כמה קטנה השונות של המודל עצמו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-variance-decomposition">The bias-variance decomposition</h3>
<p><span class="math inline">\(\mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2 =\)</span><br><br> <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\;= \mathbb{E} A^2 + B^2 + \mathbb{E} C^2 + 2 B \cdot \mathbb{E} A + 2 \mathbb{E} (AC) + 2B \cdot\mathbb{E} C\)</span><br><br></p>
<div class="fragment">
<p><span class="math inline">\(\mathbb{E}(A^2) = \sigma^2\)</span> the <strong>Irreducible error</strong> of a perfect model which knows the true <span class="math inline">\(f\)</span></p>
<p><span class="math inline">\(B^2 = \left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)^2\)</span> is the <strong>squared bias</strong> — a measure of approximation error (note <span class="math inline">\(B\)</span> is not a random variable)</p>
<p><span class="math inline">\(\mathbb{E}(C^2) = \mathbb{E} \left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0) \right)^2\)</span> is the <strong>variance</strong> of the prediction — a measure of estimation error</p>
<p><span class="math inline">\(B \cdot\mathbb{E} A = \mathbb{E} (AC) = B \cdot \mathbb{E} C = 0\)</span> due to independence and mean-0 relations</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כעת כשאני מעלה את סכום שלושת הגורמים בריבוע אני מקבל את כל אחד מהם בריבוע ועוד 2 כפול מכפלה של כל זוג. כשאני לוקח תוחלת, אני טוען שאנחנו נשארים עם הביטוי שלפנינו. מדוע?</p>
<p>התוחלת של A בריבוע - אין לי מה לעשות איתה. התוחלת של B בריבוע היא בעצם B בריבוע כי אמרנו שB לא משתנה מקרי. התוחלת של C בריבוע נשארת כפי שהיא למרות שאמרנו שאנחנו יודעים כבר שזאת שונות. ונשאר לי התוחלות של כל המכפלות, מכל אחת אני מוציא החוצה את B שאיננו משתנה מקרי.</p>
<p>מהם הגורמים שקיבלנו?</p>
<p>התוחלת של A בריבוע היא התוחלת של אפסילון בריבוע, כלומר היא השונות של אפסילון, שהיא סיגמא בריבוע. אנחנו קוראים לזה irreducible error, זה הרעש הקיים בטבע, שגם תחת מודל מדויק לא נצליח להפחית. אפסילון לא תלוי בדאטא שלנו, בהגדרה אין לנו דאטא לחזות אותו.</p>
<p>הגורם השני – B בריבוע – הוא כאמור קבוע, לא משתנה מקרי, והוא מבטא את האפרוקסימיישן ארור, עד כמה המודל שלנו בתוחלת עשיר מספיק וקרוב לf האמיתית. היינו רוצים שזה יהיה אפס, שלא תהיה הטיה בילט אין במודל שלנו. אבל אולי למשל לא הכנסנו את כל המשתנים שצריך, ויש הטיה, אז זאת מעין הטיה בריבוע, ואכן אנחנו קוראים לזה squared bias.</p>
<p>הגורם השלישי – C בריבוע – הוא כמו שאמרנו הvariance של החיזוי בנקודה X0, וזה מדד לאסטימיישן ארור. מודל עם שונות נמוכה, גם אם אקח דאטא קצת אחר אקבל חיזוי מאוד דומה, האמידה תהיה יציבה. מודל עם שונות גבוהה – זוכרים את עצי ההחלטה? – אם אקח דאטא קצת אחר אקבל חיזוי שונה, האמידה לא יציבה. ואמרנו שמתכון לטיפול בטעות כזאת יכול להיות למשל להגדיל את מדגם הלמידה, כאן הוא בא לידי ביטוי. איפה ראינו את זה כבר, מתמטית? כשדיברנו על ממוצע המדגם המקרי, שהשונות שלו היא סיגמא בריבוע חלקי n, וככל שn גדול יותר ככה היא תקטן. הרי גם בממוצע המדגם אפשר לראות עם קצת מאמץ מודל חיזוי פשוט.</p>
<p>מה נשאר לנו? להראות שכל שאר הביטויים של מכפלות הם אפס.</p>
<p>התוחלת של הגורם A, היא התוחלת של אפסילון, היא אפס. אז כל הביטוי של B כפול התוחלת של A הוא אפס.</p>
<p>התוחלת של C, היא התוחלת של f_hat פחות התוחלת של f_hat, כלומר גם היא 0 וכל הביטוי של B כפול התוחלת של C הוא אפס.</p>
<p>התוחלת של מכפלת A ו-C היא גם כן אפס. כי A ו-C הם משתנים מקריים בלתי תלויים לכן התוחלת של המכפלה שלהם היא מכפלת התוחלות והתוחלת של כל אחד מהם היא אפס.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-variance-decomposition-1">The bias-variance decomposition</h3>
<p><span class="math display">\[\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}\]</span></p>
<div class="fragment">
<p>And what was wrong with this pattern?</p>
<p><img data-src="images/bias_variance_tradeoff.png" style="width:40.0%"></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כך שאנחנו רואים ששגיאת החיזוי שלנו בשורה התחתונה, על דאטא שהמודל לא ראה, היא סכום הגורמים הריבועיים: שגיאת רעש טבעי שאינה תלויה בדאטא או במודל. שגיאת הטייה ריבועית, שתלויה במודל אבל לא בדאטא. ושגיאת שונות המודל שתלויה גם במודל וגם בדאטא.</p>
<p>ואם נחזור לתמונה שציירנו, מה לא מדויק בה?</p>
<p>הקו הירוק צריך להיות סכום של שלושה קווים, שגיאת ההטייה הריבועית (אפרוקסימיישן), שגיאת השונות של המודל (אסטימיישן) ושגיאת הirreducible error, שלא תלויה במורכבות המודל, כלומר איך היא תראה? קו ישר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="using-the-bias-variance-decomposition">Using the bias-variance decomposition</h3>
<ul>
<li><p>Our general intuition: as complexity increases, approximation error decreases and estimation error increases</p></li>
<li><p>For many models we can calculate and show these effects on the bias and variance of the model</p></li>
</ul>
<div class="fragment">
<ul>
<li>For example, for OLS regression, we can prove:</li>
</ul>
<ol type="1">
<li>That the squared bias decreases when we add more variables into the model</li>
<li>That the variance of prediction increases when we add more variables (and calculate it, under some assumptions)</li>
</ol>
</div>
<div class="fragment">
<ul>
<li>Similarly for k-NN regression the squared bias decreases and the variance increases as we decrease <span class="math inline">\(k\)</span> (fewer neighbors, more flexibility/complexity)</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>באופן כללי אנחנו רואים שהמתמטיקה מסתדרת עם האינטואיציה שלנו, ואפשר ממש לחשב את הטעויות האלה ולהראות שככל שהמודל מורכב יותר הביאס או אפרוקסימיישן יורד, והוריאנס או האסטימיישן יורד.</p>
<p>לדוגמא ברגרסיה ליניארית, ככל שנוסיף עוד משתנים המודל יהיה מורכב ומדויק יותר, ההטיה תרד ותרד. ומצד שני השונות תגדל ותגדל, האומדים שלנו ייהפכו יותר ויותר לא מדויקים ונצטרך יותר ויותר דאטא כדי שזה לא יתדרדר.</p>
<p>באופן דומה אפשר לחשוב על KNN רק ששם הכפתור הזה של מספר שכנים עובד הפוך – ככל שיש פחות שכנים המודל נורא מורכב וספציפי והביאס ירד, אבל גם השונות תעלה, אנחנו תולים את מבטחנו במעט מאוד שכנים שזאת החלטה מאוד רועשת, ספציפית מאוד.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="simulation-example-bias-variance-and-prediction-error">Simulation example: bias, variance and prediction error</h3>
<ul>
<li><p>Let’s generate data according to the following model: <span class="math inline">\(x \in \mathbb R^{20}\)</span> has multivariate normal distribution, <span class="math inline">\(y = \sum_{j=1}^{20} \sqrt{(21-j)/2} \times x_j + \epsilon\;,\;\epsilon \sim N(0, 1000)\)</span></p></li>
<li><p>So the true model is in fact linear with <span class="math inline">\(\beta = (\sqrt{10},\sqrt{9.5},\dots,\sqrt{0.5})^t\)</span></p></li>
<li><p>We have <span class="math inline">\(n=50\)</span> training observations, and want to predict at <span class="math inline">\(x_0 = (1,1,\dots,1)^t\; \Rightarrow\; y_0 = 43.6 + \epsilon\)</span></p></li>
<li><p>By generating many training sets and <span class="math inline">\(\hat{f}\)</span>’s we can evaluate bias, variance and prediction error</p></li>
<li><p>Complexity parameter: number of variables included in the model (only the first coordinate, first two, …)</p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נראה כעת סימולציה שממחישה את הדפוס הזה יפה עם מודל ליניארי. יש לנו וקטור משתנים מסבירים X בגודל P = 20. והמקדמים שלו לפי הנוסחה הזאת, שורש 10, שורש 9.5 וכולי, כלומר הם הולכים וקטנים עד שורש חצי. המקדמים הראשונים הם הכי “חשובים, משפיעים” על Y והאחרונים פחות. זאת הf האמיתית שלנו, מודל ליניארי, אלה הם הבטאות.</p>
<p>כעת תעניין אותנו תצפית חדשה לחיזוי X0 שהיא בעצם 1 בכל המשתנים, אם נחשב את Y0 נראה שהוא צריך להיות 43.6 ועוד איזשהו רעש אפסילון שנדגם מהתפלגות הרעש, כאן היא נורמלית. ויהיה לנו תקציב של 50 תצפיות בלבד.</p>
<p>נייצר הרבה מדגמים עם היחס האמיתי הזה וכל פעם נבנה מודל f_hat אחר, על מדגמים שונים ועם מספר משתנים הולך וגדל, כלומר מורכב יותר ויותר. בצורה הזאת אנחנו יכולים לאמוד ממש אמפירית את שלושת הגדלים שחישבנו ולראות שהם מסתכמים בשגיאת החיזוי. מובן שאת הreducible error לא צריך לאמוד, היא ידוע בסימולציה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" data-code-line-numbers="|1-7|9|10-12|13|14|15-16|18-20|"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>ntr <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>p <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>sigma_sq <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>n_iter <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>beta <span class="op">=</span> np.sqrt(np.array(<span class="bu">range</span>(<span class="dv">20</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>yhat_0 <span class="op">=</span> np.zeros((<span class="dv">20</span>, n_iter))</span>
<span id="cb1-7"><a href="#cb1-7"></a>err <span class="op">=</span> np.zeros((<span class="dv">20</span>, n_iter))</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb1-10"><a href="#cb1-10"></a>    X <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>(ntr, p))</span>
<span id="cb1-11"><a href="#cb1-11"></a>    Y <span class="op">=</span> X <span class="op">@</span> beta <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span>np.sqrt(sigma_sq), size<span class="op">=</span>ntr)</span>
<span id="cb1-12"><a href="#cb1-12"></a>    y0 <span class="op">=</span> np.<span class="bu">sum</span>(beta) <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span>np.sqrt(sigma_sq), size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13"></a>    <span class="cf">for</span> pnow <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, p <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-14"><a href="#cb1-14"></a>        betahat <span class="op">=</span> np.linalg.inv(X[:, :pnow].T <span class="op">@</span> X[:, :pnow]) <span class="op">@</span> X[:, :pnow].T <span class="op">@</span> Y</span>
<span id="cb1-15"><a href="#cb1-15"></a>        yhat_0[pnow <span class="op">-</span> <span class="dv">1</span>, iteration] <span class="op">=</span> np.<span class="bu">sum</span>(betahat)</span>
<span id="cb1-16"><a href="#cb1-16"></a>        err[pnow <span class="op">-</span> <span class="dv">1</span>, iteration] <span class="op">=</span> y0 <span class="op">-</span> np.<span class="bu">sum</span>(betahat)</span>
<span id="cb1-17"><a href="#cb1-17"></a>        </span>
<span id="cb1-18"><a href="#cb1-18"></a>pred_err <span class="op">=</span> np.mean(err<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># pred-error</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>pred_bias <span class="op">=</span> (np.mean(yhat_0, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> np.<span class="bu">sum</span>(beta))<span class="op">**</span><span class="dv">2</span> <span class="co"># E(B^2)</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>pred_var <span class="op">=</span> np.var(yhat_0, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># E(C^2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כאן אני מאתחל את מספר התצפיות, את סיגמא בריבוע, מספר האיטרציות והבטאות לפי הנוסחא. אני מאתחל גם מערך של החיזויים ל-Y0 לכל אחד מ-20 המודלים לכל 1000 האיטרציות. ואותו גודל של מערך לשגיאת החיזוי הסופית.</p>
<p>כעת נחזור על 20 המודלים 1000 פעם כדי שנוכל לחשב ממוצע עליהם, כמו תוחלת.</p>
<p>אנחנו מגרילים X עם 50 תצפיות -20 משתנים מהתפלגות נורמלית סטנדרטית. מחשבים את Y באמצעות המודל האמיתי f שהוא המודל הליניארי ומוסיפים את האפסילונים מהתפלגות נורמלית עם שונות 1000. לבסוף אנחנו דוגמים תצפית אמיתית שלא תהיה חלק מהמודל, Y0. למה Y0 שווה לסכום הבטאות ועוד רעש אפסילון? כי אמרנו שX0, התצפית החדשה, תהיה וקטור של אחדות.</p>
<p>עכשיו אני בונה 20 מודלים, מתחיל ממשתנה אחד ומוסיף כל פעם משתנה למודל מורכב יותר ויותר עד 20 משתנים במודל.</p>
<p>לכל מודל כזה מחשב את בטא-האט על מדגם הלמידה לפי הנובחה המוכרת.</p>
<p>ואז מחשב את החיזוי לתצפית החדשה X0, זה כאמור סכום האומדנים לבטאות, הבטא-האט. ואת טעות החיזוי, Y0 האמיתית פחות Y0 החזוי.</p>
<p>עכשיו אני מחשב את תוחלת טעות החיזוי הריבועית. זה בעצם מיצוע של השגיאות בריבוע, לכל אחד מ20 המודלים על פני 1000 איטרציות. נחשב גם את ההטיה בריבוע, זה ממוצע החיזויים על פני 1000 איטרציות, פחות f0, התצפית האמיתית ללא רעש, וכל זה בריבוע. ולבסוף, התוחלת של C בריבוע זה בעצם שונות החיזוי כמו שראינו. איפה הגורם A בריבוע? אמרנו שלא צריך לאמוד אותו, זה סיגמא בריבוע שידועה בסימולציה, היא 1000.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, p <span class="op">+</span> <span class="dv">1</span>), pred_err, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'prediction error'</span> )</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">1</span>, p <span class="op">+</span> <span class="dv">1</span>], [sigma_sq, sigma_sq], color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">'$E(A^2)$:irreducible error'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, p <span class="op">+</span> <span class="dv">1</span>), pred_bias, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$E(B^2)$:prediction bias squared'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, p <span class="op">+</span> <span class="dv">1</span>), pred_var, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$E(C^2)$:prediction variance'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'dimension'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'squared error'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error decomposition simulation'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="c14_predmodeling_theory_files/figure-revealjs/cell-4-output-1.png" width="527" height="449"></p>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ובאמת, כשאנחנו מסרטטים את טעות החיזוי, הirreducible error, ההטיה הריבועית והשונות מתקבל הדפוס הצפוי.</p>
<p>הirreducible error היא כמובן קבוע, 1000, קו ישר. ההטיה בריבוע, הקו האדום, מתחיל מאוד גבוה ממודל פשטני מדי שכולל רק משתנה אחד ועד המודל הנכון עם 20 משתנים שיש לו הטיה אפס. אם אגב במודל רק 15 משתנים היו רלוונטיים, היינו מגיעים לאפס הזה מהר יותר, ב-15 משתנים. לבסוף שונות החיזוי, מתחילה מכמעט אפס עבור מודל פשטני עם משתנה אחד, 50 תצפיות מספיקות כדי להעריך את הפרמטר שלו במדויק. אבל השונות עולה ועולה ככל שהמודל מורכב יותר, 50 תצפיות פשוט לא מספיקות לשערך מודל עם 20 משתנים גם אם הוא נכון. שימו-לב שהיא עולה בצורה ליניארית, אפשר להוכיח למה זה תמיד קורה ברגרסיה ליניארית.</p>
<p>ומה עם שגיאת החיזוי הריבועית, בשורה התחתונה? היא אכן בעלת צורת U שראינו, רק שעכשיו אנחנו מבינים ממש מתמטית איך הU נוצר. ואפשר לראות שבקירוב היא שווה לסכום שלוש השגיאות הריבועיות האחרות.</p>
<p>שימו-לב שוב, זה נכון שהמודל עם 20 משתנים הוא הוא הנכון ביותר. אבל עם תקציב של 50 תצפיות, המסר הוא שניתן לשערך היטב מודל של כ12, משתנים בלבד.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="complexity-parameters-in-our-models">Complexity parameters in our models</h3>
<table>
<colgroup>
<col style="width: 24%">
<col style="width: 30%">
<col style="width: 22%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Complexity param(s)</strong></th>
<th><strong>Low Complexity</strong></th>
<th><strong>High Complexity</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear/logistic regression</td>
<td>Number of variables</td>
<td>Few variables</td>
<td>Many variables</td>
</tr>
<tr class="even">
<td>k-NN</td>
<td>Number of neighbors</td>
<td>Many neighbors</td>
<td>Few neighbors</td>
</tr>
<tr class="odd">
<td>Tree</td>
<td>Depth</td>
<td>Shallow</td>
<td>Deep</td>
</tr>
<tr class="even">
<td>Neural Nets</td>
<td>Number of hidden nodes and layers</td>
<td>Few</td>
<td>Many</td>
</tr>
</tbody>
</table>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אפשר לסכם עם הטבלה הזאת:</p>
<p>מודל עם קומפלקסיטי נמוך – בא עם אסטימיישן ארור נמוך אבל אפרוקיסמיישן ארור גבוה.</p>
<p>מודל עם קומפלקסיטי גבוה – בא עם אסטימיישן ארור גבוה אבל אפרוקסימיישן ארור נמוך.</p>
<p>ומהו הכפתור העיקרי שנותן לנו קומפלקסיטי נמוך וגבוה? ברגרסיה מספר המשתנים, מעט זה לואו קומפלקסיטי, הרבה זה היי קומפלקסיטי. בKNN הרבה שכנים זה לואו קומםלקסיטי ומעט זה היי קומפלקסיטי. בעצים עץ שטוח כלומר מעט שכונות זה לואו, עץ עמוק עם הרבה שכונות זה היי. וברשתות מעט שכבות עם מעט נוירונים זה לואו, והרבה פרמטרים זה היי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="cross-validation" class="slide level2 title-slide center">
<h2>Cross validation</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>לנושא הבא ודאי נחשפתם כבר בתרגול אבל הגיע הזמן להציג אותו באופן רשמי - cross validation.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="cross-validation-beyond-the-test-set">Cross validation: beyond the test set</h3>
<ul>
<li><p>So far we simply divided our data to 80% training and 20% test</p></li>
<li><p>We mentioned that if we also have to choose a model parameter (like number of variables or <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-NN), we should have training-validation-test (usually 60-20-20), where we use validation to select model, and test for final evaluation</p></li>
<li><p>But there are more efficient ways to use the data</p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>עד כה בהרצאה לפחות חילקנו את הדאטה שלנו לשניים, 80 אחוז למדגם למידה ו20 אחוז למדגם טסט. בתחילת השיעור רמזנו שאם יש כפתור כזה של מודל קומפלקסיטי שצריך לבחור, כמו K בKNN, לא סביר להשתמש במדגם הטסט גם לבחור אותו וגם לדווח על שגיאת החיזוי הצפויה מהמודל. נכון יותר סטטיסטית לחלק את הדאטה לשלושה חלקים: מדגם למידה, נאמר 60 אחוז מהדאטא, מדגם ולידציה, נאמר 20 אחוז מהדאטא והוא ישמש לסיבוב הכפתור, בחירת ההיפרפרמטרים הרלוונטיים, ומדגם הטסט שישמש אותנו לאווליואציה סופית, מה הRMSE או הprecision הצפוי מהמודל, למשל.</p>
<p>אבל יש דרך יעילה יותר להשתמש בנתונים אם חושבים על זה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="fold-cross-validation">10-fold cross validation</h3>
<ol type="1">
<li>Divide the data into 10 equal size parts (folds)</li>
<li>Repeat 10 times:
<ol type="a">
<li>Fit the model on 90% of the data</li>
<li>Apply the model to the holdout fold</li>
</ol></li>
<li>Evaluate the modeling approach on 100% of the data by combining the holdout folds</li>
</ol>
<p>Once the complexity parameter has been chosen, train the model one last time on the entire data before testing it in production on <em>actual</em> unseen data.</p>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>לדוגמא 10-פולד קרוס ולידיישן. נחלק את הדאטא לעשרה חלקים שווים באופן אקראי, אלה הם הfolds או “קפלים”. ונחזור על הפרוצדורה הבאה 10 פעמים. נאמן את המודל על 90 אחוז מהדאטא, כלומר תשעה פולדים. ונעשה לו אבליואציה על 10 אחוז מהדאטא, כלומר על הפולד שלא השתמשנו בו לאימון. מובן שכל פולד ישמש בתורו כמדגם ההולדאאוט או טסט סט, כשהתשעה האחרים משמשים כמדגם הלמידה.</p>
<p>לבסוף כדי לדווח על הביצועים של המודל או כדי לבחור בקומפלקסיטי הראוי למודל, נסתכל על ממוצע טעות החיזוי שעל 100 אחוז מהדאטא, על פני כל הפולדים.</p>
<p>באופן זה אימנתי לא על 60 או 80 אחוז מהדאטא, אימנתי כל פעם על 90 אחוז מהדאטא, ומיצעתי את עקומת טעות החיזוי המוכרת לנו על פני מספר עותקים של הדאטא, כל פעם ראיתי מדגם אקראי אחר. וכמו בכל מיצוע שראינו עד כה, המשמעות היא שגם החלק הזה של בחירת הקומפלקסיטי עצמו, יהיה רועש פחות, השונות של התהליך הזה תקטן.</p>
<p>דבר אחרון שחשוב למדעני נתונים שעוסקים בפרקטיקה, אם הדגש של המודל הוא באמת על חיזוי, למשל בסביבת פרודקשן של אתר כלשהו, יש לאמן אותו פעם אחת אחרונה על 100 אחוז מהתצפיות עם רמת הקומפלקסיטי שנבחרה, הכפתור שסובבנו. כך שמודל סופי שאנחנו באמת עושים לו דיפלוימנט יאומן על כל הדאטא שהיה ברשותנו ולפיכך אמור רק להשתפר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="leave-one-out-cross-validation-loocv">Leave-one-out cross validation (LOOCV)</h3>
<ul>
<li><p>Can go beyond 10-fold, to n-fold: each time fit the model on <span class="math inline">\(n - 1\)</span> observations, and hold-out <span class="math inline">\(1\)</span></p></li>
<li><p>In mathematical notation: <span class="math inline">\(L_{n-fold} = \sum_{i=1}^n L\left(y_i, \hat{f}^{(-i)}(x_i)\right)\)</span></p></li>
<li><p>Where:</p>
<ul>
<li><span class="math inline">\(L\)</span> is the loss function we use for evaluation</li>
<li><span class="math inline">\(\hat{f}^{(-i)}\)</span> is the model we build on <span class="math inline">\(n-1\)</span> observations, removing observation <span class="math inline">\(i\)</span></li>
<li>We then apply this model to the left out observation and see how well we do</li>
</ul></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נשאלת השאלה למה לחלק לעשרה פולדים? אפשר לחלק גם ל-n פולדים, כשn הוא גודל הדאטא. בדרך זו המודל יתאמן בכל פולד על 99.99 אחוז מהנתונים, כמעט 100 אחוז.</p>
<p>זה באמת מה שעומד מאחורי leave-one-out cross validation. כל מודל נבנה על n - 1 תצפיות ונבדק על התצפית האחת שנותרה.</p>
<p>אפשר לרשום את זה בצורה כזאת: L היא איזושהי פונקצית הפסד שלפיה אנחנו שופטים את ביצועי המודל, לדוגמא הפסד ריבועי, f_hat פחות i זה המודל של שאומן על כל התצפיות חוץ מi והחיזוי שלו על תצפית i. וטעות החיזוי הכללית שלנו תהיה סכום ההפסדים על פני n המודלים.</p>
<p>אם רוצים לכוונן גם איזשהו כפתור קומפלקסיטי, לדוגמא להחליט בין חמישה לעשרה שכנים, נצטרך להוסיף פה גם איזה אינדקס K, כלומר לעשות את זה לכל בחירה של K או פרמטר קומפלקסיטי כלשהו באופן כללי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="cross-validation-advantages-and-issues">Cross validation: advantages and issues</h3>
<p>Advantages:</p>
<ol type="1">
<li>Training set is bigger (<span class="math inline">\(0.9n\)</span> or <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(0.8n\)</span>) — better models and more realistic</li>
<li>Test set is bigger (essentially of size <span class="math inline">\(n\)</span>) — more stable model evaluation and selection</li>
</ol>
<p>Disadvantages:</p>
<ol type="1">
<li>Have to build 10 (or <span class="math inline">\(n\)</span>) models — much more computing</li>
<li>What exactly are we evaluating — we don’t have a single model</li>
</ol>
<p>Lots of interesting perspectives and results</p>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מהם היתרונות של קרוס ולידיישן?</p>
<p>יתרון אחד הוא שימוש יעיל יותר בנתונים, אפשר לאמן מודל על 90 אחוז מהדאטא או n פחות 1, במקום 80 אחוז, כלומר מודלים טובים יותר.</p>
<p>יתרון שני הוא שכל הדאטא גם משתתף בטסט סט, כל פעם בפולד אחר, לכן האווליואציה עצמה עם שונות קטנה יותר וכוונון הכפתור שלנו יהיה יציב יותר, מושכל יותר.</p>
<p>מהם החסרונות של קרוס ולידיישן?</p>
<p>קודם כל חיסרון חישובי. אם מדובר במודל שלוקח זמן לאמן, נצטרך הרבה יותר זמן לאימון, הרבה יותר חישוב. אם אנחנו רוצים לכוונן את מספר המשתנים בין 1 ל100 למשל, זה גם מכפיל את הכמות הזאת פי 100. כאן כמובן אפשר לחשוב על מימוש חכם שימקבל את האימון, ובסיטואציות מסוימות אפשר בכלל להימנע מאימון של כל המודלים או אימון שלהם בצורה הדרגתית ויעילה יותר, אבל זה מחוץ לסקופ שלנו כאן.</p>
<p>החיסרון האחר הוא שבסופו של דבר המודל ייבחן פעם אחת, בפרודקשן, והטעות הממוצעת הזאת אמנם על כל הנתונים אבל על פני כמה פולדים, אולי לא משקפת בדיוק את המבחן האמיתי של המודל. גם כאן אם הדאטא גדול מאוד אפשר לחשוב על שילוב הגישות, בכל זאת להשאיר קצת דאטא בצד לטסט סט סופי סופי שבאמת לא ניגע בו ונבחן את ביצועי המודל עליו כאילו היה באמת דאטא שמגיע בסביבת הפרודקשן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularization" class="slide level2 title-slide center">
<h2>Regularization</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הנושא האחרון בו ניגע ביחידה הזאת שהיא יותר תיאורטית, הוא רגולריזציה. ניקח למשל את המודל של רגרסיה ליניארית. כדי להפוך את המודל יותר ויותר מורכב אנחנו דיברנו על האפשרות של הוספת עוד ועוד משתנים למודל. רגולריזציה מאפשרת לשלוט בקומפלקסיטי של המודל בגישה אחרת.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="regularization-controlling-the-bias-variance-tradeoff">Regularization: controlling the bias-variance tradeoff</h3>
<ul>
<li><p>As we saw, in OLS, the more variables in the model, the bigger the prediction variance</p></li>
<li><p>This is because we have to estimate many parameters, high model complexity</p></li>
<li><p>Regularization controls model complexity not by reducing the number of parameters, but in other ways</p></li>
<li><p>Most common: restrict the norm of <span class="math inline">\(\hat{\beta}\)</span>, instead of allowing it to take any value in <span class="math inline">\(\mathbb R^p\)</span></p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז איך נשלוט בשונות המודל בדרך קצת יותר מחוכמת, בלי לשנות את מספר המשתנים במודל?</p>
<p>הרי מה אנחנו עושים כשאנחנו משנים את מספר המשתנים? אנחנו מרחיבים או מצמצמים את מרחב החיפוש של הפרמטרים שלנו. אם אנחנו משתמשים בשני משתמשים המרחב נורא קטן, אם אנחנו משתמשים במאה משתנים הוא כבר עצום. רגולריזציה, שולטת בגודל הזה של מרחב החיפוש בדרך אחרת – על ידי הגבלת הנורמה של וקטור המקדמים. במקום לרדת מR^p לR ממימד קטן יותר, נישאר בR^p, אבל לא נרשה לוקטור בטא להיות בכל R^p.&nbsp;ספציפית נגדיר איזשהו כדור בR^p שרק שם מותר לוקטור בטא, למודל להיות, כך שלא ישתולל.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="norm-penalties-and-ridge-regression">Norm penalties and ridge regression</h3>
<ul>
<li><p>The common norms to penalize:</p>
<ul>
<li>Euclidean (<span class="math inline">\(\ell_2\)</span>) norm: <span class="math inline">\(\|\beta\|_2^2 = \sum_{j=1}^p \beta_j^2\;\;\)</span> <span class="math inline">\(\;\Rightarrow\;\;\)</span> <strong>Ridge regression</strong></li>
<li><span class="math inline">\(\ell_1\)</span> norm: <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^p |\beta_j|\;\;\)</span> <span class="math inline">\(\;\Rightarrow\;\;\)</span> Lasso</li>
</ul></li>
<li><p>For ridge regression, <span class="math inline">\(\hat{\beta}\)</span> is the solution of: <span class="math inline">\(\min_{\|\beta\|^2\leq c} RSS (\beta)\)</span></p></li>
<li><p>Alternative Largrange form: <span class="math inline">\(\min_\beta RSS(\beta) + \lambda \|\beta\|^2\)</span></p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מתמטית, אנחנו צריכים קודם כל לבחור נורמה. הנורמות המקובלות הן או L2, שמשמעותה סכום הבטאות בריבוע. או L1, שמשמעותה סכום הבטאות בערך מוחלט.</p>
<p>כשמדובר בנורמת L2 הפרוצדורה נקראת רידג’ ריגרשן ועליה נרחיב, כשמדובר על נורמת L1 זה נקרא לאסו ריגרשן, עליה לא נרחיב.</p>
<p>ומה זה אומר להגביל את הנורמה? זה אומר במקום להביא למינימום את הRSS בלי שום אילוץ, להביא אותו למינימום תחת האילוץ שנורמת הL2 לא גדולה מדי, היא קטנה מאיזשהו פרמטר C. למי שזוכר מלימודי החדו”א שלו, זה ממש מציאת מינימום בתוך הכדור שסביב הראשית במרחב הפרמטרים הp מימדי הזה.</p>
<p>בעיה שקולה היא לקחת את הקריטריון למינימום ולהוסיף לאילוץ כופל לגראנז’ למדא. וכשאנחנו כותבים את הקריטריון הסופי בצורה כזו זה אומר מציאת מינימום לRSS תוך ענישת הנורמה של וקטור המקדמים בטא. אם נשים למדא גדול מאוד, העונש יהיה כבד והמקדמים שלנו ייאלצו להיות קטנים, כלומר להתקרב לראשית. אם למדא יהיה קטן העונש יהיה קל יותר ונאפשר למקדמים להיות גדולים יותר. זאת בעיה קלה יותר לפתרון אבל הן שקולות.</p>
<p>ומה כל זה יועיל לנו? אפשר להוכיח שבדרך זו אנחנו מקטינים את השונות של וקטור המקדמים אבל לא נעשה את זה כאן. אני מעדיף שנחשוב על זה לפי המוטיבציה שהביאה אותנו לענישה הזאת של הנורמה – אנחנו מקטינים את מרחב החיפוש של המקדמים ומגדילים אותו ובאופן הזה שולטים בקומפלקסיטי של המודל, עם פרמטר למדא במקום p.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="solving-ridge-regression">Solving ridge regression</h3>
<ul>
<li><p>We want to minimize penalized RSS: <span class="math inline">\(PRSS(\beta) = \|Y-X\beta\|^2 + \lambda \|\beta\|^2.\)</span></p></li>
<li><p>Differentiating relative to <span class="math inline">\(\beta\)</span> and equating to <span class="math inline">\(0\)</span> : <span class="math inline">\(\nabla_\beta PRSS(\beta) = -2X^TY + 2X^TX\beta + 2\lambda \beta = 0.\)</span></p></li>
<li><p>Solution: <span class="math inline">\(\hat{\beta}(\lambda) = (X^TX + \lambda I_p)^{-1} X^T Y.\)</span></p></li>
<li><p>This is a minimum because the function is quadratic in <span class="math inline">\(\beta\)</span> (or can check Hessian)</p></li>
<li><p>Similar to OLS solution, with additional term in the inverse</p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מכל מקום, אם בחקנו במטריקת L2 מסתבר שעדיין יש פתרון סגור לרגרסית רידג’. הקריטריון שלנו הוא כעת penalized RSS או PRSS, השאריות בריבוע ועוד עונש על הנורמה הריבועית. נשים לב שזו עדיין פונקציה קוואדרטית של וקטור בטא לכן יהיה לנו פתרון. כמו שעשינו ברגרסיה אפשר לגזור את הPRSS לפי בטא, לקבל את הביטוי שמופיע לנו כאן (בעצם אותו ביטוי שקיבלנו ברגרסיה ליניארית ועוד נגזרת הענישה), להשוות לאפס, ולחלץ לבטא.</p>
<p>יתקבל הביטוי שלפנינו, X’X ועוד מטריצה אלכסון עם למדא על האלכסון, כל זה בהופכי כפול X טרנספוז Y.</p>
<p>מה ההבדל בין פתרון רידג’ לבטא לפתרון OLS שאנחנו מכירים לבטא? כאן נוסף עוד איזשהו קבוע קטן לאלכסון של המטריצה X’X, עיבינו קצת את האלכסון, ואפשר לחשוב שהוא נראה עכשיו בולט יותר כמו רכס, לכן השם ridge regression.</p>
<p>מלבד זאת אפשר כאמור להראות שאנחנו מקטינים או מייצבים את השונות של וקטור המקדמים של בטא באמצעות הוספת קבוע קטן למטריצה הזאת לפני ההפיכה שלה, פרוצדורה מקובלת מאוד באלגברה לטיפול במטריצות לא יציבות שאנחנו רוצים להפוך. מי שרוצה עוד פיתוחים מתמטיים על פתרון רידג’ מוזמן לקחת קורסים מתקדמים יותר בנושא. כרגע כהרגלנו בקודש נסתפק בסימולציה שמראה את התועלת.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" data-code-line-numbers="|13-14|"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>ntr <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>p <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>sigma_sq <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>n_iter <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>beta <span class="op">=</span> np.sqrt(np.array(<span class="bu">range</span>(<span class="dv">20</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>))<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>yhat_0 <span class="op">=</span> np.zeros((<span class="dv">30</span>, n_iter))</span>
<span id="cb3-7"><a href="#cb3-7"></a>err <span class="op">=</span> np.zeros((<span class="dv">30</span>, n_iter))</span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb3-10"><a href="#cb3-10"></a>    X <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>(ntr, p))</span>
<span id="cb3-11"><a href="#cb3-11"></a>    Y <span class="op">=</span> X <span class="op">@</span> beta <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span>np.sqrt(sigma_sq), size<span class="op">=</span>ntr)</span>
<span id="cb3-12"><a href="#cb3-12"></a>    y0 <span class="op">=</span> np.<span class="bu">sum</span>(beta) <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span>np.sqrt(sigma_sq), size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-13"><a href="#cb3-13"></a>    <span class="cf">for</span> lamb <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">29</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-14"><a href="#cb3-14"></a>        betahat <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X <span class="op">+</span> lamb <span class="op">*</span> np.identity(p)) <span class="op">@</span> X.T <span class="op">@</span> Y</span>
<span id="cb3-15"><a href="#cb3-15"></a>        yhat_0[lamb, iteration] <span class="op">=</span> np.<span class="bu">sum</span>(betahat)</span>
<span id="cb3-16"><a href="#cb3-16"></a>        err[lamb, iteration] <span class="op">=</span> y0 <span class="op">-</span> np.<span class="bu">sum</span>(betahat)</span>
<span id="cb3-17"><a href="#cb3-17"></a>        </span>
<span id="cb3-18"><a href="#cb3-18"></a>pred_err <span class="op">=</span> np.mean(err<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-19"><a href="#cb3-19"></a>pred_var <span class="op">=</span> np.var(yhat_0, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-20"><a href="#cb3-20"></a>pred_bias <span class="op">=</span> (np.mean(yhat_0, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> np.<span class="bu">sum</span>(beta))<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז הסימולציה שלנו כמעט זהה לסימולציה עם הרגרסיה ליניארית רגילה שעשינו, שם טיפלנו בקומפלקסיטי של המודל באמצעות הוספת עוד ועוד משתנים. ההבדל היחיד הוא בשתי השורות האלה:</p>
<p>ראשית, אנחנו לא משנים את p מספר המשתנים במודל, אלא את למדא, להיות בין כ30 כלומר ענישה חמורה כל כך שבגללה מקדמים רבים בוקטור יהיו קרובים מאוד לאפס והמודל יהיה מאוד לא מורכב, ובין 0 כלומר בלי ענישה בכלל, מה שיאפשר לוקטור בטא להשתולל ולתת מודל מורכב מאוד.</p>
<p>שנית, האומד שלנו לבטא הוא לפי הנוסחה של האומד רידג’, לא האומד של OLS, כשאנחנו מוסיפים למדא לאלכסון המטריצה X’X. וp מספר המשתנים נשאר קבוע 20, אנחנו משתמשים תמיד בכל המשתנים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">29</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), pred_err, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'prediction error'</span> )</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">29</span>, <span class="dv">0</span>], [sigma_sq, sigma_sq], color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">'$E(A^2)$:irreducible error'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">29</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), pred_bias, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$E(B^2)$:prediction bias squared'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">29</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), pred_var, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$E(C^2)$:prediction variance'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'regularization level ($\lambda$)'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'squared error'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error decomposition simulation'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img data-src="c14_predmodeling_theory_files/figure-revealjs/cell-8-output-1.png" width="527" height="451"></p>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כפי שאפשר לראות קיבלנו דפוס דומה. ככל שהענישה חמורה יותר המודל פשטני מדי והטעות חיזוי גבוהה, איפשהו באמצע נראה טעות חיזוי אופטימלית, ועם ענישה לא חמורה בכלל הביאס קטן מאוד אבל הוריאנס גם גדול מאוד ושוב טעות החיזוי עולה.</p>
<p>הדבר המדהים הוא, שאם תשוו את טעות החיזוי הכי טובה ברגרסיית רידג’ לטעות החיזוי הכי טובה ברגרסיית OLS בסימולציה הקודמת, תראו שברגרסיית רידג’ היא טובה יותר, היא מתחת ל-1500! יש הרבה דרכים להסביר את זה, ואני מקווה שזה יגרה אתכם להמשיך ללמוד, בשורה התחתונה הרבה פעמים הוספת פנאלטי קטן של רידג’ במיוחד בבעיות ממימד גבוה או בבעיות בהם p בכלל גדול מn, טעות החיזוי משתפרת מאוד.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="summary">Summary</h3>
<div class="fragment">
<p>We have discussed several types of modeling families:</p>
<ol type="1">
<li>Parametric traditional approaches like OLS and logistic regression</li>
<li>Local non-parametric approaches like k-NN and trees</li>
<li>Modern high dimensional approaches: RF, boosting, CNN</li>
</ol>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נסכם את החלק של מודלים לחיזוי. למדנו משפחות שונות של מודלים:</p>
<p>מודלים פרמטריים כמו רגרסיה ליניארית ולוגיסטית, מודלים ישנים יותר עמוסים בהנחות. מודלים א-פרמטריים לוקאליים שמתבססים על שכונות, כמו KNN ועצי החלטה. ומודלים חזקים במיוחד שמבוססים על איסוף של הרבה מודלים חלשים כמו רנדום ופורסט או על ארכיטקטורה חדשנית ומימוש יעיל, כמו רשתות נוירונים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="summary-ii">Summary (II)</h3>
<div class="fragment">
<p>When designing a specific predictive model we should consider the apsects we have discussed:</p>
<ol type="1">
<li>What probabilistic or parametric assumptions make sense?</li>
<li>What is the right model complexity given the amount of data we have?</li>
<li>We can control complexity and hence approximation-estimation tradeoff through number of parameters or regularization</li>
<li>Evaluation on independent data: What is the loss function to evaluate model performance? Use test set or cross validation?</li>
</ol>
</div>
<div class="fragment">
<p><strong>These universal considerations are largely common to all approaches</strong></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>והיום דיברנו באופן כללי יותר על אילו אספקטים כדאי לקחת בחשבון כשאנחנו רוצים לבחור בין מודלים.</p>
<p>אילו הנחות יש למודל, הסתברותיות או פרמטריות והאם נראה לנו שזה מתאים לבעיה שלנו.</p>
<p>מה רמת הקומפלקסיטי המתאימה למודל שלנו, ואיך לשלוט בה. האם יש מספר פרמטרים שצריך לכוונן, אחד או שאין בכלל, וכמה קל זה יהיה בהתחשב במאפיינים של הדאטא שלנו.</p>
<p>ודיברנו הרבה על איווליואציה של מודל: מה הלוס פאנקשן שמתאימה לנו, ואיזו אסטרטגיה להעריך אותה נבחר, טסט סט יחיד או קרוס ולידיישן.</p>
<p>כמו הרבה עקרונות בקורס שלנו, הדברים שלמדנו היום ילוו אתכם בכל מודל שתלמדו בהמשך, ואני מקווה שתמיד תקחו אותם בחשבון כשאתם מגיעים למדל נתונים במיוחד אם המטרה שלכם היא חיזוי איכותי ככל הניתן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<img src="../Intro2DS_logo_white.jpg" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://intro2ds2023.github.io/mooc/" target="_blank">Intro to Data Science</a></p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>