=== 1. חזרה על מודלים לחיזוי ===

ביחידה הזאת לא נלמד אף מודל חיזוי חדש, אלא נעסוק בתיאוריה של מודלים לחיזוי מעט יותר לעומק. הרציונל מאחורי הבחירה ללמוד תיאוריה לאחר מעשה, הוא שכעת יש לנו הרבה דוגמאות להמחשת התיאוריה.

:::

אז מה עשינו בחצי השני של הקורס?

דיברנו על הסטאפ הבסיסי שבו X הוא וקטור משתנים וY סקלאר שאנחנו מנסים למדל ולחזות, אם Y ממשי קראנו לזה רגרסיה ואם קטגוריאלי קלסיפיקציה.

הפרדיגמה הכללית שלנו היא להשתמש בדאטא נפרד ללמידה שמסומן בTR ודאטא אחר לחיזוי שמסומן בTE, ואמרנו שהרבה פעמים זה אומר בפועל לקחת את הדאטא שיש לנו ולחלק אותו בעצמנו לטריין וטסט. ראינו אינטואיטיבית למה שנעשה דבר כזה, מדגם הטריין לרוב יכול להגיע לאפס אחוז טעות, מה שנקרא אוברפיטינג, ואיכות המודל נשפטת על-פי מדגם הטסט. כאן טמונה בעיה שאולי נחשפתם אליה בתרגול: אם יש לי טסט סט אחד, ועל-פיו החלטתי לקחת לדוגמא עץ בעומק 5 במקום עץ בעומק 4 -- אם אני רוצה לשקף מה הביצועים הסופיים של המודל שלי, האם אני יכול לדווח על הביצועים של המודל עם הטסט סט שבו השתמשתי לבחירה בין מודלים? אני מקווה שזה נשמע לכם מעט בעייתי, נדבר על זה היום.

מבחינת שיטות המודלים הפרמטריים שלמדנו היו רגרסיה ליניארית ורגרסיה לוגיסטית. שיטות שאין להן פרמטרים שהן צריכות לאמוד ולכן ניתן לקרוא להן אפרמטריות, שיטות שמבוססות על הגדרת שכונות -- היו KNN ועצים.

למדנו גם על שיטות אנסמבל מבוססות עצים - רנדום פורסט ובוסטינג. ולמדנו על רשתות נוירונים מהבסיס, והנציג של למידה עמוקה או דיפ לרנינג שהצגנו היה רשתות קונבולוציה.

ננסה לחשוב על כל השיטות שלמדנו בצורה אחודה, באמצעות חשיבה רוחבית על כמה תכונות של מודל לחיזוי.

:::

תכונה אחת מעניינת היא המסובכות או הקומפלקסיטי של השיטות. אני בכוונה לא אומר סיבוכיות כי זה מושג אחר לחלוטין.

ראינו שלכל השיטות שלנו יש פרמטר אחד או יותר, כמו כפתור כזה, שניתן לסובב כדי לנוע בין מודל פשטני מאוד למודל מורכב מאוד אולי מורכב מדי.

ברגרסיה אפשר לחשוב על מספר המשתנים שנכנסים למודל, אפשר הרי לנסח מודל עם מספר מצומצם של משתנים, ואפשר להוסיף עוד משתנים או אפילו להנדס עוד ועוד משתנים למודל.

בKNN יש לנו את מספר השכנים K שאפשר לשנות, בעצים יש את עומק העץ, פרמטר הmax_depth, אבל אמרנו שאפשר לחשוב גם על איזשהו סף לירידה בRSS בכל פיצול. אם הסף יהיה נמוך אני אמשיך לפצל ואגדל עצים מורכבים יותר ויותר, ואם יהיה גבוה יהיו לי מעט פיצולים איכותיים אבל עץ שטוח ולא מורכב.

וברשתות יש הרבה היפרמטרים שצריך לכוונן ויכולים לעשות את הרשת גמישה יותר ויותר, לדוגמא מספר הנוירונים ברשת.

המשותף כאמור לכל אלה, ככל שאני מסובב את הכפתור אני מקבל מודל מורכב ועשיר יותר לתיאור היחס בין המשתנים המסבירים למשתנה התלוי.

:::

הדפוס הדומה לכל השיטות שראינו: על ציר הY טעות החיזוי, למשל RMSE, על ציר הX רמת המורכבות של המודל. וככל שהמודל מורכב וגמיש יותר שגיאת הטריין בכחול יורדת, לפעמים עד לאפס, ואילו לשגיאת הטסט יש איזשהו אופטימום.

לא הוכחנו תיאורטית למה זה קורה אלא רק נתנו אינטואיציה, המודל עם הקומפלקסיטי האופטימלי גמיש מספיק כדי לבטא יחסים מורכבים אמיתיים בין X לY אבל לא גמיש עד כדי כך שהוא מתאים את עצמו לכל רעש קטן במדגם הלמידה ומביא לאוברפיטינג.

=== 2. טעות קירוב וחיזוי ===

ננסה כעת לשיים את שתי התופעות שאנחנו רואים. נתחיל בעוד קצת אינטואיציה ונגיע אל המושגים המתמטיים פרופר: ביאס, וריאנס, והטריידאוף ביניהן.

:::

אנחנו נקרא אפרוקיסמיישן ארור, לטעות שמבטאת עד כמה המודל שלנו מסוגל באמת לתפוס את הקשר האמיתי בין X לY. לדוגמא, בדוגמא עם הסיכוי לחלות במחלת לב, אם הוא מושפע משלושה משתנים, גיל, היסטוריה משפחתית ומשקל, אבל אני מכניס למודל רק שני משתנים, גיל ומשקל, הטעות של המודל -- האפרוקסימיישן ארור -- תנבע מזה שהוא בכלל לא רואה את משתנה ההיסטוריה המשפחתית, הוא לא מדבר בשפה הנכונה לקלוט את התלות האמיתית של Y בX, חסרים לו משתנים, הוא לא מורכב מספיק, הוא דל. כלומר מודל שתהיה לו אפרוקסימיישן ארור גבוהה הוא מודל עם קומפלקסיטי נמוך -- מעט מדי משתנים או עץ לא עמוק מספיק.

אסטימיישן ארור, מבטא עד כמה יש לנו מספיק אינפורמציה ממדגם הלמידה כדי לחזות על נתונים חדשים, כדי להתאים את המודל שלנו. בדוגמא של מחלות לב, נניח ואנחנו כוללים במודל בדיוק את שלושת המשתנים שאנחנו צריכים, המודל שלנו עשיר בדיוק כדי לבטא את התלות של Y בX, ולא תהיה לו אפרוקסימיישן ארור. אבל מה אם אגיד לכם שאני לומד משתי תצפיות בלבד? ברור שאין לי שום יכולת לאמן את המודל הזה, והטעות שתהיה לי לא תהיה מפני שהמודל לא נכון, המודל מצוין, פשוט אין לי מספיק תצפיות לקבל הערכה מדויקת של הפרמטרים, אם מדברים על רגרסיה. ואם אני אשתמש בדאטא אחר סביר שאגיע לפרמטרים אחרים, ההחלטה שלי רועשת.

מצד שני, אולי לא שתי תצפיות כי זה מקרה קיצוני, אבל מעט תצפיות כן יספיקו, כדי להתאים מודל פשוט יותר, למשל מודל רגרסיה עם משתנה אחד, אין צורך במיליון תצפיות כדי לתאר קו ישר. והאסטימיישן ארור, כמה אנחנו מדייקים בשערוך הפרמטרים שלנו, תהיה קטנה. אם אשתמש בדאטא אחר סביר שאגיע לאותם פרמטרים, שיפוע וחותך במקרה של קו ישר.

ואנחנו מגיעים לאיזשהו טריידאוף: מודל פשוט מדי עם קומפלקסיטי נמוך, מדבר בשפה "דלה", יהיה לו אפרוקסימיישן ארור גבוהה אבל אסטימיישן ארור נמוכה כי יהיו לו מעט פרמטרים שנשערך היטב. מודל מורכב ועמוק, יהיה לו אפרוקסימיישן ארור נמוכה כי הוא יכול לבטא יחסים נורא מסובכים, אבל אסטימיישן ארור גבוהה כי נצטרך המון תצפיות לדייק בשערוך כל הפרמטרים שלו.

מה מעניין אותנו? הטסט ארור, שגיאת החיזוי על נתונים שהמודל לא ראה. הציור שלפנינו מפרק אותה לשני מרכיבים (תיכף נראה שזה עוד לא מדויק): האפרוקסימיישן והאסטימיישן ארורז. אנחנו רואים את הצורת U המוכרת לנו ומבינים סוף סוף, אם כי בנפנוף ידיים, איך היא נוצרת. ככל שהמודל מורכב יותר טעות אפרוקסימיישן יורדת. אבל טעות האסטימיישן עולה, אנחנו מחפשים במרחב גדול הרבה יותר של מודלים וצריכים הרבה יותר דאטא כדי לשערך אותם. והצורת U קורית כי יש בחיבור של הטעויות האלו איזו נקודה אופטימלית של מורכבות המודל, שבה שתיהן כמה שיותר נמוכות.

ושאלה אחרונה לראות שהבנו: אם אגדיל עוד ועוד את הדאטא שיש לי לצורך למידה, על איזו מסוג הטעויות אשפיע, אוריד אותה, אפרוקסימיישן או אסטימיישן? אסטימיישן כמובן. ככל שיש לי יותר דאטא אני יכול ללמוד מודלים מורכבים יותר מדויק יותר. אין לזה השפעה על האפרוקסימיישן ארור, שהרי גם עם שתי תצפיות בלבד ניתן לתאר מודל מורכב עם המון משתנים. הבעיה תהיה באסטימיישן שלו.

=== 3. הטריידאוף של ביאס וריאנס ===

ננסה לפרמל את המושגים שאנחנו מבינים אינטואיטיבית, ניתן להם את השמות ביאס ווריאנס, ולאחר מכן נדגים עם סימולציה.

:::

נתמקד ברגרסיה, שם אנחנו מניחים שY הוא איזושהי פונקציה f של X ועוד רעש עם תוחלת אפס ושונות סיגמא בריבוע, לאו דוקא מהתפלגות נורמלית. מהו f אם ככה, אם התוחלת של אפסילון היא אפס? f הוא התוחלת המותנית של Y בהינתן הנתונים בX. הערך הנמדד של Y הוא לא בדיוק הערך הצפוי שלו, התוחלת, אלא התוחלת ועוד איזשהו רעש.

כעת אנחנו לוקחים טריינינג דאטא TR וממנו לומדים את f. אנחנו יכולים עכשיו לבדוק את הביצועים של המודל על מדגם הטסט, אבל אני רוצה שנחשוב על כל התהליך הזה של דגימת נתונים, בניית מודל f, למידה שלו מהנתונים - כעל תהליך אקראי. ולהסתכל על התוחלת של הלוס שלנו, לדוגמא ברגרסיה השגיאה הריבועית, על פני ביצוע התהליך הזה הרבה פעמים, אם היה לי טריינינג דאטא קצת שונה כל פעם. רוצה לומר גם f_hat המודל שבנינו הוא משתנה מקרי שמבוסס על מקריות מדגם הלמידה שלנו!

אז תגיע תצפית חדשה X0 וכמו שאמרנו נסתכל על תוחלת הלוס שלה, כאן השגיאה בין Y0 האמיתי שלה, למודל שבנינו f_hat של X0. בהקשר לקו הירוק של פרדיקשן ארור שראינו בשקף הקודם אנחנו בעצם מסתכלים על נקודה בו ועושים לה תוחלת, או מסתכלים על ההתנהגות האופיינית שלה, על פני הרבה מדגמי למידה.

כעת אני רושם את ההפרש כסכום של כמה אלמנטים. כל מה שאני עושה זה לחסר ולהוסיף את f של X0, שהוא התלות האמיתית של Y0 בX0, התוחלת המותנית. וגם מחסר ומוסיף את התוחלת של f_hat. שוב, למה זה דבר שקיים לנו? כי f_hat הוא בעצמו מעין משתנה מקרי שכאילו נדגם מתוך הרבה מודלים סופיים שנובעים ממדגמי למידה שונים.

כעת אנחנו פשוט שמים סוגריים ומסמנים אלמנט A, אלמנט B ואלמנט C. נרצה לפתח את הריבוע ולראות אכן ששגיאת החיזוי שלנו בתוחלת מתפרקת לביטויים מעניינים ולתת להם את השם המתמטי שלהם.

:::

אילו מהגורמים תלויים באקראיות מדגם הלמידה?

הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס כלומר כן משתנה מקרי אבל לא תלוי במדגם הלמידה.

הגורם B - נראה בהתחלה שתלוי במדגם הלמידה כי יש בו את המודל הנאמד f_hat, אבל יש כאן תוחלת, כלומר זה קבוע. כך שגורם זה הוא מספר, לא משתנה מקרי, ומה היינו מצפים שיהיה המספר הזה? אפס. אנחנו מקווים שהמודל שלנו עשיר מספיק שבסופו של דבר בתוחלת הוא קירוב טוב ליחס האמיתי f.

הגורם C - כאן בעצם יש את הגורם שתלוי במדגם הלמידה - המרחק של f_hat המודל שלמדנו מהתוחלת שלו. אם אני מעלה את זה בריבוע ולוקח תוחלת מה זה? שונות! שונות המודל f_hat.

כך שעוד לפני שהוכחנו, אתם כבר יכולים להבין שגורם A הוא איזושהי טעות שאין לי הרבה מה לעשות לגביה. גורם B הוא בעצם האפרוקסימיישן ארור, כמה המודל שלנו בתוחלת עשיר מספיק כדי לבטא את הקשר האמיתי בין X לY. וגורם C הוא האסטימיישן ארור, האם יש לי מספיק דאטא כדי שאם אחזור על התהליך הזה עם דאטא קצת אחר אקבל מודל דומה, כלומר כמה קטנה השונות של המודל עצמו.

:::

כעת כשאני מעלה את סכום שלושת הגורמים בריבוע אני מקבל את כל אחד מהם בריבוע ועוד 2 כפול מכפלה של כל זוג. כשאני לוקח תוחלת, אני טוען שאנחנו נשארים עם הביטוי שלפנינו. מדוע? נסתכל שוב על הגורמים שקיבלנו, בריבוע:

התוחלת של A בריבוע היא התוחלת של אפסילון בריבוע, כלומר היא השונות של אפסילון, שהיא סיגמא בריבוע. אנחנו קוראים לזה irreducible error, זה הרעש הקיים בטבע, שגם תחת מודל מדויק לא נצליח להפחית. אפסילון לא תלוי בדאטא שלנו, בהגדרה אין לנו דאטא לחזות אותו.

הגורם השני -- B בריבוע -- הוא כאמור קבוע, לא משתנה מקרי, והוא מבטא את האפרוקסימיישן ארור, עד כמה המודל שלנו בתוחלת עשיר מספיק וקרוב לf האמיתית. היינו רוצים שזה יהיה אפס, שלא תהיה הטיה בילט אין במודל שלנו. אבל אולי למשל לא הכנסנו את כל המשתנים שצריך, ויש הטיה, אז זאת מעין הטיה בריבוע, ואכן אנחנו קוראים לזה squared bias.

הגורם השלישי -- C בריבוע -- הוא כמו שאמרנו הvariance של החיזוי בנקודה X0, וזה מדד לאסטימיישן ארור. מודל עם שונות נמוכה, גם אם אקח דאטא קצת אחר אקבל חיזוי מאוד דומה, האמידה תהיה יציבה. מודל עם שונות גבוהה -- זוכרים את עצי ההחלטה? -- אם אקח דאטא קצת אחר אקבל חיזוי שונה, האמידה לא יציבה. ואמרנו שמתכון לטיפול בטעות כזאת יכול להיות למשל להגדיל את מדגם הלמידה, כאן הוא בא לידי ביטוי. איפה ראינו את זה כבר, מתמטית? כשדיברנו על ממוצע המדגם המקרי, שהשונות שלו היא סיגמא בריבוע חלקי n, וככל שn גדול יותר ככה היא תקטן. הרי גם בממוצע המדגם אפשר לראות עם קצת מאמץ מודל חיזוי פשוט.

מה נשאר לנו? להראות שכל שאר הביטויים של מכפלות הם אפס.

התוחלת של הגורם A, היא התוחלת של אפסילון, היא אפס. אז כל הביטוי של B כפול התוחלת של A הוא אפס.

התוחלת של C, היא התוחלת של f_hat פחות התוחלת של f_hat, כלומר גם היא 0 וכל הביטוי של B כפול התוחלת של C הוא אפס.

התוחלת של מכפלת A ו-C היא גם כן אפס. כי A ו-C הם משתנים מקריים בלתי תלויים לכן התוחלת של המכפלה שלהם היא מכפלת התוחלות והתוחלת של כל אחד מהם היא אפס.

:::

כך שאנחנו רואים ששגיאת החיזוי שלנו בשורה התחתונה, על דאטא שהמודל לא ראה, היא סכום הגורמים הריבועיים: שגיאת רעש טבעי שאינה תלויה בדאטא או במודל. שגיאת הטייה ריבועית, שתלויה במודל אבל לא בדאטא. ושגיאת שונות המודל שתלויה גם במודל וגם בדאטא.

ואם נחזור לתמונה שציירנו, מה לא מדויק בה?

הקו הירוק צריך להיות סכום של שלושה קווים, שגיאת ההטייה הריבועית (אפרוקסימיישן), שגיאת השונות של המודל (אסטימיישן) ושגיאת הirreducible error, שלא תלויה במורכבות המודל, כלומר איך היא תראה? קו ישר.

:::

באופן כללי אנחנו רואים שהמתמטיקה מסתדרת עם האינטואיציה שלנו, ואפשר ממש לחשב את הטעויות האלה ולהראות שככל שהמודל מורכב יותר הביאס או אפרוקסימיישן יורד, והוריאנס או האסטימיישן עולה.

לדוגמא ברגרסיה ליניארית, ככל שנוסיף עוד משתנים המודל יהיה מורכב ומדויק יותר, ההטיה תרד ותרד. ומצד שני השונות תגדל ותגדל, האומדים שלנו ייהפכו יותר ויותר לא מדויקים ונצטרך יותר ויותר דאטא כדי שזה לא יתדרדר.

באופן דומה אפשר לחשוב על KNN רק ששם הכפתור הזה של מספר שכנים עובד הפוך -- ככל שיש פחות שכנים המודל נורא מורכב וספציפי והביאס ירד, אבל גם השונות תעלה, אנחנו תולים את מבטחנו במעט מאוד שכנים שזאת החלטה מאוד רועשת, ספציפית מאוד.

:::

נראה כעת סימולציה שממחישה את  הדפוס הזה יפה עם מודל ליניארי. יש לנו וקטור משתנים מסבירים X בגודל P = 20. והמקדמים שלו לפי הנוסחה הזאת, שורש 10, שורש 9.5 וכולי, כלומר הם הולכים וקטנים עד שורש חצי. המקדמים הראשונים הם הכי "חשובים, משפיעים" על Y והאחרונים פחות. זאת הf האמיתית שלנו, מודל ליניארי, אלה הם הבטאות.

כעת תעניין אותנו תצפית חדשה לחיזוי X0 שהיא בעצם 1 בכל המשתנים, אם נחשב את Y0 נראה שהוא צריך להיות 43.6 ועוד איזשהו רעש אפסילון שנדגם מהתפלגות הרעש, כאן היא נורמלית. ויהיה לנו תקציב של 50 תצפיות בלבד.

נייצר הרבה מדגמים עם היחס האמיתי הזה וכל פעם נבנה מודל f_hat אחר, על מדגמים שונים ועם מספר משתנים הולך וגדל, כלומר מורכב יותר ויותר. בצורה הזאת אנחנו יכולים לאמוד ממש אמפירית את שלושת הגדלים שחישבנו ולראות שהם מסתכמים בשגיאת החיזוי. מובן שאת הreducible error לא צריך לאמוד, היא ידועה בסימולציה.

:::

כאן אני מאתחל את מספר התצפיות, את סיגמא בריבוע, מספר האיטרציות והבטאות לפי הנוסחא. אני מאתחל גם מערך של החיזויים ל-Y0 לכל אחד מ-20 המודלים לכל 1000 האיטרציות. ואותו גודל של מערך לשגיאת החיזוי הסופית.

כעת נחזור על 20 המודלים 1000 פעם כדי שנוכל לחשב ממוצע עליהם, כמו תוחלת.

אנחנו מגרילים X עם 50 תצפיות ו-20 משתנים מהתפלגות נורמלית סטנדרטית. מחשבים את Y באמצעות המודל האמיתי f שהוא המודל הליניארי ומוסיפים את האפסילונים מהתפלגות נורמלית עם שונות 1000. לבסוף אנחנו דוגמים תצפית אמיתית שלא תהיה חלק מהמודל, Y0. למה Y0 שווה לסכום הבטאות ועוד רעש אפסילון? כי אמרנו שX0, התצפית החדשה, תהיה וקטור של אחדות.

עכשיו אני בונה 20 מודלים, מתחיל ממשתנה אחד ומוסיף כל פעם משתנה למודל מורכב יותר ויותר עד 20 משתנים במודל.

לכל מודל כזה מחשב את בטא-האט על מדגם הלמידה לפי הנוסחה המוכרת.

ואז מחשב את החיזוי לתצפית החדשה X0, זה כאמור סכום האומדנים לבטאות, הבטא-האט. ואת טעות החיזוי, Y0 האמיתית פחות Y0 החזוי.

עכשיו אני מחשב את תוחלת טעות החיזוי הריבועית. זה בעצם מיצוע של השגיאות בריבוע, לכל אחד מ20 המודלים על פני 1000 איטרציות. נחשב גם את ההטיה בריבוע, זה ממוצע החיזויים על פני 1000 איטרציות, פחות f0, התצפית האמיתית ללא רעש, וכל זה בריבוע. ולבסוף, התוחלת של C בריבוע זה בעצם שונות החיזוי כמו שראינו. איפה הגורם A בריבוע? אמרנו שלא צריך לאמוד אותו, זה סיגמא בריבוע שידועה בסימולציה, היא 1000.

:::

ובאמת, כשאנחנו מסרטטים את טעות החיזוי, הirreducible error, ההטיה הריבועית והשונות מתקבל הדפוס הצפוי.

הirreducible error היא כמובן קבוע, 1000, קו ישר. ההטיה בריבוע, הקו האדום, מתחיל מאוד גבוה ממודל פשטני מדי שכולל רק משתנה אחד ועד המודל הנכון עם 20 משתנים שיש לו הטיה אפס. אם אגב במודל רק 15 משתנים היו רלוונטיים, היינו מגיעים לאפס הזה מהר יותר, ב-15 משתנים. לבסוף שונות החיזוי, מתחילה מכמעט אפס עבור מודל פשטני עם משתנה אחד, 50 תצפיות מספיקות כדי להעריך את הפרמטר שלו במדויק. אבל השונות עולה ועולה ככל שהמודל מורכב יותר, 50 תצפיות פשוט לא מספיקות לשערך מודל עם 20 משתנים גם אם הוא נכון. שימו-לב שהיא עולה בצורה ליניארית, אפשר להוכיח למה זה תמיד קורה ברגרסיה ליניארית.

ומה עם שגיאת החיזוי הריבועית, בשורה התחתונה? היא אכן בעלת צורת U שראינו, רק שעכשיו אנחנו מבינים ממש מתמטית איך הU נוצר. ואפשר לראות שבקירוב היא שווה לסכום שלוש השגיאות הריבועיות האחרות.

שימו-לב שוב, זה נכון שהמודל עם 20 משתנים הוא הוא הנכון ביותר. אבל עם תקציב של 50 תצפיות, המסר הוא שניתן לשערך היטב מודל של כ12, משתנים בלבד.

:::

אפשר לסכם עם הטבלה הזאת:

מודל עם קומפלקסיטי נמוך -- בא עם אסטימיישן ארור נמוך אבל אפרוקיסמיישן ארור גבוה.

מודל עם קומפלקסיטי גבוה -- בא עם אסטימיישן ארור גבוה אבל אפרוקסימיישן ארור נמוך.

ומהו הכפתור העיקרי שנותן לנו קומפלקסיטי נמוך וגבוה? ברגרסיה מספר המשתנים, מעט זה לואו קומפלקסיטי, הרבה זה היי קומפלקסיטי. בKNN הרבה שכנים זה לואו קומפלקסיטי ומעט זה היי קומפלקסיטי. בעצים עץ שטוח כלומר מעט שכונות זה לואו, עץ עמוק עם הרבה שכונות זה היי. וברשתות מעט שכבות עם מעט נוירונים זה לואו, והרבה פרמטרים זה היי.

=== 4. קרוס-ולידציה ===

לנושא הבא ודאי נחשפתם כבר בתרגול אבל הגיע הזמן להציג אותו באופן רשמי - cross validation.

:::

עד כה בהרצאה לפחות חילקנו את הדאטה שלנו לשניים, 80 אחוז למדגם למידה ו20 אחוז למדגם טסט. בתחילת השיעור רמזנו שאם יש כפתור כזה של מודל קומפלקסיטי שצריך לבחור, כמו K בKNN, לא סביר להשתמש במדגם הטסט גם לבחור אותו וגם לדווח על שגיאת החיזוי הצפויה מהמודל. נכון יותר סטטיסטית לחלק את הדאטה לשלושה חלקים: מדגם למידה, נאמר 60 אחוז מהדאטא, מדגם ולידציה, נאמר 20 אחוז מהדאטא והוא ישמש לסיבוב הכפתור, בחירת ההיפרפרמטרים הרלוונטיים, ומדגם הטסט שישמש אותנו לאווליואציה סופית, מה הRMSE או הprecision הצפוי מהמודל, למשל.

אבל יש דרך יעילה יותר להשתמש בנתונים אם חושבים על זה.

:::

לדוגמא 10-פולד קרוס ולידיישן. נחלק את הדאטא לעשרה חלקים שווים באופן אקראי, אלה הם הfolds או "קפלים". ונחזור על הפרוצדורה הבאה 10 פעמים. נאמן את המודל על 90 אחוז מהדאטא, כלומר תשעה פולדים. ונעשה לו אבליואציה על 10 אחוז מהדאטא, כלומר על הפולד שלא השתמשנו בו לאימון. מובן שכל פולד ישמש בתורו כמדגם ההולדאאוט או טסט סט, כשהתשעה האחרים משמשים כמדגם הלמידה.

לבסוף כדי לדווח על הביצועים של המודל או כדי לבחור בקומפלקסיטי הראוי למודל, נסתכל על ממוצע טעות החיזוי שעל 100 אחוז מהדאטא, על פני כל הפולדים.

באופן זה אימנתי לא על 60 או 80 אחוז מהדאטא, אימנתי כל פעם על 90 אחוז מהדאטא, ומיצעתי את עקומת טעות החיזוי המוכרת לנו על פני מספר עותקים של הדאטא, כל פעם ראיתי מדגם אקראי אחר. וכמו בכל מיצוע שראינו עד כה, המשמעות היא שגם החלק הזה של בחירת הקומפלקסיטי עצמו, יהיה רועש פחות, השונות של התהליך הזה תקטן.

דבר אחרון שחשוב למדעני נתונים שעוסקים בפרקטיקה, אם הדגש של המודל הוא באמת על חיזוי, למשל בסביבת פרודקשן של אתר כלשהו, יש לאמן אותו פעם אחת אחרונה על 100 אחוז מהתצפיות עם רמת הקומפלקסיטי שנבחרה, הכפתור שסובבנו. כך שמודל סופי שאנחנו באמת עושים לו דיפלוימנט יאומן על כל הדאטא שהיה ברשותנו ולפיכך אמור רק להשתפר.

:::

נשאלת השאלה למה לחלק לעשרה פולדים? אפשר לחלק גם ל-n פולדים, כשn הוא גודל הדאטא. בדרך זו המודל יתאמן בכל פולד על 99.99 אחוז מהנתונים, כמעט 100 אחוז.

זה באמת מה שעומד מאחורי leave-one-out cross validation. כל מודל נבנה על n - 1 תצפיות ונבדק על התצפית האחת שנותרה.

אפשר לרשום את זה בצורה כזאת: L היא איזושהי פונקצית הפסד שלפיה אנחנו שופטים את ביצועי המודל, לדוגמא הפסד ריבועי, f_hat פחות i זה המודל של שאומן על כל התצפיות חוץ מi והחיזוי שלו על תצפית i. וטעות החיזוי הכללית שלנו תהיה סכום ההפסדים על פני n המודלים.

אם רוצים לכוונן גם איזשהו כפתור קומפלקסיטי, לדוגמא להחליט בין חמישה לעשרה שכנים, נצטרך להוסיף פה גם איזה אינדקס K, כלומר לעשות את זה לכל בחירה של K או פרמטר קומפלקסיטי כלשהו באופן כללי.

:::

מהם היתרונות של קרוס ולידיישן?

יתרון אחד הוא שימוש יעיל יותר בנתונים, אפשר לאמן מודל על 90 אחוז מהדאטא או n פחות 1, במקום 80 אחוז, כלומר מודלים טובים יותר.

יתרון שני הוא שכל הדאטא גם משתתף בטסט סט, כל פעם בפולד אחר, לכן האווליואציה עצמה עם שונות קטנה יותר וכוונון הכפתור שלנו יהיה יציב יותר, מושכל יותר.

מהם החסרונות של קרוס ולידיישן?

קודם כל חיסרון חישובי. אם מדובר במודל שלוקח זמן לאמן, נצטרך הרבה יותר זמן לאימון, הרבה יותר חישוב. אם אנחנו רוצים לכוונן את מספר המשתנים בין 1 ל100 למשל, זה גם מכפיל את הכמות הזאת פי 100. כאן כמובן אפשר לחשוב על מימוש חכם שימקבל את האימון, ובסיטואציות מסוימות אפשר בכלל להימנע מאימון של כל המודלים או אימון שלהם בצורה הדרגתית ויעילה יותר, אבל זה מחוץ לסקופ שלנו כאן.

החיסרון האחר הוא שבסופו של דבר המודל ייבחן פעם אחת, בפרודקשן, והטעות הממוצעת הזאת אמנם על כל הנתונים אבל על פני כמה פולדים, אולי לא משקפת בדיוק את המבחן האמיתי של המודל. גם כאן אם הדאטא גדול מאוד אפשר לחשוב על שילוב הגישות, בכל זאת להשאיר קצת דאטא בצד לטסט סט סופי סופי שבאמת לא ניגע בו ונבחן את ביצועי המודל עליו כאילו היה באמת דאטא שמגיע בסביבת הפרודקשן.

=== 5. רגולריזציה ===

הנושא האחרון בו ניגע ביחידה הזאת שהיא יותר תיאורטית, הוא רגולריזציה. ניקח למשל את המודל של רגרסיה ליניארית. כדי להפוך את המודל יותר ויותר מורכב אנחנו דיברנו על האפשרות של הוספת עוד ועוד משתנים למודל. רגולריזציה מאפשרת לשלוט בקומפלקסיטי של המודל בגישה אחרת.

:::

אז איך נשלוט בשונות המודל בדרך קצת יותר מחוכמת, בלי לשנות את מספר המשתנים במודל?

הרי מה אנחנו עושים כשאנחנו משנים את מספר המשתנים? אנחנו מרחיבים או מצמצמים את מרחב החיפוש של הפרמטרים שלנו. אם אנחנו משתמשים בשני משתמשים המרחב נורא קטן, אם אנחנו משתמשים במאה משתנים הוא כבר עצום. רגולריזציה, שולטת בגודל הזה של מרחב החיפוש בדרך אחרת -- על ידי הגבלת הנורמה של וקטור המקדמים. במקום לרדת מR^p לR ממימד קטן יותר, נישאר בR^p, אבל לא נרשה לוקטור בטא להיות בכל R^p. ספציפית נגדיר איזשהו כדור בR^p שרק שם מותר לוקטור בטא, למודל להיות, כך שלא ישתולל.

:::

מתמטית, אנחנו צריכים קודם כל לבחור נורמה. הנורמות המקובלות הן או L2, שמשמעותה סכום הבטאות בריבוע. או L1, שמשמעותה סכום הבטאות בערך מוחלט.

כשמדובר בנורמת L2 הפרוצדורה נקראת רידג' ריגרשן ועליה נרחיב, כשמדובר על נורמת L1 זה נקרא לאסו ריגרשן, עליה לא נרחיב.

ומה זה אומר להגביל את הנורמה? זה אומר במקום להביא למינימום את הRSS בלי שום אילוץ, להביא אותו למינימום תחת האילוץ שנורמת הL2 לא גדולה מדי, היא קטנה מאיזשהו פרמטר C. למי שזוכר מלימודי החדו"א שלו, זה ממש מציאת מינימום בתוך הכדור שסביב הראשית במרחב הפרמטרים הp מימדי הזה.

בעיה שקולה היא לקחת את הקריטריון למינימום ולהוסיף לאילוץ כופל לגראנז' למדא. וכשאנחנו כותבים את הקריטריון הסופי בצורה כזו זה אומר מציאת מינימום לRSS תוך ענישת הנורמה של וקטור המקדמים בטא. אם נשים למדא גדול מאוד, העונש יהיה כבד והמקדמים שלנו ייאלצו להיות קטנים, כלומר להתקרב לראשית. אם למדא יהיה קטן העונש יהיה קל יותר ונאפשר למקדמים להיות גדולים יותר. זאת בעיה קלה יותר לפתרון אבל הן שקולות.

ומה כל זה יועיל לנו? אפשר להוכיח שבדרך זו אנחנו מקטינים את השונות של וקטור המקדמים אבל לא נעשה את זה כאן. אני מעדיף שנחשוב על זה לפי המוטיבציה שהביאה אותנו לענישה הזאת של הנורמה -- אנחנו מקטינים את מרחב החיפוש של המקדמים ומגדילים אותו ובאופן הזה שולטים בקומפלקסיטי של המודל, עם פרמטר למדא במקום p.

:::

מכל מקום, אם בחרנו במטריקת L2 מסתבר שעדיין יש פתרון סגור לרגרסית רידג'. הקריטריון שלנו הוא כעת penalized RSS או PRSS, השאריות בריבוע ועוד עונש על הנורמה הריבועית. נשים לב שזו עדיין פונקציה קוואדרטית של וקטור בטא לכן יהיה לנו פתרון. כמו שעשינו ברגרסיה אפשר לגזור את הPRSS לפי בטא, לקבל את הביטוי שמופיע לנו כאן (בעצם אותו ביטוי שקיבלנו ברגרסיה ליניארית ועוד נגזרת הענישה), להשוות לאפס, ולחלץ לבטא.

יתקבל הביטוי שלפנינו, X'X ועוד מטריצה אלכסון עם למדא על האלכסון, כל זה בהופכי כפול X טרנספוז Y.

מה ההבדל בין פתרון רידג' לבטא לפתרון OLS שאנחנו מכירים לבטא? כאן נוסף עוד איזשהו קבוע קטן לאלכסון של המטריצה X'X, עיבינו קצת את האלכסון, ואפשר לחשוב שהוא נראה עכשיו בולט יותר כמו רכס, לכן השם ridge regression.

מלבד זאת אפשר כאמור להראות שאנחנו מקטינים או מייצבים את השונות של וקטור המקדמים של בטא באמצעות הוספת קבוע קטן למטריצה הזאת לפני ההפיכה שלה, פרוצדורה מקובלת מאוד באלגברה לטיפול במטריצות לא יציבות שאנחנו רוצים להפוך. מי שרוצה עוד פיתוחים מתמטיים על פתרון רידג' מוזמן לקחת קורסים מתקדמים יותר בנושא. כרגע כהרגלנו בקודש נסתפק בסימולציה שמראה את התועלת.

:::

אז הסימולציה שלנו כמעט זהה לסימולציה עם הרגרסיה ליניארית רגילה שעשינו, שם טיפלנו בקומפלקסיטי של המודל באמצעות הוספת עוד ועוד משתנים. ההבדל היחיד הוא בשתי השורות האלה:

ראשית, אנחנו לא משנים את p מספר המשתנים במודל, אלא את למדא, להיות בין כ30 כלומר ענישה חמורה כל כך שבגללה מקדמים רבים בוקטור יהיו קרובים מאוד לאפס והמודל יהיה מאוד לא מורכב, ובין 0 כלומר בלי ענישה בכלל, מה שיאפשר לוקטור בטא להשתולל ולתת מודל מורכב מאוד.

שנית, האומד שלנו לבטא הוא לפי הנוסחה של האומד רידג', לא האומד של OLS, כשאנחנו מוסיפים למדא לאלכסון המטריצה X'X. וp מספר המשתנים נשאר קבוע 20, אנחנו משתמשים תמיד בכל המשתנים.

:::

כפי שאפשר לראות קיבלנו דפוס דומה. ככל שהענישה חמורה יותר המודל פשטני מדי והטעות חיזוי גבוהה, איפשהו באמצע נראה טעות חיזוי אופטימלית, ועם ענישה לא חמורה בכלל הביאס קטן מאוד אבל הוריאנס גם גדול מאוד ושוב טעות החיזוי עולה.

הדבר המדהים הוא, שאם תשוו את טעות החיזוי הכי טובה ברגרסיית רידג' לטעות החיזוי הכי טובה ברגרסיית OLS בסימולציה הקודמת, תראו שברגרסיית רידג' היא טובה יותר, היא מתחת ל-1500! יש הרבה דרכים להסביר את זה, ואני מקווה שזה יגרה אתכם להמשיך ללמוד, בשורה התחתונה הרבה פעמים הוספת פנאלטי קטן של רידג' במיוחד בבעיות ממימד גבוה או בבעיות בהם p בכלל גדול מn, טעות החיזוי משתפרת מאוד.

:::

נסכם את החלק של מודלים לחיזוי. למדנו משפחות שונות של מודלים:

מודלים פרמטריים כמו רגרסיה ליניארית ולוגיסטית, מודלים ישנים יותר עמוסים בהנחות. מודלים א-פרמטריים לוקאליים שמתבססים על שכונות, כמו KNN ועצי החלטה. ומודלים חזקים במיוחד שמבוססים על איסוף של הרבה מודלים חלשים כמו רנדום ופורסט או על ארכיטקטורה חדשנית ומימוש יעיל, כמו רשתות נוירונים.

:::

והיום דיברנו באופן כללי יותר על אילו אספקטים כדאי לקחת בחשבון כשאנחנו רוצים לבחור בין מודלים.

אילו הנחות יש למודל, הסתברותיות או פרמטריות והאם נראה לנו שזה מתאים לבעיה שלנו.

מה רמת הקומפלקסיטי המתאימה למודל שלנו, ואיך לשלוט בה. האם יש מספר פרמטרים שצריך לכוונן, אחד או שאין בכלל, וכמה קל זה יהיה בהתחשב במאפיינים של הדאטא שלנו.

ודיברנו הרבה על איווליואציה של מודל: מה הלוס פאנקשן שמתאימה לנו, ואיזו אסטרטגיה להעריך אותה נבחר, טסט סט יחיד או קרוס ולידיישן.

כמו הרבה עקרונות בקורס שלנו, הדברים שלמדנו היום ילוו אתכם בכל מודל שתלמדו בהמשך, ואני מקווה שתמיד תקחו אותם בחשבון כשאתם מגיעים למדל נתונים במיוחד אם המטרה שלכם היא חיזוי איכותי ככל הניתן.

:::
