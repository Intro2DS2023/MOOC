---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Unsupervised Learning: Cluster Analysis"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Unsupervised Learning: Cluster Analysis - Class 15

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Intro. to Unsupervised Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From Supervised to Unsupervised

- Recall: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$) and a scalar $y$

- Our goal is to build a model of the relationship between $x$ and $y$:
$$y \approx f(x)$$

- IID assumption: each pair $(x_i, y_i)$ is drawn indepednently from some distribution $P_{x,y}$

- A modeling approach takes $(X, y)$ as input and outputs a *prediction model* $\hat{f}(x)$

- In prediction: we get a new value $x_0$ and predict $\hat{y}_0 = \hat{f}(x_0)$. 

- How good is our prediction? We typically define a loss function $L(y,\hat{y})$ and the quality of the model is $\mathbb{E}_{x_0,y_0}(L(y_0, \hat{y}_0))$

::: {.fragment}
What if there is no $y$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Unsupervised Learning

- Now: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$)

- IID assumption: each observation $x_i$ is drawn indepednently from some distribution $P_{x}$

- Our goal is to *learn* distrubution $P_{x}$ (or properties of it)

- "without a supervisor"

::: {.incremental}
- Example: Clustering = Finding modes of $P_{x}$ with high density
    - If we do find them, maybe $P(X)$ can be represented by a mixture of simpler densities?
- This isn't new, is it?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### KDE as unsupervised learning

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

X = np.concatenate([
    np.random.normal(-2, size=100),
    np.random.normal(2, size=100),
    np.random.normal(7, size=1000)
])
sns.kdeplot(X, bw_adjust=1)
plt.ylabel('Density')
plt.xlabel('X')
plt.title('KDE as Clustering: Three modes in Px')
plt.show()
```

::: {.fragment}
Will typically work for $p \le 3$, above that: "curse of dimensionality"
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Cluster Analysis

Group a set of observations into subsets, clusters, s.t. those within each cluster are more closely related to one other than observations assigned to different clusters

::: {.fragment}
What for?

- EDA: interesting groups in the data
- Segmentation: customers, products, distribution centers location, software
- Hierarchy: diseases, evolution
- Factorization of distributions
- Feature engineering
:::

::: {.fragment}
Many, many algorithms:

- Partition clustering: K-means
- Hierarchical clustering: Agglomerative
- Density-based clustering: DBSCAN
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Example: Microarray Clustering

![](images/microarray_clustering.png)

[source](https://hastie.su.domains/ElemStatLearn/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Unsupervised Learning Main Drawback

- Unless there is "ground truth", no clear measure of success (as opposed to $\mathbb{E}_{x_0,y_0}(L(y_0, \hat{y}_0))$)

- Many times involves scrutinizing results and interpretation

- Not for the faint of heart

---

## K-means Clustering {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How to evaluate a partition?

- Assume $K$ clusters are given

- $C(i) = k$ is some function assigning cluster $k \in \{1, \dots, K\}$ to observation $i \in \{1, \dots, n\}$

- $d(x_i, x_j)$ is a distance metric for pair $i, j$, e.g. Euclidean

- We wish to minimize the extent to which observations assigned to the same cluster tend to be close to one another

::: {.incrmental}
- The "within cluster" scatter/loss:
$$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} d(x_i, x_j)$$

- equivalent to maximizing $B(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) \neq k} d(x_i, x_j)$

- Can we go over all possible $C(i)$ to find the global minimum?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Road to K-means

- Euclidean distance: $d(x_i, x_j) = \sum_{m=1}^p (x_{im} - x_{jm})^2 = ||x_i - x_j||^2$

- Can show that:
$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} ||x_i - x_j||^2 = \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - \bar{x}_k||^2$

- $\bar{x}_k \in \mathbb{R}^p$ being the mean in cluster $k$, and $n_k$ number of observations in cluster $k$

::: {.incremental}
- But for any set of observations $S$, which $m$ would minimize $\sum_{i \in S} ||x_i - m||^2$?

- Thus, the final goal of K-means:
$$$\min\limits_{C, m_1, \dots, m_K} \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - m_k||^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means

0. Start with initial guess for $m_1, \dots, m_K$

1. Assign each observation to the closest cluster mean. That is:
$$C(i) = \arg\min\limits_{k = 1\dots K} ||x_i - m_k||^2$$

2. Update means $m_1, \dots, m_K$. That is the centroids:
$$m_k = \frac{\sum_{C(i) = k}x_i}{n_k}$$

3. Repeat 1 and 2 until $C(i)$ doesn't change

::: {.incremental}
- Convergence is guaranteed (steps 1 and 2 can only reduce $W(C)$)

- Global optimum is NOT guaranteed

- Can try many different initial starting points
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means Demo: Initial Guess

```{python}
#| echo: false

from sklearn.datasets import make_blobs
from sklearn.metrics import pairwise_distances_argmin

X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)

rng = np.random.RandomState(42)
centers = [0, 4] + rng.randn(4, 2)

def draw_points(ax, c, factor=1):
    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',
               s=50 * factor, alpha=0.3)
    
def draw_centers(ax, centers, factor=1, alpha=1.0):
    ax.scatter(centers[:, 0], centers[:, 1],
               c=np.arange(4), cmap='viridis', s=200 * factor,
               alpha=alpha)
    ax.scatter(centers[:, 0], centers[:, 1],
               c='black', s=50 * factor, alpha=alpha)

def make_ax(fig, gs):
    ax = fig.add_subplot(gs)
    ax.xaxis.set_major_formatter(plt.NullFormatter())
    ax.yaxis.set_major_formatter(plt.NullFormatter())
    return ax
```

```{python}
#| echo: false

fig = plt.figure(figsize=(4.55, 5))
ax = fig.add_subplot()
draw_points(ax, 'gray', factor=2)
draw_centers(ax, centers, factor=2)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means Demo: Iteration 1

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means Demo: Iteration 2

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
# ax = fig.add_subplot()
# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means Demo: Iteration 3

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
# ax = fig.add_subplot()
# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means on Netflix

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
from sklearn.model_selection import train_test_split

ratings = pd.read_csv('../datasets/netflix/train_ratings_all.csv', header = None)
miss_cong = pd.read_csv('../datasets/netflix/train_y_rating.csv', header = None, names = ['score'])
movies = pd.read_csv('../datasets/netflix/movie_titles.csv', header = None, names = ['year', 'title'])

netflix_X = ratings.iloc[:, :14]
netflix_X.columns = movies['title'][:14]
netflix_Y = miss_cong.iloc[:, 0]

NE_Xtr, NE_Xte, NE_Ytr, NE_Yte = train_test_split(netflix_X, netflix_Y, test_size=0.2, random_state=42)
```

```{python}
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2)
_ = kmeans.fit(NE_Xtr)
```

What did we get?

```{python}
#| output-location: fragment
print(kmeans.cluster_centers_.shape)
```

```{python}
#| output-location: fragment
print(kmeans.labels_[:10])
```

```{python}
#| output-location: fragment
print(f'{kmeans.inertia_:.2f}')
```

Can easily "predict":

```{python}
#| output-location: fragment
test_labels = kmeans.predict(NE_Xte)
test_labels[:10]
```

---

### K-means on Netflix: the Centroids

```{python}
pd.DataFrame({'title': kmeans.feature_names_in_,
    'mean_score': NE_Xtr.mean(axis = 0),
    'm_1': kmeans.cluster_centers_[0],
    'm_2': kmeans.cluster_centers_[1]}).set_index('title').head(8).round(2)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### K-means on Netflix: similar to first PC!

```{python}
#| code-fold: true

from sklearn.decomposition import PCA

X_centered = NE_Xtr - NE_Xtr.mean(axis=0)
pca = PCA(n_components=2)
pca.fit(X_centered)

T = pca.transform(X_centered)

c_dict = {0:'Cluster1', 1:'Cluster2', 2:'Cluster3', 3:'Cluster4'}
clusters = np.vectorize(c_dict.get)(kmeans.labels_)
ax = sns.jointplot(x=T[:, 0], y=T[:, 1], hue=clusters, height=5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### KMeans on Netflix: higher $K$

```{python}
#| code-fold: true

kmeans = KMeans(n_clusters=4)
kmeans.fit(NE_Xtr)

clusters = np.vectorize(c_dict.get)(kmeans.labels_)
ax = sns.jointplot(x=T[:, 0], y=T[:, 1], hue=clusters, height=5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How to choose $K$?

```{python}
W_C = []
for K in range(2, 20):
    kmeans = KMeans(n_clusters=K)
    kmeans.fit(NE_Xtr)
    W_C.append(kmeans.inertia_)
```

```{python}
#| echo: false
plt.figure(figsize=(6, 4))
plt.plot(range(2, 20), W_C, '--bo')
plt.title('KMeans on Netflix: Within Clusters SS vs. K')
plt.ylabel('W(C)')
plt.xlabel('K')
plt.xticks([2,6,10,14,18])
plt.show()
```

::: {.fragment}
The "elbow" method won't always work, there are others.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---
