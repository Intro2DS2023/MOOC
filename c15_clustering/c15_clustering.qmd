---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Unsupervised Learning: Cluster Analysis"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Unsupervised Learning: Cluster Analysis - Class 15

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Intro. to Unsupervised Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נקדיש יחידה אחת ללמידה מסוג שונה. למידה בלתי מפוקחת, או unsupervised learning. מאחר שזה נושא חדש, בואו נדבר קצת על מה זה unsupervised learning ומה שונה ממנה לעומת הלמידה שעסקנו בה עד כה, שמסתבר שאפשר לקרוא לה supervised learning, כלומר למידה כן מפוקחת.
:::
:::

---

### From Supervised to Unsupervised

- Recall: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$) and a scalar $y$

- Our goal is to build a model of the relationship between $x$ and $y$:
$$y \approx f(x)$$

- IID assumption: each pair $(x_i, y_i)$ is drawn indepednently from some distribution $P_{x,y}$

- A modeling approach takes $(X, y)$ as input and outputs a *prediction model* $\hat{f}(x)$

- In prediction: we get a new value $x_0$ and predict $\hat{y}_0 = \hat{f}(x_0)$. 

- How good is our prediction? We typically define a loss function $L(y,\hat{y})$ and the quality of the model is $\mathbb{E}_{x_0,y_0}(L(y_0, \hat{y}_0))$

::: {.fragment}
What if there is no $y$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בלמידה מסוג supervised, יש לנו וקטור X של p משתנים, וסקלאר Y. המטרה היא למדל את Y כפונקציה f של X. אנחנו מניחים שזוגות התצפיות X, Y שלנו מגיעים בלתי תלויים מאיזושהי התפלגות משותפת Pxy, ובונים מודל לחיזוי באמצעות נתוני מדגם הלמידה X, Y, מודל שנקרא f_hat. כשתגיע תצפית חדשה לחיזוי X0 נפעיל עליה את המודל הנלמד f_hat וזה יהיה החיזוי שלנו עבורה. ואיך אנחנו מכמתים את הביצועים של המודל שלנו? באידאל באמצעות איזושהי פונקצית הפסד L בין תצפיות Y האמיתיות והחזויות, כשאנחנו לוקחים תוחלת על תצפיות שהמודל לא ראה. בפועל אנחנו לא יודעים את ההתפלגות של התצפיות שהמודל לא ראה ואנחנו לוקחים את הממוצע האמפירי על מדגם הטסט.

נשאלת השאלה, מה אם אין Y, המשתנה התלוי לחיזוי?
:::
:::

---

### Unsupervised Learning

- Now: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$)

- IID assumption: each observation $x_i$ is drawn indepednently from some distribution $P_{x}$

- Our goal is to *learn* distrubution $P_{x}$ (or properties of it)

- "without a supervisor"

::: {.incremental}
- Example: Clustering = Finding modes of $P_{x}$ with high density
    - If we do find them, maybe $P_{x}$ can be represented by a mixture of simpler densities?
- This isn't new, is it?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בלמידה לא מפוקחת יש לנו רק וקטור של משתנים X ממימד p. אנחנו עדיין מניחים שהתצפיות מגיעות בלתי תלויות מאיזו התפלגות לא-ידועה Px, והמטרה שלנו היא לא לאמוד איזושהי פונקציה או קשר אלא ממש את ההתפלגות הזאת, או תכונות שלה.

לדוגמא, ניתוח אשכולות או קלאסטרינג -- היינו רוצים ללמוד איזורים בהתפלגות עם צפיפות גבוהה, או השכיחים של P_x. אם נמצא שP_x מתחלקת בבירור לכמה איזורים כאלה למשל, אולי ניתן לייצג אותה בעזרתם, ואז זה יפשט אותה, במקום להיות למשל פונקציה מורכבת בהרבה מימדים נוכל לחלק אותה לצירוף של כמה פונקציות פשוטות יותר.

אבל זה לא ממש חדש נכון? למצוא שכיחים בפונקצית התפלגות, כבר עשינו את זה.
:::
:::

---

### KDE as unsupervised learning

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

X = np.concatenate([
    np.random.normal(-2, size=100),
    np.random.normal(2, size=100),
    np.random.normal(7, size=1000)
])
sns.kdeplot(X, bw_adjust=1)
plt.ylabel('Density')
plt.xlabel('X')
plt.title('KDE as Clustering: Three modes in Px')
plt.show()
```

::: {.fragment}
Will typically work for $p \le 3$, above that: "curse of dimensionality"
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
זה בדיוק מה שעשינו עם kernel density estimation או KDE, שבו קיבלנו מדגם של נתונים מX שהוא משתנה רציף, וכדי ממש לאמוד את פונקצית הצפיפות שלו עשינו מעין החלקה להיסטוגרמה שלו עם KDE. לדוגמא כאן אפשר לראות שKDE עונה בדיוק על המטרה שלנו בקלאסטרינג, הוא מוצא שאפשר לייצג את פונקצית ההתפלגות של X בעצם בעזרת צירוף של שלוש פונקציות פשוטות נאמר נורמליות, לכל אחת שכיח משלה. בפועל זה נראה שX מורכס משלוש קבוצות או אשכולות שונים, והתובנה הזאת יכולה להיות בעלת ערך לחוקר.

דיברנו על KDE עבור משתנה אחד. ניתן לחשוב על החלקה כזאת בדו-מימד, ואולי בתלת-מימד. מעבר לזה KDE כבר לא כל-כך מעשי בגלל קללת המימד שדיברנו עליה בהקשר של KNN ושיטות מבוססות שכנים. כדי להעריך נכון את הצפיפות על פני מרחב ממימד גבוה צריך הרבה מאוד תצפיות, ולכן הגישה הזאת של לאמוד את Px עבור יותר משניים-שלושה משתנים היא פשוט אל פרקטית, ואנחנו עוברים לתחום של clustering.
:::
:::

---

### Cluster Analysis

Group a set of observations into subsets, clusters, s.t. those within each cluster are more closely related to one another than observations assigned to different clusters

::: {.fragment}
What for?

- EDA, Feature Engineering: interesting groups in the data
- Segmentation: customers, products, distribution centers location, software
- Hierarchy: diseases, evolution
- Deduplication
- Anomaly Detection
:::

::: {.fragment}
Many, many algorithms:

- Partition clustering: K-means
- Hierarchical clustering: Agglomerative
- Density-based clustering: DBSCAN
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בואו נדבר בקלאסטרינג לא על צפיפות כמושג הסתברותי אלא כמושג אינטואיטיבי: אנחנו רוצים למצוא קלאסטרים, קבוצות, איזורים או גושים בדאטא שבהם התצפיות מאוד צפופות וקרובות אחת לשניה, לעומת הקרבה ביניהן לתצפיות בקלאסטרים אחרים.

למה שנרצה לעשות את זה? הנה רשימה חלקית:

קודם כל ראינו שקלאסטרינג משמש אותו באקספלורטורי דאטא אנליסיס, איזושהי אנליזה רכה לנתונים שמסייעת להבין אותם, למצוא קבוצות מעניינות בדאטא. את הקבוצות האלה או ההשתייכות לקבוצות האלה אפשר לנצל אחר-כך כפיצ'רים מעניינים בבניית מודלים לחיזוי.

מטרה אחת אפשר לתאר בגדול כסגמנטציה: אתר שמנסה לחלק את הגולשים שלו לכמה טיפוסים, כמה פרופילים. למשל כדי להקצות לכל פרופיל איש מכירות אחר שהפרופיל הזה יהיה המומחיות שלו. או אתר עם המון מוצרים כמו אמזון שרוצה לחסוך בעבודה הקשה של הרבה מומחים לאלקטרוניקה שהתפקיד שלהם הוא להגיד שפלאפון אייפון 5 ופלאפון אנדרואיד שייכים לאותה קטגוריה או מוצר, אולי אפשר לעשות את זה אוטומטית עם קלאסטרינג על פיצ'רים שמראים שאותם אנשים קונים את שני הפריטים האלה או שהם עשויים מאותם חומרים ועולים סכום דומה של כסף. אפשר לחשוב על חברת פיצה שנכנסת לעיר חדשה ושואלת את עצמה איפה למקום שלושה מרכזי הפצה שהיא מתכננת להקים, הכי משתלם למקם אותם באיזורים שונים שבכל אחד ה"צפיפות" בביקוש לפיצה יהיה גבוה. גם בתוכנה אנחנו עושים קלאסטרינג, במיוחד לתוכנות גדולות ומסורבלות שנוצרו לפני שנים ולאט לאט התפתחו ונוצר צורך לפרק אותן לכמה מודולים, גם בשביל יעילות וגם כדי לחלק את האחריות של אנשי התוכנה להמשיך לפתח ולעשות מיינטננס לכל מודול בנפרד.

בביולוגיה נהוג להשתמש הרבה בקלאסטרינג, למשל להסתכל על פיצ'רים של מחלות כמו תסמינים ולקבץ אותן לסוגים שונים, או לבנות היררכיה של מחלות, עץ.

דוגמא אחרת יכולות להיות דידופליקציה, אם נחזור לאתר שמוכר הרבה פריטים, וכל מוכר נותן כותרת אחרת לפריט שלו, הרבה פעמים לא מדובר במוצר שונה אלא אותו מוצר בדיוק וצריך למחוק במערכות מוצרים חדשים לכאורה שהמערכת כבר אמורה להכיר, דופליקיישנז. דוגמא אחרונה כאן היא זיהוי תצפיות חריגות או אאוטליירז, או בשם המקובל anomaly detection. אם כל האייפונים התקבצו לקלאסטר מסוים ואייפון מסוים לא שייך לקלאסטר, יכול להיות שהוא אנומליה. יכול להיות שבכלל לא מדובר באייפון, למשל הפריט הוא רק כיסוי לאייפון, שהמוכר החליט להכניס לקטגוריה של אייפונים כדי להגדיל את החשיפה שלו.

ויש המון סוגים של אלגוריתמים לקלאסטרינג. יש אלגוריתמים מסוג partition שמבוססים על חלוקה של המרחב לאיזורים שונים זה מזה ככל שניתן וצפופים בתוכם, דוגמא לזה אפשר לראות את KMeans. הרבה אלגוריתמים הם היררכיים, שזה אומר שהם מנסים ליצור חלוקה לקלאסטרים ממצב שכל תצפית היא קלאסטר בפני עצמה, ואנחנו לאט לאט מאחדים זוגות של תצפיות קרובות אחת לשניה, ועד למצב שהדאטא נמצא בקלאסטר אחד גדול. זהו אגלומורטיב קלאסטרינג. אלגוריתמים אחרים באמת חוזרים לרעיון שצריך לאמוד את הצפיפות הטבעית של הדאטא, לא להניח מראש כמה קלאסטרים יש בו ולא להניח שכל התצפיות מתחלקות אליהם, דוגמא לזה הוא הDBSCAN. נעסוק היום בKMeans וב-DBSCAN כמייצגים של המשפחות האלה וגם כי הם סופר-פופולריים בתעשייה.
:::
:::

---

### Example: Microarray Clustering

![](images/microarray_clustering.png)

[source](https://hastie.su.domains/ElemStatLearn/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דוגמא לה ניתן להשיג מקלאסטרינג מבחינת אקספלורטורי דאטא אנליסיס או EDA.

יש כאן בעמודות כמה עשרות דגימות של תאים של גידולי סרטן מסוגים שונים כמו סרטן השחלה או סרטן השד. ובשורות כ6800 גנים ורמת הביטוי שלהם בדגימה. ככל שצבע הגן ירוק יותר כך הוא "בא יותר לביטוי" בדגימה הזאת. התבצע קלאסטרינג מהסוג ההיררכי שדיברנו עליו, גם על השורות וגם על העמודות, כך שזוג שורות קרובות הונחו אחת ליד השניה וגם כל זוג עמודות, ואפשר לראות איך הצעד הזה מחלק יפה גם למשפחות של סוגי סרטן בעמודות וגם למשפחות של גנים שונים שקשורים או לא קשורים לסוגים שונים של סרטן.
:::
:::

---

### Unsupervised Learning Main Drawback

- Unless there is "ground truth", no clear measure of success (as opposed to $\mathbb{E}_{x_0,y_0}(L(y_0, \hat{y}_0))$)

- Many times involves scrutinizing results and interpretation

- Not for the faint of heart

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מילה אחרונה של אזהרה בכל הנוגע לתחום של unsupervised learning. אין בלמידה לא-מפוקחת קריטריון ברור להצלחה, בניגוד ללמידה מפוקחת שם יש לנו את ה"מפקח" את Y ואת הלוס L, שלאורם אנחנו מכווננים את המודל שלנו, אנחנו מודדים את עצמנו.

הרבה פעמים כמו בדוגמא בשקף הקודם ההצלחה של מודל קלאסטרינג מערבת הרבה בחינה ידנית ושיפוט של הלקוח הסופי. הרבה בערך. ברור שלפעמים יש ground truth, ואנחנו מצפים מהקלאסטרים להיות בחפיפה עם קבוצות ידועות באוכלוסיה או בהתפלגות. ברור גם שפתרון קלאסטרינג על אותם נתונים שהצליח למצוא קבוצות יותר צפופות בתוכן ויותר רחוקות אחת מהשניה - הוא פתרון טוב יותר.

אבל בשורה התחתונה יש הרבה יותר מקום לפרשנות, אין קריטריון מנצח - וכדאי להיות מוכנים מנטלית לזה כשניגשים לפרויקט קלאסטרינג.
:::
:::

---

## K-means Clustering {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נדבר כעת על אלגוריתם KMeans, אולי המוכר ביותר בתחום, וגם יעיל יחסית ועובד טוב עם נתונים ממימד גבוה.
:::
:::

---

### How to evaluate a partition?

- Assume $K$ clusters are given

- $C(i) = k$ is some function assigning cluster $k \in \{1, \dots, K\}$ to observation $i \in \{1, \dots, n\}$

- $d(x_i, x_j)$ is a distance metric for pair $i, j$, e.g. Euclidean

- We wish to minimize the extent to which observations assigned to the same cluster tend to be close to one another

::: {.incremental}
- The "within cluster" scatter/loss:
$$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} d(x_i, x_j)$$

- equivalent to maximizing $B(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) \neq k} d(x_i, x_j)$

- Can we go over all possible $C(i)$ to find the global minimum?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נניח שיש לנו חלוקה נתונה. paritition. איך אנחנו עושים איבליואציה של חלוקה כזאת על הנתונים שלנו.

נניח שאנחנו יודעים כבר את מספר הקלאסטרים בדאטא ומסמנים אותו בK, ונניח שיש לנו כבר כלל חלוקה C שמתאים לכל תצפית i בנתונים את הקלאסטר k שמתאים לה, מ1 עד K גדול.

נניח כעת שצפיפות של נתונים או עד כמה זוג תצפיות קרובות אחת לשניה נמדוד באמצעות איזושהי מטריקת מרחק d, לדוגמא נתחיל עם מרחק אוקלידי.

ואנחנו רוצים לבטא באמצעות איזושהי מטריקה את האינטואיציה שלנו שתצפיות באותו קלאסטר צריכות להיות צפופות ורחוקות מתצפיות בקלאסטרים אחרים.

הרבה אלגוריתמים מסתכלים על המטריקה הבאה, הפיזור within cluster שנסמן בW(C), והוא סכום על כל זוגות התצפיות ששייכות לקלאסטר k של המרחקים ביניהן, וסכום על כל הקלאסטרים, מוכפל פי חצי כי אנחנו סופרים כל זוג ככה פעמיים.

מאחר שהפיזור בין כל זוגות התצפיות בלי קשר לחלוקה לקלאסטרים נשאר זהה, אפשר להראות שלעשות מינימום לקריטריון שלנו אקוויולנטי ללעשות מקסימום לכמות המשלימה של between clusters סקאטר או לוס: סכום המרחקים בין כל זוגות התצפיות שנמצאות בקלאסטרים שונים. הרי סכום הכמות הזאת והכמות שלנו W(C) מסתכמם בפיזור כללי נאמר T(C).

אז יש לנו מטריקה לעשות לה מינימום. אבל אפילו עם K נתון, נאמר שאנחנו רוצים לחלק את הדאטא ל4 קלאסטרים. האם אנחנו יכולים לעבור על כל החלוקות C(i) האפשריות כדי להגיע למינימום גלובלי? ברור שלא. יש לזה נוסחה שלא מופיעה כאן, אפשר לחשב למשל שלעבור על כל האפשרויות של חלוקת 20 תצפיות ל5 קלאסטרים אנחנו כבר מדברים על כמעט 750 מיליארד אפשרויות!
:::
:::

---

### Road to K-means

- Euclidean distance: $d(x_i, x_j) = \sum_{m=1}^p (x_{im} - x_{jm})^2 = ||x_i - x_j||^2$

- Can show that:
$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} ||x_i - x_j||^2 = \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - \bar{x}_k||^2$

- $\bar{x}_k \in \mathbb{R}^p$ being the mean in cluster $k$, and $n_k$ number of observations in cluster $k$

::: {.incremental}
- But for any set of observations $S$, which $m$ would minimize $\sum_{i \in S} ||x_i - m||^2$?

- Thus, the final goal of K-means:
$$\min\limits_{C, m_1, \dots, m_K} \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - m_k||^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז Kmeans קודם כל מצמצם אותנו למרחק אוקלידי בלבד.

תחת מרחק אוקלידי, אפשר להראות שאפשר לרשום את הקריטריון שלנו בצורה פשוטה יותר: בכל קלאסטר סכום המרחקים מהתצפיות אל ממוצע הקלאסטר, להכפיל במספר התצפיות בקלאסטר n_k, ולסכום על כל הקלאסטרים.

אבל אנחנו יודעים כבר שאם נסתכל על קריטריון דומה, ונשאל מה הנקודה שמביאים למינימום את סכום המרחקים המרובעים ממנה -- נגיע לממוצע המדגם.

לכן נהוג לרשום את הקריטריון של Kmeans בצורה כוללת יותר: למצוא את החלוקה ואת הנקודות של קלאסטרים שיביאו למינימום את סכום המרחקים המרובעים בתוך כל קלאסטר, על פני כל הקלאסטרים. צורת הרישום הזאת מסייעת לנסח את האלגוריתם של Kmeans.
:::
:::

---

### K-means

0. Start with initial guess for $m_1, \dots, m_K$

1. Assign each observation to the closest cluster mean. That is:
$$C(i) = \arg\min\limits_{k = 1\dots K} ||x_i - m_k||^2$$

2. Update means $m_1, \dots, m_K$. That is the centroids:
$$m_k = \frac{\sum_{C(i) = k}x_i}{n_k}$$

3. Repeat 1 and 2 until $C(i)$ doesn't change

::: {.incremental}
- Convergence is guaranteed (steps 1 and 2 can only reduce $W(C)$)

- Global optimum is NOT guaranteed

- Can try many different initial starting points
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מהו האלגוריתם של Kmeans?

K נתון, ואנחנו מתחילים עם איזשהו ניחוש התחלתי עבור הממוצעים m1 עד mk.

כעת החלוקה C(i) של כל תצפית תהיה לפי הקלאסטר שהממוצע שלו הוא הקרוב אליה ביותר.

לאחר החלוקה נעדכן את הממוצעים, בכל קלאסטר ניקח את הממוצע של התצפיות ששייכות אליו.

ונחזור על צעדים 1 ו2 עד שהחלוקה לא משתנה או עד איזשהו קריטריון התכנסות, למשל אפשר לחשב את הלוס שלנו W(C) ולראות שהוא לא משתנה יותר מדי.

מה הבעיה הראשונה באלגוריתם? אמנם התכנסות מובטחת, הצעדים שלנו יכולים רק להפחית את W(C), או לא לשנות אותו. אבל אין הבטחה למינימום גלובלי ואנחנו מאוד תלויים בבחירה הראשונית של הממוצעים שיכולה להיות אקראית.

נהוג לכן בהרבה מימושים לבצע מספר פעמים Kmeans כל פעם מנקודת התחלה אקראית אחרת ולבחור את הפתרון עם הלוס המינימלי.
:::
:::

---

### K-means Demo: Initial Guess

```{python}
#| echo: false

# inspired by https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html

from sklearn.datasets import make_blobs
from sklearn.metrics import pairwise_distances_argmin

X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)

rng = np.random.RandomState(42)
centers = [0, 4] + rng.randn(4, 2)

def draw_points(ax, c, factor=1):
    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',
               s=50 * factor, alpha=0.3)
    
def draw_centers(ax, centers, factor=1, alpha=1.0):
    ax.scatter(centers[:, 0], centers[:, 1],
               c=np.arange(4), cmap='viridis', s=200 * factor,
               alpha=alpha)
    ax.scatter(centers[:, 0], centers[:, 1],
               c='black', s=50 * factor, alpha=alpha)

def make_ax(fig, gs):
    ax = fig.add_subplot(gs)
    ax.xaxis.set_major_formatter(plt.NullFormatter())
    ax.yaxis.set_major_formatter(plt.NullFormatter())
    return ax
```

```{python}
#| echo: false

fig = plt.figure(figsize=(4.55, 5))
ax = fig.add_subplot()
draw_points(ax, 'gray', factor=2)
draw_centers(ax, centers, factor=2)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמא שלפנינו ברור שיש 4 קלאסטרים, ואנחנו מתחילים עם 4 נקודות אקראיות כממוצעים, מאוד לא מתאימות לחלוקה.
:::
:::

---

### K-means Demo: Iteration 1

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בצעד 1 אנחנו מחלקים כל תצפית לקלאסטר עם הממוצע הכי קרוב אליה במרחק אוקלידי, כאן זה אומר לצבוע אותן ב4 צבעים שונים.

בצעד 2 אנחנו מעדכנים את הממוצעים, וכבר ניתן לראות איך כל ממוצע מייצג כבר איזור הרבה יותר צפוף באופן טבעי.
:::
:::

---

### K-means Demo: Iteration 2

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ושוב אנחנו מחלקים את התצפיות לפי הקרבה שלהן לממוצע החדש.

ושוב אנחנו מעדכנים את הממוצעים. כאן הם כבר זזים ממש מעט.
:::
:::

---

### K-means Demo: Iteration 3

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באיטרציה השלישית כבר בקושי אפשר לראות הבדל, הקלאסטרים כבר ברורים מאוד.
:::
:::

---

### K-means on Netflix

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
from sklearn.model_selection import train_test_split

ratings = pd.read_csv('../datasets/netflix/train_ratings_all.csv', header = None)
miss_cong = pd.read_csv('../datasets/netflix/train_y_rating.csv', header = None, names = ['score'])
movies = pd.read_csv('../datasets/netflix/movie_titles.csv', header = None, names = ['year', 'title'])

netflix_X = ratings.iloc[:, :14]
netflix_X.columns = movies['title'][:14]
netflix_Y = miss_cong.iloc[:, 0]

NE_Xtr, NE_Xte, NE_Ytr, NE_Yte = train_test_split(netflix_X, netflix_Y, test_size=0.2, random_state=42)
```

```{python}
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=0)
_ = kmeans.fit(NE_Xtr)
```

What did we get?

```{python}
#| output-location: fragment
print(kmeans.cluster_centers_.shape)
```

```{python}
#| output-location: fragment
print(kmeans.labels_[:10])
```

```{python}
#| output-location: fragment
print(f'{kmeans.inertia_:.2f}')
```

Can easily "predict":

```{python}
#| output-location: fragment
test_labels = kmeans.predict(NE_Xte)
test_labels[:10]
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה עכשיו איך מבצעים Kmeans על הנתונים של נטפליקס.

כאן אני עושה import מתוך מודול קלאסטר לקלאס Kmeans. אני מאתחל אותו ומבקש 2 קלאסטרים על הצופים שלנו שדירגו סרטים מ1 עד 5. כשאני קורא לfit על הטריינינג דאטא אז רץ האלגוריתם. מה קיבלנו?

בשדה cluster_centers קיבלנו את הממוצעים עצמם. בעצם מדובר במטריצה עם 2 שורות ו14 עמודות, כי ביקשנו שני קלאסטרים ויש לנו 14 סרטים בדאטא, כל ממוצע הוא וקטור באורך 14.

בשדה לייבלז יש את האינדיקטור שמתאר כל תצפית האם היא שייכת לקלאסטר הראשון או השני, או יותר אם היינו מבקשים יותר קלאסטרים.

בשדה inertia יש את קריטריון הW(C) שלנו.

ובאלגוריתם Kmeans באופן טבעי אפשר לחזות במרכאות לאיזה קלאסטר שייכת תצפית שלא ראינו ממדגם הטסט. אני אומר לחזות במרכאות כי זה בעצם יותר לשייך אותה. תצפית שלא ראינו שייכת לקלאסטר עם הממוצע הקרוב ביותר אליה.
:::
:::

---

### K-means on Netflix: the Centroids

```{python}
pd.DataFrame({'title': kmeans.feature_names_in_,
    'mean_score': NE_Xtr.mean(axis = 0),
    'm_1': kmeans.cluster_centers_[0],
    'm_2': kmeans.cluster_centers_[1]}).set_index('title').head(8).round(2)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עכשיו כפי שאמרנו קשה לדעת עם Kmeans במיוחד אם הדאטא ממימד גבוה אם הגענו לפתרון "איכותי" ומה משמעות הקלאסטרים, בלי דרכים יצירתיות לנסות להבין את זה, בצורה קצת ידנית.

כאן אני לוקח את שמונת הסרטים הראשונים, מדפיס את ממוצע הדירוגים שלהם, ולידם את ממוצע הקלאסטר הראשון ואת ממוצע הקלאסטר השני.

דבר ראשון שאני שם לב אליו זה שהציון הממוצע של כל סרט נמצא בין שני הקלאסטרים (להדגים). דבר שני הוא שבכל הסרטים הממוצע של הקלאסטר הראשון גבוה יותר מממוצע הקלאסטר השני. זה מזכיר לכם משהו? זה מזכיר מאוד את הPC הראשון של הנתונים האלה, שאמרנו שבעצם מדרג את כל הצופים על-פי כמה הם מסכימים עם הדירוג הכללי שהוא די גבוה, כלומר כמה הם נוטים לצאת ציונים גבוהים לעומת לתת ציונים נמוכים לכל הסרטים.
:::
:::

---

### K-means on Netflix: "discrete" first PC!

```{python}
#| code-fold: true

from sklearn.decomposition import PCA

X_centered = NE_Xtr - NE_Xtr.mean(axis=0)
pca = PCA(n_components=2)
pca.fit(X_centered)

T = pca.transform(X_centered)

c_dict = {0:'Cluster1', 1:'Cluster2', 2:'Cluster3', 3:'Cluster4'}
clusters = np.vectorize(c_dict.get)(kmeans.labels_)
ax = sns.jointplot(x=T[:, 0], y=T[:, 1], hue=clusters, height=5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אפשר לראות את זה אם נצבע את כל הצופים בשני צבעים שונים על-פי ההשתייכות שלהם לקלאסטרים, במרחב ההטלה שמצאנו בPCA, כשאנחנו עושים תרשים פיזור של PC1 מול PC2 שכבר ראינו.

בקלאסטר אחד יש את הצופים שנוטים לתת ציונים גבוהים, ובקלאסטר שני צופים שנוטים לתת ציונים נמוכים. במובן מסוים זה נראה כאילו הפתרון של Kmeans הוא איזושהי דיסקרטיזציה של הפתרון PCA, במקום ממשי על פני כל הכיוון של הPC הראשון קיבלנו חלוקה לשתי קטגוריות, גבוה ונמוך. ויש עבודות שמראות שזה כמובן לא מקרי, אפשר לראות Kmeans כדיסקרטיזציה של PCA ומזה לצאת לעוד תובנות.

אם הייתי מבקש שלושה קלאסטרים הייתי מקבל 3 קבוצות בכיוון הPC הראשון, נמוך בינוני, גבוה.
:::
:::

---

### K-means on Netflix: higher $K$

```{python}
#| code-fold: true

kmeans = KMeans(n_clusters=4)
kmeans.fit(NE_Xtr)

clusters = np.vectorize(c_dict.get)(kmeans.labels_)
ax = sns.jointplot(x=T[:, 0], y=T[:, 1], hue=clusters, height=5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ורק כשאני מבקש K = 4, אני מקבל ביטוי של הPC השני. עדיין שתי קבוצות גדולות של צופים שנוטים לדרג גבוה או נמוך, והקבוצה השלישית מתחלק על-פי הטעם שלהם בסרטים רומנטיים.
:::
:::

---

## K-means Issues {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כל זה מרתק וגם יכול לעבוד בסקאלות די גבוהות של תצפיות ושל משתנים, אבל Kmeans ידוע לשמצה במספר בעיות שיש לו, בואו נראה כמה.
:::
:::

---

### How to choose $K$?

```{python}
W_C = []
for K in range(2, 20):
    kmeans = KMeans(n_clusters=K)
    kmeans.fit(NE_Xtr)
    W_C.append(kmeans.inertia_)
```

```{python}
#| echo: false
plt.figure(figsize=(6, 4))
plt.plot(range(2, 20), W_C, '--bo')
plt.title('KMeans on Netflix: Within Clusters SS vs. K')
plt.ylabel('W(C)')
plt.xlabel('K')
plt.xticks([2,6,10,14,18])
plt.show()
```

::: {.fragment}
The "elbow" method won't always work, there are others.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדומה לשיטות שראינו בלמידה supervised יש לנו כאן מעין היפרפרמטר שהשיטה לא לומדת, אנחנו צריכים לתת לה אותו כאינפוט, וזה הK, מספר הקלאסטרים.

והקריטריון של Kmeans בעייתי כי ככל שנגדיל את מספר הקלאסטרים כך הוא יקטן. בקצה אפשר לחשוב על מצב שבו K = N, כל תצפית בקלאסטר משלה, היא גם הממוצע ולכן סכום המרחקים מהממוצע יהיה 0. נהוג לכן לבצע Kmeans עבור K שונים על מדגם למידה, ולנסות לראות בשיטת המרפק או האלבואו איפה יש ירידה חדה בקריטריון ככה שנראה שעברנו איזשהו סף של חלוקה טבעית, וזה הK שנבחר.

הבעיה שכמו בנתונים שלפנינו השיטה הזאת לא תמיד תעבוד, לא ברור מה הK המתאים לנתונים האלה מהקריטריון, והיא גם ידנית. יש שיטות אחרות עם קריטריונים מחוכמים יותר. יש גם פתרונות שמשווים את הקריטריון של החלוקה שלנו לקריטריון המושג עם חלוקה אקראית  או חלוקה שמניחה שהדאטא מתפלג בצורה אחיד על פני המרחב, ואז בוחרים בK שמביא להפרש הגדול ביותר, הK שמביא להפתעה הגדולה ביותר לעומת חלוקה אקראית. מי שרוצה לקרוא עוד יכול לקרוא על הgap sttaistic.
:::
:::

---

### Should you always standardize?

As with KNN, K-means would be higly influenced by a feature with high variance.

But what if *that* feature is important for clustering?

```{python}
#| code-fold: true
#| output-location: fragment

from sklearn.preprocessing import StandardScaler

n = 30
m1 = 2
m2 = 0
sig = np.eye(2)
rng = np.random.RandomState(4)
X1 = rng.multivariate_normal(mean=[-m1, m2], cov=sig, size=n)
X2 = rng.multivariate_normal(mean=[m1, m2], cov=sig, size=n)
X = np.concatenate([X1, X2], axis=0)

kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
axes[0].scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
axes[0].set_xlim((-6, 6))
axes[0].set_ylim((-6, 6))
axes[0].set_title('K-means without standardizing')
X_stan = StandardScaler().fit_transform(X)
kmeans_stan = KMeans(n_clusters=2, random_state=0).fit(X_stan)
axes[1].scatter(X_stan[:, 0], X_stan[:, 1], c=1-kmeans_stan.labels_, cmap='viridis')
axes[1].set_xlim((-6, 6))
axes[1].set_ylim((-6, 6))
axes[1].set_title('K-means with standardizing')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עניין אחר בKmeans הוא שבהגדרה מדובר במרחק אוקלידי. אי אפשר פשוט להשתמש בKmeans עם מרחק אחר. וזה אומר שבדומה לKNN הוא מושפע מאוד מתצפיות חריגות וממשתנים בסקאלות רחבות מאוד, הם ישפיעו הרבה יותר על הפתרון. אז בעקרון ההמלצה היא לעשות סטנדרטיזציה לנתונים, אבל חשוב להזכיר להביט בנתונים כמה שניתן, יכולים להיות מקרים קיצוניים שבהם הסטנדרטיזציה הזאת עלולה להזיק לאיכות הפתרון.

כאן למשל די ברור שיש לנו שני קלאסטרים בדאטא הצהוב והסגול כי ככה הם גם נוצרו. אבל הם נבדלים זה מזה הודות למשתנה שבציר הX, הם לא נבדלים זה מזה בכלל במשתנה שבציר הY. אבל למשתנה בציר הX יש פיזור גבוה הרבה יותר, ואכן, לאחר סטנדרטיזציה כדי שלמשתנים יהיה פיזור דומה, החלקה לשני קלאסטרים שהיתה כל כך ברורה מיטשטשטת, וKmeans מוצא פתרון שמתאים פחות לבעיה המקורית, ואין דרך לדעת את זה.
:::
:::

---

### K-means failures (I)

Prefers separable spherical clusters (Gaussian).

```{python}
#| code-fold: true

from sklearn.datasets import make_circles

X_circles, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)

kmeans = KMeans(n_clusters=2, random_state=0).fit(X_circles)
plt.scatter(X_circles[:, 0], X_circles[:, 1], c=kmeans.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
Kmeans אוהב דאטא שמחולק ל"גושים" אפשר להראות שהוא דומה מאוד לאלגוריתם שמנסים לחלק את הדאטא לצפיפויות של מספר התפלגויות נורמליות, עם פיזור סימטרי כזה בכל הכיוונים סביב הממוצע.

כשהדאטא נראה כמו כמו הדאטא שלנו, שברור שיש שני קלאסטרים אבל הם לא spherical, הם באים בדפוס הרבה יותר מתוחכמים של טבעות -- Kmeans נכשל לחלוטין.
:::
:::

---

### K-means failures (II)

Always specify $K$.

```{python}
#| code-fold: true

X_square = np.random.rand(1000, 2)
kmeans = KMeans(n_clusters=3, random_state=0).fit(X_square)
plt.scatter(X_square[:, 0], X_square[:, 1], c=kmeans.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עניין אחר שתמיד צריך להגיד מהו K, ותמיד נקבל חלוקה לK קלאסטרים, בלי שום מטריקה שתגיד לנו אם החלוקה עושה שכל או לא. כאן לדוגמא יש ריבוע עם המון תצפיות בהתפלגות אחידה וKmeans פשוט מחזיר 3 איזורים איך שבא לא, הוא לא מגיע עם אזהרה שאומרת שאין חלוקה "הגיונית" לשלושה קלאסטרים לדאטא כאן.
:::
:::

---

### K-means failures (III)

No concept of outliers.

```{python}
#| code-fold: true

centers = np.eye(2) * 5
X_out, _ = make_blobs(n_samples=100, centers=centers, random_state=0)
X_out = np.concatenate([X_out, np.array([[10, 10]])], axis=0)

kmeans = KMeans(n_clusters=2, random_state=0).fit(X_out)
plt.scatter(X_out[:, 0], X_out[:, 1], c=1-kmeans.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בהמשך לדוגמא הקודמת לכל תצפית גם חייב להיות סיווג בKmeans. אין לו קונספט כזה של outliers, ואם לא מבצעים עוד איזשהו עיבוד פוסט-הוק ולוקחים א הסיווג של כל תצפית כמובן מאליו, אפשר לקבל מצב כמו כאן שהתצפית החריגה שייכת לקבוצה הסגולה למרות שברור שהיא לא.
:::
:::


---

### K-means failures (IV)

Bad with unequal densities, unequal cluster sizes.

```{python}
#| code-fold: true

X_varied, y = make_blobs(n_samples=[300, 150, 300], cluster_std=[1.0, 2.5, 0.5], random_state=170)

kmeans = KMeans(n_clusters=3, random_state=170).fit(X_varied)
plt.scatter(X_varied[:, 0], X_varied[:, 1], c=kmeans.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן יש דוגמא שמראה גם קלאסטרים בגודל שנה וגם קלאסטרים עם רמת צפיפות שונה, והבעייתיות שבפתרון של Kmeans. בקלאסטר האמצעי יש גם פי 2 פחות תצפיות מהקלאסטרים האחרים וגם הפיזור שלו גדול הרבה יותר. ואפשר לראות איך הקלאסטרים עם הרבה תצפיות וצפופים מושכים אליהם תצפיות שאולי היינו מסווגים כשייכות לקלאסטר הפנימי אם היינו יכולים למדל את כל השלוש כצפיפויות נורמליות, ככה הנתונים נוצרו.

אז יש מספר בעיות עם Kmeans. בואו נראה אלגוריתם מאוחר יותר שמטפל בהרבה מהבעיות האלה, גם הוא מאוד פופולרי בתעשייה, הוא נקרא DBSCAN.
:::
:::


---

## DBSCAN {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דיביסקאן פירושו Density-Based Spatial Clustering of Applications with Noise, והוא חלק ממשפחה של אלגוריתמים של קלאסטרינג שמחזירים אותנו למושג של צפיפות או התפלגות.
:::
:::

---

### Density Based Clustering

- Back to premise, data comes from a distribution: $X \sim P_x$

- Estimating $P_x$ is hard

- Find high-density regions through *connected components*

::: {.fragment}
- Immediate Pros:
    - No specifying $K$
    - Outliers
    - More complex clustering structures
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הבעיה בחזרה לקונספט של התפלגות או צפיפות שהיינו רוצים לאמוד היא שזה גם מחזיר אותנו לקושי לעשות את זה בצורה טובה. וכאן החוקרים שפיתחו את DBSCAN, לקחו השראה מתורת הגרפים שם אנחנו עושים קלאסטרינג על גרף של צמתים בצורה טבעית מאוד, שני צמתים קרובים זה לזה אם יש ביניהם קשת. ואז מתקבל הרבה פעמים מבנה שנקרא connected components, תת גרפים שבהם הצמתים קשורים זה לזה.

היתרונות המיידיים של השיטה הזאת: אין צורך לפרט את K, הוא צומח הישר מהדאטא. אולי יש כאן קומפוננטה אחת, אולי חמש.

יתרון אחר הוא שאם יש תצפיות שלא שייכות לקומפוננטות הגדולות, הן אנומליות, הן אאוטליירז.

לבסוף כפי שתיכף נראה הגישה המאוד א-פרמטרית הזאת מביאה גם לאלגוריתמים שמזהים נכון קלאסטרים עם מבנים מסובכים יותר מ"גושים" של תצפיות, למשל טבעת בתוך טבעת שראינו.
:::
:::

---

### DBSCAN

Density-Based Spatial Clustering of Applications with Noise

![](images/dbscan_example.png){width="70%"}

- 3 types of points: core, border, noise
- Core points are high-density points (parameters: $\varepsilon$ (radius), $minPts$)
- Connect core points into clusters
- Assign border points to clusters
- All else: noise

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך עובד DBSCAN.

דיביסקאן יחלק כל תצפית לאחד משלושה סוגים: core, ליבה, border, גבול, noise, רעש.

תצפיות קור הן תצפיות שנמצאות באיזור צפוף של נתונים. בצורה מדויקת יותר אם נגדיר איזשהו רדיוס של שכונה אפסילון, נשאל האם ברדיוס הזה סביב התצפית יש לפחות מינימום תצפיות, לפי פרמטר אחר שנגדיר מינפוינטס. אם כן נסווג תצפית כזאת כתצפית ליבה.

נחבר את כל תצפיות הקור שנמצאות בשכונה אחת של השכונה לקלאסטרים, וגם התצפיות שנמאות בשכונה שלהן אבל הן לא סווגו כתצפיות ליבה. תצפיות כאלה הן תצפיות בורדר, גבול. לבסוף שאר התצפיות שלא שייכות לקלאסטר הן תצפיות נויז, רעש, אאוטליירז.
:::
:::

---

### DBSCAN - Abstract

![](images/dbscan_algo2.png)

[Nice visualiztion](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כל מה שאמרתי עכשיו אפשר לקרוא כאלגוריתם פשוט מהמאמר המקורי (להדגים).

אפשר גם להביט בויזואליזציה נחמדה שממחישה זאת (להדגים).
:::
:::

---

### DBSCAN - Actual

![](images/dbscan_algo1.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי לייעל את הפרוצדורה כמו שהמחברים כותבים וכמו שאולי שמתם לב בדמו, האלגוריתם שבאמת נמצא בשימוש הוא קצת יותר מורכב ויש אותו כאן בפסאודו קוד.

אין מה להיבהל מזה, אם תעברו עליו לאט לאט תראו שהוא דווקא די הגיוני. אנחנו עוברים על כל תצפית. אם כבר סיווגנו אותה לקלאסטר נמשיך לתצפית הבאה. אם לא -- צריך לסווג אותה. נסתכל על כל השכנים שלה, כלומר כל התצפיות שנמצאות במרחק עד אפסילון ממנה, נניח שזה כבר חושב עבורנו. אם מספר התצפיות לא עובר את פרמטר מינפוינטס התצפית כרגע רעש ועוברים לתצפית הבאה. אם כן יש מספיק שכנים נתחיל קלאסטר חדש c ונשייך את התצפית אליו, ונרצה לשייך את כל השכנים שלה אליו.

נעבור שכן שכן. אם השכן הוגדר כרעש ברור שצריך להיות מסווג לקלאסטר הנוכחי c. אם הוא כבר סווג לקלאסטר אחר לא נשנה את זה. ואם הוא לא מוגדר אז שוב ברור שהוא שייך לקלאסטר הנוכחי, אבל גם נבדוק את השכנים שלו. אם יש לו מעט אפשר להמשיך לשכן הבא, אבל אם יש לו הרבה אפשר פשוט להוסיף אותם לרשימת השכנים ולתת לחיפוש להמשיך לפעפע ברשימת השכנים החדשה.

באופן כזה עוברים על כל התצפיות כפי שראינו בדמו, כל פעם מתחילים קלאסטר חדש אם צריך וממצים אותו לעומק עד שנגמר החיפוש.
:::
:::

---

### DBSCAN on Netflix

```{python}
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=1.0, min_samples=10, metric='euclidean')
_ = dbscan.fit(NE_Xtr)
```

What did we get?

```{python}
#| output-location: fragment
print(dbscan.core_sample_indices_.shape)
```

```{python}
#| output-location: fragment
print(dbscan.labels_[:10])
clusters, counts = np.unique(dbscan.labels_, return_counts=True)
d = dict(zip(clusters, counts))
print(d)
print(f'no. of noise points: {np.sum(dbscan.labels_ == -1)}')
```

No concept of prediction!

```{python}
#| output-location: fragment
#| error: true
test_labels = dbscan.predict(NE_Xte)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשמבצעים dbscan צריך לייבא את הקלאסטר המתאים, ולאתחל אותו עם איזשהו אפסילון רדיוס השכונה, ופרמטר מינפוינטס, כאן הוא נקרא min_samples ואפשר לראות שביקשתי 10 כלומר תצפית תהיה קור אם ברדיוס "1" ממנה יש לפחות 10 תצפיות אחרות. בדיביסקאן ניתן כמובן להחליף לכל מטריקת מרחק, כאן אוקלידית.

מה מקבלים כשמריצים fit על הנתונים של נטפליקס?

האינדקסים של נקודות הליבה בשדה core_sample_indices, כאן התקבלו כ300 מתוך 8000 נקודות במדגם הלמידה.

בשדה labels מופיע הקלאס של כל תצפית, כאשר אם היא לא שייכת לאף קלאסטר היא מקבלת מינוס 1. כאן אפשר לראות שכמעט כל התצפיות לא סיווג לקלאסטר, שבעת אלפים מתוך שמונת אלפים. נמצאו באופן טבעי על-ידי האלגוריתם רק עוד מספר קטן של קלאסטרים, הכי גדול בהם עם כמה מאות תצפיות. בריצות עם פרמטרים קצת אחרים תקבלו תוצאות די דומות.

אבל נקודה חשובה, האלגוריתם dbscan ללא איזה עיבוד פוסט הוק לא נותן לכם עוד מידע על הקלאסטרים שגילה כמו מרכז המסה שלהם נניח, כמו שלkmeans יש את הממוצע. לכן לא ניתן לסווג תצפיות חדשות בצורה טבעית ואין מתודה predict לאוביקט שמתקבל מdbscan, תקבלו שגיאה.
:::
:::

---

### DBSCAN on Netflix: Weak relation to PCA?

```{python}
#| code-fold: true

from sklearn.decomposition import PCA

X_centered = NE_Xtr - NE_Xtr.mean(axis=0)
pca = PCA(n_components=2)
pca.fit(X_centered)

T = pca.transform(X_centered)

c_dict = {-1: 'Noise', 0:'Cluster1', 1:'Cluster2', 2:'Cluster3', 3:'Cluster4'}
clusters = np.vectorize(c_dict.get)(dbscan.labels_)
ax = sns.jointplot(x=T[:, 0], y=T[:, 1], hue=clusters, height=5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ושוב קשה בקלאסטרינג באופן כללי להבין למה קיבלנו את החלוקה שקיבלנו, האם היא עושה שכל, במיוחד כשהמימד גבוה. אז אני שוב צובע את התצפיות כאן לפי קלאסטר במרחב במימד שאני כן יודע לצייר, המרחב שנוצר בPCA עם שני הPC הראשונים. הקלאסטר היחיד שעושה שכל זה הקלאסטר הכתום שאכן מתלכד עם ה"דיעה" של PCA שיש קבוצה של צופים שפשוט נותנים ציונים גבוהים לכל הסרטים. לא הצלחתי למצוא קלאסטרים מעניינים אחרים.
:::
:::

---

## DBSCAN on K-means Failures {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נחזור לדוגמאות שלנו של מצבים מאתגרים ונראה במה הועיל DBSCAN.
:::
:::

---

### DBSCAN on K-means Failures (I)

```{python}
#| code-fold: true

dbscan = DBSCAN(eps=0.2).fit(X_circles)
plt.scatter(X_circles[:, 0], X_circles[:, 1], c=dbscan.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז במבנה הטבעות שלנו dbscan מוצא את פתרון ללא קושי, אין לפרוצדורה ביאס דווקא לגושים או כדורים בדאטא, ככל שהקלאסטרים מובחנים טוב יותר ככה יהיה לו קל יותר למצוא אותם, זה פחות קשור למבנה עצמו שלהם.
:::
:::

---

### DBSCAN on K-means Failures (II)

```{python}
#| code-fold: true

dbscan = DBSCAN(eps=0.2).fit(X_square)
plt.scatter(X_square[:, 0], X_square[:, 1], c=dbscan.labels_)
plt.show()
```


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
על הדאטא בהתפלגות אחידה על הריבוע שלנו dbscan גם כאן מוצא שאין צורך ביותר מקלאסטר אחד באופו טבעי.
:::
:::

---

### DBSCAN on K-means Failures (III)

```{python}
#| code-fold: true

dbscan = DBSCAN(eps=1.2).fit(X_out)
plt.scatter(X_out[:, 0], X_out[:, 1], c=dbscan.labels_)
plt.show()
```


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ועל הדאטא שבו יש שני קלאסטרים מובחנים מאוד זה מזה ותצפית אאוטלייר שבבירור לא שייכת לאף אחד מהם, dbscan אכן מגדיר אותה כנקודת רעש, הוא לא רואה חובה לסווג כל תצפית ותצפית.
:::
:::

---

### DBSCAN on K-means Failures (IV)

```{python}
#| code-fold: true

dbscan = DBSCAN(eps=0.9).fit(X_varied)
plt.scatter(X_varied[:, 0], X_varied[:, 1], c=dbscan.labels_)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
איפה שdbscan לא הועיל, זה למשל האתגר של קלאסטרים עם צפיפויות שונות שעדיין קרובים זה לזה. עם צפיפות כל כך נמוכה אנחנו שבפריפריה של הקלאסטר האמצעי dbscan נוטה למצוא הרבה רעש בתצפיות שהן חלק מהקלאסטר וצריך עוד קצת מאמץ בבחירת הפרמטרים שלו כדי שיגיע לסיווג שאנחנו יודעים שהוא נכון.
:::
:::

---

| Method  | Pros | Cons|
|---------|------|-----|
| K-means | Faster, Scalable, Simple <br/>Related to other methods (PCA, EM, GMM) | Need $K$ <br/>Separable spherical clusters <br/>No outliers <br/>Bad with unequal densities <br/>Only Euclidean distance|
| DBSCAN  | Complex structures <br/>No need of $K$ <br/>Any distance metric <br/>Outliers | Slower <br/>Very sensitive to $\varepsilon, minPts$ <br/>Bad with unequal densities|

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם בהשוואה בין שתי השיטות:

Kmeans הוא אלגוריתם פשוט מאוד וסקלבילי להרבה תצפיות ולהרבה משתנים, יש לו קשר הדוק לאלגוריתמים אחרים כמו PCA, EM, GMM שלא דיברנו עליו.

אבל צריך לפרט מהו הK, יש לו העדפה למבנים של ספירות או גושים בדאטא, אין לו קונספט של אאוטליירז וכל תצפית שייכת לקלאסטר יחיד. קשה לו עם צפיפויות משתנות והוא בנוי על מרחק אוקלידי בילט אין.

DBSCAN יכול להבחין במבנים הרבה יותר מוזרים, הוא "מגלה" לבד את הK הנכון, אפשר להציב בו כל מטריקת מרחק והוא לא חייב לסווג את כל התצפיות לקלאסטרים, יכול גם להכריז על אאוטליירז.

מצד שני DBSCAN הרבה יותר איטי, מעבר על נקודה וחיפוש עמוק ורקורסיבי על כל השכנים שלה, רגיש מאוד לפרמטרים אחרים, האפסילון והמינפוינטס, וגם הוא מוצא קלאסטרים בצפיפות נמוכה כאתגר.
:::
:::

---

## Clustering after Dimensionality Reduction {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הזכרנו ביחידה הזאת את PCA. ואם אתם זוכרים גם כשדיברנו על PCA אזכרנו קלאסטרים, או אשכולות. אמרנו שהרבה פעמים אחרי הורדת מימד מתקבלים קלאסטרים מעניינים בדאטא, אז עכשיו זה הזמן לבדוק האם הורדת מימד באמת יכולה לשפר איכות של קלאסטרינג.
:::
:::

---

### The FNIST Dataset

```{python}
from tensorflow.keras.datasets import fashion_mnist
from sklearn.decomposition import PCA

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train = X_train.astype(np.float32) / 255
X_test = X_test.astype(np.float32) / 255

print(X_train.shape)
print(y_train.shape)
```

```{python}
y_train[:10]
```

```{python}
#| code-fold: true
fig, axes = plt.subplots(1, 3, figsize=(6, 3))
axes[0].imshow(X_train[0], cmap="binary")
axes[1].imshow(X_train[1], cmap="binary")
axes[2].imshow(X_train[2], cmap="binary")
_ = axes[0].axis('off')
_ = axes[1].axis('off')
_ = axes[2].axis('off')
plt.show()
```

```{python}
#| echo: false

fnist_dict = {
 0: 'T-shirt/top',
 1: 'Trouser',
 2: 'Pullover',
 3: 'Dress',
 4: 'Coat',
 5: 'Sandal',
 6: 'Shirt',
 7: 'Sneaker',
 8: 'Bag',
 9: 'Ankle boot'
}
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי להדגים את הנושא אני רוצה להציג לכם דאטאסט חדש, הFashion MNIST, או FNIST בקיצור. הדאטא הזה לוקח השראה מדאטא מפורסם אחר שנקרא MNIST ובו יש אלפי תמונות שחור-לבן, כלומר שכבה אחת, של הספרות 0 עד 9 כתובות בכתב יד. כאן במקום ספרות יש לנו 10 סוגים של פריטי לבוש. כמו נעליים, חולצה, שמלה ועוד. יש לנו 60000 תמונות בטריינינג עם 6000 דוגמאות לכל פריט לבוש, ועוד 10000 דוגמאות בטסט סט.

המטרה בד"כ בדאטא כזה היא לבנות מודל קלסיפיקציה לחיזוי תמונות שהמודל לא ראה לאחד מעשרת פריטי הלבוש. כאן דווקא אבצע קלאסטרינג על הדאטא עם Kmeans, ואשתמש בפריטי הלבוש כground truth לראות אם הקלאסטרים קשורים אליהם, כלומר האם, במצב אידאלי, 10 קלאסטרים ייצגו עשרה פריטי לבוש.

:::
:::

---

### K-means on FNIST

```{python}
X_train_flat = X_train.reshape((X_train.shape[0], -1))
print(X_train_flat.shape)
```

```{python}
kmeans = KMeans(n_clusters=10, random_state=0)
kmeans.fit(X_train_flat)

print(pd.crosstab(y_train, kmeans.labels_).rename(index=fnist_dict))
```

::: {.fragment}
Metrics for evaluating clustering vs. ground truth: Rand index, Mutual information, V-measure...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לפני שאני מבצע קלאסטרינג עם Kmeans ברור שהוא לא יכול לעבוד על תמונות שאלה מערכים דו-מימדיים, אז אני משטח 28 על 28 פיקסלים לוקטור ארוך של 784 פיקסלים.

אני מבצע Kmeans ועושה קרוסטאב של הלייבלז שקיבלתי עם הground truth, עם y_train, פריטי הלבוש האמיתיים.

ברור שאין משמעות לסדר הלייבלז 0 עד 9, מעניין רק לראות אם קלאסטרים מסוימים הם אכן מוקדשים כולם או רובם לפריט לבוש ספציפי.

למשל כאן ניתן לראות שמכנסיים אכן מקבלות קלאסטר משלהן, וכן גם נעליים מסוג סניקרז. אבל בגדים עליונים כמו חולצה, מעיל וסוודר מתפזרים להם על פני קלאסטרים שונים.

יש אגב הרבה מטריקות לבדוק את הפתרון של קלאסטרינג כמו אלה שרשומות כאן. למשל האינדקס של ראנד בוחן את אחוז הזוגות של תצפיות מאותו קלאסטר אמיתי כמו חולצה וחולצה, שהסתווגו לאותו קלאסטר כמו 0 ו0.
:::
:::

---

### Recall: PCA might discover clusters


```{python}
X_train_flat_centered = X_train_flat - X_train_flat.mean(axis=0)
pca = PCA(n_components = 2)
pca.fit(X_train_flat_centered)
T = pca.transform(X_train_flat_centered)
```

```{python}
#| code-fold: true
clusters = np.vectorize(fnist_dict.get)(y_train)

sns.scatterplot(x=T[:, 0], y=T[:, 1], hue=clusters, s=10, palette='tab10')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אמרנו שהורדת מימד עשויה לפעמים לגלות קלאסטרים בדאטא. כאן אנחנו עושים PCA עם שני PCs, כלומר מנסים להוריד מימד מ784 פיקסלים ל2 רכיבים בלבד. כשאנחנו צובעים את התצפיות במרחב ההטלה לפי סוגי הלבוש, אנחנו רואים שאכן סוגי לבוש שונים מתמפים קרוב זה לזה. במקרה של מכנסיים בורוד כאן זה מאוד בולט, אולי גם מגפיים. אבל פריטי לבוש אחרים נוטים להתערבב זה בזה, אז לא בטוח כמה Kmeans על המרחב הזה, על מטריצה T יהיה "טוב יותר" במובן שיפריד בין פריטי הלבוש.
:::
:::

---

### Does it improve clustering?

```{python}
kmeans = KMeans(n_clusters=10, random_state=0)
kmeans.fit(T)

print(pd.crosstab(y_train, kmeans.labels_).rename(index=fnist_dict))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו עושים Kmeans על מטריצה T, מטריצה עם שני מימדים בלבד, אין ספק שהוא מהיר הרבה יותר, אבל אם תבחנו את הקרוסטאב תראו שלא ממש הועלנו, ואם תחשבו מדדים כמו rand index תראו שאולי אפילו הזקנו קצת.
:::
:::

---

### Increasing latent dimension $q$

```{python}
pca = PCA(n_components = 30)
pca.fit(X_train_flat_centered)
T = pca.transform(X_train_flat_centered)

kmeans = KMeans(n_clusters=10, random_state=0)
kmeans.fit(T)

print(pd.crosstab(y_train, kmeans.labels_).rename(index=fnist_dict))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הסבר אחד יכול להיות שהשתמשנו במעט מדי PCs, בכל זאת הדאטא ממימד 784 ואנחנו מורידים אותו ל2 בלבד. אם משתמשים ב10 PCs למשל, אכן מקבלים תוצאה איכותית יותר, כעת גם כפכפים וסנדלים יש להם קלאסטר ברור משלהם.

הסבר אחר יכול להיות שהורדת המימד שPCA עושה, שאמרנו בנפנופי ידיים שהיא "ליניארית" פשוט נוקשה מדי, לא מתוחכמת מספיק כדי להגיע לאיזשהו לייטנט ספייס בדרך לא-ליניארית. נסיים את היחידה בלדבר ולהדגים ממש בהיי לבל, דרך כזאת להורדת מימד.
:::
:::

---

### Autoencoders

![](images/ae.png)

- A branch of unsupervised learning: Representation Learning
- How do we learn representations useful for downstream tasks (e.g. clustering)
- Most basic AE: Encode $x_i \in \mathbb{R}^p$ via **encoder** network $f$ to lower dimension $q$, decode with **decoder** network $g$ back to dimension $p$, such that: $x_i \approx g(f(x_i))$
- $f(x_i) \in \mathbb{R}^q$ hopefully *represents* the data well

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אוטואנקודרים הם ארכיטקטורה מאוד פופולרית של רשתות נוירונים להורדת מימד. באופן כללי משתמשים בהם למה שנקרא representation learning, איך אני לומד ייצוגים, כלומר וקטורים ממימד קטן, לעצמים שלי, באופן שיועיל להמשך למשימות כמו חיזוי, כמו קלאסטרינג. כלומר איך אני הופך אוביקט כמו מילה, כמו תמונה, מסמך לוקטור מספרים, שישפר לי ביצועים במשימות כאלה.

האוטואנקודר הכי פשוט הוא זה שאנחנו רואים כאן: אנחנו לוקחים וקטור של משתנים X ממימד p, ולומדים איזושהי רשת, אנקודר f לממד נמוך יותר q, ואז לומדים רשת אחרת ממימד q בחזרה למימד המקורי p, באופן שישחזר כמה שיותר טוב את הוקטור X המקורי, למשל על-ידי מינימזציה לשגיאה ריבועית. או נרשום זאת כך: שX יהיה כמה שיותר קרוב לG על F של X.

אם הייצוגים הם מה שמעניינים אותנו, הוקטורים ממימד נמוך, ניקח את האאוטפוט של רשת f האנקודר שלנו. הוא יחזיר לנו וקטור ממשי ממימד q לכל תצפית. ואנחנו מקווים שאם שגיאת השחזור הריבועית אכן קטנה, הדחיסה הזאת שווה משהו, הייצוגים הנלמדים אכן לוכדים במימד נמוך איזושהי משמעות בתצפיות. אפשר לחשוב על כל אחד מהאלמנטים בוקטור הייצוג כאיזשהו פיצ'ר מחוכם מאוד שהאוטואנקודר למד.

ובמה זה קשור לPCA? קשור מאוד. נסו לחשוב על ארכיטקטורה של הרשת שתבצע PCA ממש. אולי תתחילו עם PC אחד. מכל מקום אפשר לנסח אוטואנקודרים כהכללה של PCA, והם יכולים למצוא את המרחב הלטנטי המעניין הזה בצורה לא-ליניארית, בגלל שהאנקודר ודיקודר הם רשתות.
:::
:::

---

### AE with Keras

```{python}
#| code-line-numbers: "|4-8|9-13|14|15|16|"
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, Reshape

stacked_encoder = Sequential([
    Flatten(input_shape=[28, 28]),
    Dense(100, activation="relu"),
    Dense(30, activation="relu"),
])
stacked_decoder = Sequential([
    Dense(100, activation="relu", input_shape=[30]),
    Dense(28 * 28, activation="sigmoid"), # make output 0-1
    Reshape([28, 28])
])
stacked_ae = Sequential([stacked_encoder, stacked_decoder])
stacked_ae.compile(loss='mse', optimizer='adam')
history = stacked_ae.fit(X_train, X_train, epochs=20, verbose=0)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו רואים מימוש של אוטואנקודר הכי פשוט באמצעות קראס.

הרבה פעמים זה נקרא stacked autoencoder לכן ככה הוא נקרא כאן. כדי להגדיר את האנקודר אני משתמש ברשת שמקבלת את התמונות בגודל 28 על 28, עושה להם השטחה למימד 784 ומוסיפה שתי שכבות עם אקטיבציה רלו.

הדקודר הוא בארכיטקטורה זהה רק בהיפוך: מקבל את הייצוגים ממימד 30, ומוסיף שתי שכבות. השכבה האחרונה ממימד 784 והוא מחזיר אותה לצורת תמונה עם שכבת reshape שאין לה פרמטרים.

כעת האוטואנקודר עצמו הוא שרשור של האנקודר ודיקודר. אני עושה לו קומפליציה עם שגיאה ריבועית כלומר MSE ואיזשהו אופטימייזר, והאינפוט שלו שימו לב הוא X וגם האאוטפוט הוא X! אין כאן הרי y, אנחנו מנסים לקחת X ולבנות זוג רשתות שישחזרו אותו הכי טוב.
:::
:::

---

### Can it reconstruct?

```{python}
def show_reconstructions(sae, images, n_images=5):
  reconstructions = sae.predict(images[:n_images], verbose=0)
  fig = plt.figure(figsize=(n_images * 1.5, 3))
  for image_index in range(n_images):
      plt.subplot(2, n_images, 1 + image_index)
      plt.imshow(images[image_index], cmap='binary')
      plt.axis('off')
      plt.subplot(2, n_images, 1 + n_images + image_index)
      plt.imshow(reconstructions[image_index], cmap='binary')
      plt.axis('off')
show_reconstructions(stacked_ae, X_test)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן יש הדגמה שמראה שאכן על תמונות שהרשת לא ראתה היא מצליחה לשחזר לא רע. בשורה העליונה תמונות שהרשת לא ראתה, בשורה התחתונה השחזור שלהן, אחרי שהעברנו אותן באנקודר-ודיקודר הנלמדים. נראה לא רע, כלומר הרשת מצאה רשת לדחוס את התמונות לוקטורים באורך 30 שיש להם מספיק משמעות שנוכל גם לשחזר את התמונות המקוריות מהם.
:::
:::

---

### Does it improve clustering?

```{python}
T = stacked_encoder.predict(X_train, verbose=0)

print(T.shape)
```

```{python}
kmeans = KMeans(n_clusters=10, random_state=0)
kmeans.fit(T)

print(pd.crosstab(y_train, kmeans.labels_).rename(index=fnist_dict))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת כדי להוריד מימד אני מבקש predict על האנקודר בלבד, על הf הנלמד, ואני אכן מקבל מטריצה T של 60 אלף פריטי הלבוש שלי על 30 מימדים בלבד.

אני מבצע Kmeans במרחב הזה ורוצה לראות אם הועלתי. נראה שכן, ברור שיש כאן יותר אפסים, אבל קשה להגיד שזה "מובהק". המדדים שהזכרנו כמו rand index משתפרים קצת, אבל עדיין ניתן לראות את הבלבול בבגדים עליונים כמו מעיל ושמלה. צריך גם לזכור שעכשיו העלינו את המימד הלטנטי, את q ל30.
:::
:::

---

```{python}
#| eval: false

from sklearn.manifold import TSNE

T_te = stacked_encoder.predict(X_test, verbose=False)

tsne = TSNE(init='pca', learning_rate='auto')
X_test_2D = tsne.fit_transform(T_te)
X_test_2D = (X_test_2D - X_test_2D.min()) / (X_test_2D.max() - X_test_2D.min())
```

```{python}
#| code-fold: true
#| eval: false

clusters = np.vectorize(fnist_dict.get)(y_test)
sns.scatterplot(x=X_test_2D[:, 0], y=X_test_2D[:, 1], hue=clusters, s=10, palette='tab10')
plt.xlabel('D1')
plt.ylabel('D2')
plt.show()
```

![](images/fnist_dim30_tsne.png){width="50%"}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ובכל זאת, כשאני מבצע שיטה שלא למדנו כמו TSNE לצייר את פריטי הלבוש שהם עכשיו ממימד 30 על מפה דו-מימדית, אני רואה הפרדה מרשימה בין סוגי הפריטים. ועדיין קשה מאוד להבדיל בתמונות בגודל 28 על 28 פיקסלים בין חולצות באדום שמתפרשות על כל המרחב הקטן הזה, לבין מעילים לדוגמא.

נסיים בטעימה הזאת על אוטואנקודרים כנציג של למידת ייצוגים, representation learning. אני מקווה שזה יגרה אתכם ללמוד עוד על התחום המרתק הזה. כך אנחנו לומדים עם מילים למשל, עם גרפים ועם כל עצם שלא ברור איך הופכים אותו לוקטור ממימד נמוך של פיצ'רים כמותיים, רציפים.
:::
:::
