=== 1. הקדמה ללמידה בלתי מפוקחת ===

נקדיש יחידה אחת ללמידה מסוג שונה. למידה בלתי מפוקחת, או unsupervised learning. מאחר שזה נושא חדש, בואו נדבר קצת על מה זה unsupervised learning ומה שונה ממנה לעומת הלמידה שעסקנו בה עד כה, שמסתבר שאפשר לקרוא לה supervised learning, כלומר למידה כן מפוקחת.

:::

בלמידה מסוג supervised, יש לנו וקטור X של p משתנים, וסקלאר Y. המטרה היא למדל את Y כפונקציה f של X. אנחנו מניחים שזוגות התצפיות X, Y שלנו מגיעים בלתי תלויים מאיזושהי התפלגות משותפת Pxy, ובונים מודל לחיזוי באמצעות נתוני מדגם הלמידה X, Y, מודל שנקרא f_hat. כשתגיע תצפית חדשה לחיזוי X0 נפעיל עליה את המודל הנלמד f_hat וזה יהיה החיזוי שלנו עבורה. ואיך אנחנו מכמתים את הביצועים של המודל שלנו? באידאל באמצעות איזושהי פונקצית הפסד L בין תצפיות Y האמיתיות והחזויות, כשאנחנו לוקחים תוחלת על תצפיות שהמודל לא ראה. בפועל אנחנו לא יודעים את ההתפלגות של התצפיות שהמודל לא ראה ואנחנו לוקחים את הממוצע האמפירי על מדגם הטסט.

נשאלת השאלה, מה אם אין Y, המשתנה התלוי לחיזוי?

:::

בלמידה לא מפוקחת יש לנו רק וקטור של משתנים X ממימד p. אנחנו עדיין מניחים שהתצפיות מגיעות בלתי תלויות מאיזו התפלגות לא-ידועה Px, והמטרה שלנו היא לא לאמוד איזושהי פונקציה או קשר אלא ממש את ההתפלגות הזאת, או תכונות שלה.

לדוגמא, ניתוח אשכולות או קלאסטרינג -- היינו רוצים ללמוד איזורים בהתפלגות עם צפיפות גבוהה, או השכיחים של P_x. אם נמצא שP_x מתחלקת בבירור לכמה איזורים כאלה למשל, אולי ניתן לייצג אותה בעזרתם, ואז זה יפשט אותה, במקום להיות למשל פונקציה מורכבת בהרבה מימדים נוכל לחלק אותה לצירוף של כמה פונקציות פשוטות יותר.

אבל זה לא ממש חדש נכון? למצוא שכיחים בפונקצית התפלגות, כבר עשינו את זה.

:::

זה בדיוק מה שעשינו עם kernel density estimation או KDE, שבו קיבלנו מדגם של נתונים מX שהוא משתנה רציף, וכדי ממש לאמוד את פונקצית הצפיפות שלו עשינו מעין החלקה להיסטוגרמה שלו עם KDE. לדוגמא כאן אפשר לראות שKDE עונה בדיוק על המטרה שלנו בקלאסטרינג, הוא מוצא שאפשר לייצג את פונקצית ההתפלגות של X בעצם בעזרת צירוף של שלוש פונקציות פשוטות נאמר נורמליות, לכל אחת שכיח משלה. בפועל זה נראה שX מורכב משלוש קבוצות או אשכולות שונים, והתובנה הזאת יכולה להיות בעלת ערך לחוקר.

דיברנו על KDE עבור משתנה אחד. ניתן לחשוב על החלקה כזאת בדו-מימד, ואולי בתלת-מימד. מעבר לזה KDE כבר לא כל-כך מעשי בגלל קללת המימד שדיברנו עליה בהקשר של KNN ושיטות מבוססות שכנים. כדי להעריך נכון את הצפיפות על פני מרחב ממימד גבוה צריך הרבה מאוד תצפיות, ולכן הגישה הזאת של לאמוד את Px עבור יותר משניים-שלושה משתנים היא פשוט לא פרקטית, ואנחנו עוברים לתחום של clustering.

:::

אז בואו נדבר בקלאסטרינג לא על צפיפות כמושג הסתברותי אלא כמושג אינטואיטיבי: אנחנו רוצים למצוא קלאסטרים, קבוצות, איזורים או גושים בדאטא שבהם התצפיות מאוד צפופות וקרובות אחת לשניה, לעומת המרחק ביניהן לתצפיות בקלאסטרים אחרים.

למה שנרצה לעשות את זה? הנה רשימה חלקית:

קודם כל ראינו שקלאסטרינג משמש אותו באקספלורטורי דאטא אנליסיס, איזושהי אנליזה רכה לנתונים שמסייעת להבין אותם, למצוא קבוצות מעניינות בדאטא. את הקבוצות האלה או ההשתייכות לקבוצות האלה אפשר לנצל אחר-כך כפיצ'רים מעניינים בבניית מודלים לחיזוי.

מטרה אחת אפשר לתאר בגדול כסגמנטציה: אתר שמנסה לחלק את הגולשים שלו לכמה טיפוסים, כמה פרופילים. למשל כדי להקצות לכל פרופיל איש מכירות אחר שהפרופיל הזה יהיה המומחיות שלו. או אתר עם המון מוצרים כמו אמזון שרוצה לחסוך בעבודה הקשה של הרבה מומחים לאלקטרוניקה שהתפקיד שלהם הוא להגיד שפלאפון אייפון 5 ופלאפון אנדרואיד שייכים לאותה קטגוריה או מוצר, אולי אפשר לעשות את זה אוטומטית עם קלאסטרינג על פיצ'רים שמראים שאותם אנשים קונים את שני הפריטים האלה או שהם עשויים מאותם חומרים ועולים סכום דומה של כסף. אפשר לחשוב על חברת פיצה שנכנסת לעיר חדשה ושואלת את עצמה איפה למקום שלושה מרכזי הפצה שהיא מתכננת להקים, הכי משתלם למקם אותם באיזורים שונים שבכל אחד ה"צפיפות" בביקוש לפיצה תהיה גבוהה. גם בתוכנה אנחנו עושים קלאסטרינג, במיוחד לתוכנות גדולות ומסורבלות שנוצרו לפני שנים ולאט לאט התפתחו ונוצר צורך לפרק אותן לכמה מודולים, גם בשביל יעילות וגם כדי לחלק את האחריות של אנשי התוכנה להמשיך לפתח ולעשות מיינטננס לכל מודול בנפרד.

בביולוגיה נהוג להשתמש הרבה בקלאסטרינג, למשל להסתכל על פיצ'רים של מחלות כמו תסמינים ולקבץ אותן לסוגים שונים, או לבנות היררכיה של מחלות, עץ.

דוגמא אחרת יכולה להיות דידופליקציה, אם נחזור לאתר שמוכר הרבה פריטים, וכל מוכר נותן כותרת אחרת לפריט שלו, הרבה פעמים לא מדובר במוצר שונה אלא אותו מוצר בדיוק וצריך למחוק במערכות מוצרים חדשים לכאורה שהמערכת כבר אמורה להכיר, דופליקיישנז. דוגמא אחרונה כאן היא זיהוי תצפיות חריגות או אאוטליירז, או בשם המקובל anomaly detection. אם כל האייפונים התקבצו לקלאסטר מסוים ואייפון מסוים לא שייך לקלאסטר, יכול להיות שהוא אנומליה. יכול להיות שבכלל לא מדובר באייפון, למשל הפריט הוא רק כיסוי לאייפון, שהמוכר החליט להכניס לקטגוריה של אייפונים כדי להגדיל את החשיפה שלו.

ויש המון סוגים של אלגוריתמים לקלאסטרינג. יש אלגוריתמים מסוג partition שמבוססים על חלוקה של המרחב לאיזורים שונים זה מזה ככל שניתן וצפופים בתוכם, דוגמא לזה אפשר לראות את KMeans. הרבה אלגוריתמים הם היררכיים, שזה אומר שהם מנסים ליצור חלוקה לקלאסטרים ממצב שכל תצפית היא קלאסטר בפני עצמה, ואנחנו לאט לאט מאחדים זוגות של תצפיות קרובות אחת לשניה, ועד למצב שהדאטא נמצא בקלאסטר אחד גדול. זהו אגלומורטיב קלאסטרינג. אלגוריתמים אחרים באמת חוזרים לרעיון שצריך לאמוד את הצפיפות הטבעית של הדאטא, לא להניח מראש כמה קלאסטרים יש בו ולא להניח שכל התצפיות מתחלקות אליהם, דוגמא לזה הוא הDBSCAN. נעסוק היום בKMeans וב-DBSCAN כמייצגים של המשפחות האלה וגם כי הם סופר-פופולריים בתעשייה.

:::

דוגמא למה ניתן להשיג מקלאסטרינג מבחינת אקספלורטורי דאטא אנליסיס או EDA.

יש כאן בעמודות כמה עשרות דגימות של תאים של גידולי סרטן מסוגים שונים כמו סרטן השחלה או סרטן השד. ובשורות כ6800 גנים ורמת הביטוי שלהם בדגימה. ככל שצבע הגן ירוק יותר כך הוא "בא יותר לביטוי" בדגימה הזאת. התבצע קלאסטרינג מהסוג ההיררכי שדיברנו עליו, גם על השורות וגם על העמודות, כך שזוג שורות קרובות הונחו אחת ליד השניה וגם כל זוג עמודות, ואפשר לראות איך הצעד הזה מחלק יפה גם למשפחות של סוגי סרטן בעמודות וגם למשפחות של גנים שונים שקשורים או לא קשורים לסוגים שונים של סרטן.

:::

מילה אחרונה של אזהרה בכל הנוגע לתחום של unsupervised learning. אין בלמידה לא-מפוקחת קריטריון ברור להצלחה, בניגוד ללמידה מפוקחת שם יש לנו את ה"מפקח" את Y ואת הלוס L, שלאורם אנחנו מכווננים את המודל שלנו, אנחנו מודדים את עצמנו.

הרבה פעמים כמו בדוגמא בשקף הקודם ההצלחה של מודל קלאסטרינג מערבת הרבה בחינה ידנית ושיפוט של הלקוח הסופי. הרבה בערך. ברור שלפעמים יש ground truth, ואנחנו מצפים מהקלאסטרים להיות בחפיפה עם קבוצות ידועות באוכלוסיה או בהתפלגות. ברור גם שפתרון קלאסטרינג על אותם נתונים שהצליח למצוא קבוצות יותר צפופות בתוכן ויותר רחוקות אחת מהשניה - הוא פתרון טוב יותר.

אבל בשורה התחתונה יש הרבה יותר מקום לפרשנות, אין קריטריון מנצח - וכדאי להיות מוכנים מנטלית לזה כשניגשים לפרויקט קלאסטרינג.

=== 2. אלגוריתם Kmeans ===

נדבר כעת על אלגוריתם KMeans, אולי המוכר ביותר בתחום, וגם יעיל יחסית ועובד טוב עם נתונים ממימד גבוה.

:::

נניח שיש לנו חלוקה נתונה. paritition. איך אנחנו עושים איבליואציה של חלוקה כזאת על הנתונים שלנו.

נניח שאנחנו יודעים כבר את מספר הקלאסטרים בדאטא ומסמנים אותו בK, ונניח שיש לנו כבר כלל חלוקה C שמתאים לכל תצפית i בנתונים את הקלאסטר k שמתאים לה, מ1 עד K גדול.

נניח כעת שצפיפות של נתונים או עד כמה זוג תצפיות קרובות אחת לשניה נמדוד באמצעות איזושהי מטריקת מרחק d, לדוגמא נתחיל עם מרחק אוקלידי.

ואנחנו רוצים לבטא באמצעות איזושהי מטריקה את האינטואיציה שלנו שתצפיות באותו קלאסטר צריכות להיות צפופות ורחוקות מתצפיות בקלאסטרים אחרים.

הרבה אלגוריתמים מסתכלים על המטריקה הבאה, הפיזור within cluster שנסמן בW(C), והוא סכום על כל זוגות התצפיות ששייכות לקלאסטר k של המרחקים ביניהן, וסכום על כל הקלאסטרים, מוכפל פי חצי כי אנחנו סופרים כל זוג ככה פעמיים.

מאחר שהפיזור בין כל זוגות התצפיות בלי קשר לחלוקה לקלאסטרים נשאר זהה, אפשר להראות שלעשות מינימום לקריטריון שלנו אקוויולנטי ללעשות מקסימום לכמות המשלימה של between clusters סקאטר: סכום המרחקים בין כל זוגות התצפיות שנמצאות בקלאסטרים שונים. הרי סכום הכמות הזאת והכמות שלנו W(C) מסתכמם בפיזור כללי נאמר T(C).

אז יש לנו מטריקה לעשות לה מינימום. אבל אפילו עם K נתון, נאמר שאנחנו רוצים לחלק את הדאטא ל4 קלאסטרים. האם אנחנו יכולים לעבור על כל החלוקות C(i) האפשריות כדי להגיע למינימום גלובלי? ברור שלא. יש לזה נוסחה שלא מופיעה כאן, אפשר לחשב למשל שלעבור על כל האפשרויות של חלוקת 20 תצפיות ל5 קלאסטרים אנחנו כבר מדברים על כמעט 750 מיליארד אפשרויות!

:::

אז Kmeans קודם כל מצמצם אותנו למרחק אוקלידי בלבד.

תחת מרחק אוקלידי, מסתבר שאפשר לרשום את הקריטריון שלנו בצורה פשוטה יותר: בכל קלאסטר סכום המרחקים מהתצפיות אל ממוצע הקלאסטר, להכפיל במספר התצפיות בקלאסטר n_k, ולסכום על כל הקלאסטרים.

אבל אנחנו יודעים כבר שאם נסתכל על קריטריון דומה, ונשאל מה הנקודה שמביאים למינימום את סכום המרחקים המרובעים ממנה -- נגיע לממוצע המדגם.

לכן נהוג לרשום את הקריטריון של Kmeans בצורה כוללת יותר: למצוא את החלוקה ואת הנקודות של קלאסטרים שיביאו למינימום את סכום המרחקים המרובעים בתוך כל קלאסטר, על פני כל הקלאסטרים. צורת הרישום הזאת מסייעת לנסח את האלגוריתם של Kmeans.

:::

אז מהו האלגוריתם של Kmeans?

K נתון, ואנחנו מתחילים עם איזשהו ניחוש התחלתי עבור הממוצעים m1 עד mk.

כעת החלוקה C(i) של כל תצפית תהיה לפי הקלאסטר שהממוצע שלו הוא הקרוב אליה ביותר.

לאחר החלוקה נעדכן את הממוצעים, בכל קלאסטר ניקח את הממוצע של התצפיות ששייכות אליו.

ונחזור על צעדים 1 ו2 עד שהחלוקה לא משתנה או עד איזשהו קריטריון התכנסות, למשל אפשר לחשב את הלוס שלנו W(C) ולראות שהוא לא משתנה יותר מדי.

מה הבעיה הראשונה באלגוריתם? אמנם התכנסות מובטחת, הצעדים שלנו יכולים רק להפחית את W(C), או לא לשנות אותו. אבל אין הבטחה למינימום גלובלי ואנחנו מאוד תלויים בבחירה הראשונית של הממוצעים שיכולה להיות אקראית.

נהוג לכן בהרבה מימושים לבצע מספר פעמים Kmeans כל פעם מנקודת התחלה אקראית אחרת ולבחור את הפתרון עם הלוס המינימלי.

:::

בדוגמא שלפנינו ברור שיש 4 קלאסטרים, ואנחנו מתחילים עם 4 נקודות אקראיות כממוצעים, מאוד לא מתאימות לחלוקה.

:::

בצעד 1 אנחנו מחלקים כל תצפית לקלאסטר עם הממוצע הכי קרוב אליה במרחק אוקלידי, כאן זה אומר לצבוע אותן ב4 צבעים שונים.

בצעד 2 אנחנו מעדכנים את הממוצעים, וכבר ניתן לראות איך כל ממוצע מייצג כבר איזור הרבה יותר צפוף באופן טבעי.

:::

ושוב אנחנו מחלקים את התצפיות לפי הקרבה שלהן לממוצע החדש.

ושוב אנחנו מעדכנים את הממוצעים. כאן הם כבר זזים ממש מעט.

:::

באיטרציה השלישית כבר בקושי אפשר לראות הבדל, הקלאסטרים כבר ברורים מאוד.

:::

נראה עכשיו איך מבצעים Kmeans על הנתונים של נטפליקס.

כאן אני עושה import מתוך מודול קלאסטר לקלאס Kmeans. אני מאתחל אותו ומבקש 2 קלאסטרים על הצופים שלנו שדירגו סרטים מ1 עד 5. כשאני קורא לfit על הטריינינג דאטא אז רץ האלגוריתם. מה קיבלנו?

בשדה cluster_centers קיבלנו את הממוצעים עצמם. בעצם מדובר במטריצה עם 2 שורות ו14 עמודות, כי ביקשנו שני קלאסטרים ויש לנו 14 סרטים בדאטא, כל ממוצע הוא וקטור באורך 14.

בשדה לייבלז יש את האינדיקטור שמתאר כל תצפית האם היא שייכת לקלאסטר הראשון או השני, או יותר אם היינו מבקשים יותר קלאסטרים.

בשדה inertia יש את קריטריון הW(C) שלנו.

ובאלגוריתם Kmeans באופן טבעי אפשר לחזות במרכאות לאיזה קלאסטר שייכת תצפית שלא ראינו ממדגם הטסט. אני אומר לחזות במרכאות כי זה בעצם יותר לשייך אותה. תצפית שלא ראינו שייכת לקלאסטר עם הממוצע הקרוב ביותר אליה.

:::

עכשיו כפי שאמרנו קשה לדעת עם Kmeans במיוחד אם הדאטא ממימד גבוה אם הגענו לפתרון "איכותי" ומה משמעות הקלאסטרים, בלי דרכים יצירתיות לנסות להבין את זה, בצורה קצת ידנית.

כאן אני לוקח את שמונת הסרטים הראשונים, מדפיס את ממוצע הדירוגים שלהם, ולידם את ממוצע הקלאסטר הראשון ואת ממוצע הקלאסטר השני.

דבר ראשון שאני שם לב אליו זה שהציון הממוצע של כל סרט נמצא בין שני הקלאסטרים (להדגים). דבר שני הוא שבכל הסרטים הממוצע של הקלאסטר הראשון גבוה יותר מממוצע הקלאסטר השני. זה מזכיר לכם משהו? זה מזכיר מאוד את הPC הראשון של הנתונים האלה, שאמרנו שבעצם מדרג את כל הצופים על-פי כמה הם מסכימים עם הדירוג הכללי שהוא די גבוה, כלומר כמה הם נוטים לתת ציונים גבוהים לעומת ציונים נמוכים לכל הסרטים.

:::

אפשר לראות את זה אם נצבע את כל הצופים בשני צבעים שונים על-פי ההשתייכות שלהם לקלאסטרים, במרחב ההטלה שמצאנו בPCA, כשאנחנו עושים תרשים פיזור של PC1 מול PC2 שכבר ראינו.

בקלאסטר אחד יש את הצופים שנוטים לתת ציונים גבוהים, ובקלאסטר שני צופים שנוטים לתת ציונים נמוכים. במובן מסוים זה נראה כאילו הפתרון של Kmeans הוא איזושהי דיסקרטיזציה של הפתרון PCA, במקום רצף ממשי על פני כל הכיוון של הPC הראשון קיבלנו חלוקה לשתי קטגוריות, גבוה ונמוך. ויש עבודות שמראות שזה כמובן לא מקרי, אפשר לראות Kmeans כדיסקרטיזציה של PCA ומזה לצאת לעוד תובנות.

אם הייתי מבקש שלושה קלאסטרים הייתי מקבל 3 קבוצות בכיוון הPC הראשון, נמוך בינוני, גבוה.

:::

ורק כשאני מבקש K = 4, אני מקבל ביטוי של הPC השני. עדיין שתי קבוצות גדולות של צופים שנוטים לדרג גבוה או נמוך, והקבוצה השלישית מתחלק על-פי הטעם שלהם בסרטים רומנטיים.

:::

=== 3. בעיות בKmeans ===

כל זה מרתק וגם יכול לעבוד בסקאלות די גבוהות של תצפיות ושל משתנים, אבל Kmeans ידוע לשמצה במספר בעיות שיש לו, בואו נראה כמה.

:::

בדומה לשיטות שראינו בלמידה supervised יש לנו כאן מעין היפרפרמטר שהשיטה לא לומדת, אנחנו צריכים לתת לה אותו כאינפוט, וזה הK, מספר הקלאסטרים.

והקריטריון של Kmeans בעייתי כי ככל שנגדיל את מספר הקלאסטרים כך הוא יקטן. בקצה אפשר לחשוב על מצב שבו K = N, כל תצפית בקלאסטר משלה, היא גם הממוצע ולכן סכום המרחקים מהממוצע יהיה 0. נהוג לכן לבצע Kmeans עבור K שונים על מדגם למידה, ולנסות לראות בשיטת המרפק או האלבואו איפה יש ירידה חדה בקריטריון ככה שנראה שעברנו איזשהו סף של חלוקה טבעית, וזה הK שנבחר.

הבעיה שכמו בנתונים שלפנינו השיטה הזאת לא תמיד תעבוד, לא ברור מה הK המתאים לנתונים האלה מהקריטריון, והיא גם ידנית. יש שיטות אחרות עם קריטריונים מחוכמים יותר. יש גם פתרונות שמשווים את הקריטריון של החלוקה שלנו לקריטריון המושג עם חלוקה אקראית  או חלוקה שמניחה שהדאטא מתפלג בצורה אחידה על פני המרחב, ואז בוחרים בK שמביא להפרש הגדול ביותר, הK שמביא להפתעה הגדולה ביותר לעומת חלוקה אקראית. מי שרוצה לקרוא עוד יכול לקרוא על הgap statistic.

:::

עניין אחר בKmeans הוא שבהגדרה מדובר במרחק אוקלידי. אי אפשר פשוט להשתמש בKmeans עם מרחק אחר. וזה אומר שבדומה לKNN הוא מושפע מאוד מתצפיות חריגות וממשתנים בסקאלות רחבות מאוד, הם ישפיעו הרבה יותר על הפתרון. אז בעקרון ההמלצה היא לעשות סטנדרטיזציה לנתונים, אבל חשוב להזכיר להביט בנתונים כמה שניתן, יכולים להיות מקרים קיצוניים שבהם הסטנדרטיזציה הזאת עלולה להזיק לאיכות הפתרון.

כאן למשל די ברור שיש לנו שני קלאסטרים בדאטא הצהוב והסגול כי ככה הם גם נוצרו. אבל הם נבדלים זה מזה הודות למשתנה שבציר הX, הם לא נבדלים זה מזה בכלל במשתנה שבציר הY. אבל למשתנה בציר הX יש פיזור גבוה הרבה יותר, ואכן, לאחר סטנדרטיזציה כדי שלמשתנים יהיה פיזור דומה, החלוקה לשני קלאסטרים שהיתה כל כך ברורה מיטשטשטת, וKmeans מוצא פתרון שמתאים פחות לבעיה המקורית, ואין דרך לדעת את זה.

:::

Kmeans אוהב דאטא שמחולק ל"גושים" אפשר להראות שהוא דומה מאוד לאלגוריתם שמנסה לחלק את הדאטא לצפיפויות של מספר התפלגויות נורמליות, עם פיזור סימטרי יפה בכל הכיוונים סביב הממוצע.

כשהדאטא נראה כמו כמו הדאטא שלנו, שברור שיש שני קלאסטרים אבל הם לא spherical, הם באים בדפוס הרבה יותר מתוחכם של טבעות -- Kmeans נכשל לחלוטין.

:::

עניין אחר בעייתי, שתמיד צריך להגיד מהו K, ותמיד נקבל חלוקה לK קלאסטרים, בלי שום מטריקה שתגיד לנו אם החלוקה עושה שכל או לא. כאן לדוגמא יש ריבוע עם המון תצפיות בהתפלגות אחידה וKmeans פשוט מחזיר 3 איזורים איך שבא לו, הוא לא מגיע עם אזהרה שאומרת שאין חלוקה "הגיונית" לשלושה קלאסטרים לדאטא כאן.

:::

בהמשך לדוגמא הקודמת לכל תצפית גם חייב להיות סיווג בKmeans. אין לו קונספט כזה של outliers, ואם לא מבצעים עוד איזשהו עיבוד פוסט-הוק ולוקחים את הסיווג של כל תצפית כמובן מאליו, אפשר לקבל מצב כמו כאן שהתצפית החריגה שייכת לקבוצה הסגולה למרות שברור שהיא לא.

:::

כאן יש דוגמא שמראה גם קלאסטרים בגודל שונה וגם קלאסטרים עם רמת צפיפות שונה, והבעייתיות שבפתרון של Kmeans. בקלאסטר האמצעי גם יש פי 2 פחות תצפיות מהקלאסטרים האחרים וגם הפיזור שלו גדול הרבה יותר. ואפשר לראות איך הקלאסטרים עם הרבה תצפיות וצפופים מושכים אליהם תצפיות שאולי היינו מסווגים כשייכות לקלאסטר הפנימי אם היינו יכולים למדל את כל השלוש כצפיפויות נורמליות. ככה הנתונים נוצרו.

אז יש מספר בעיות עם Kmeans. בואו נראה אלגוריתם מאוחר יותר שמטפל בהרבה מהבעיות האלה, גם הוא מאוד פופולרי בתעשייה, הוא נקרא DBSCAN.

:::

=== 4. DBSCAN ===

דיביסקאן פירושו Density-Based Spatial Clustering of Applications with Noise, והוא חלק ממשפחה של אלגוריתמים של קלאסטרינג שמחזירים אותנו למושג של צפיפות או התפלגות.

:::

הבעיה בחזרה לקונספט של התפלגות או צפיפות שהיינו רוצים לאמוד היא שזה גם מחזיר אותנו לקושי לעשות את זה בצורה טובה. וכאן החוקרים שפיתחו את DBSCAN, לקחו השראה מתורת הגרפים שם אנחנו עושים קלאסטרינג על גרף של צמתים בצורה טבעית מאוד, שני צמתים קרובים זה לזה אם יש ביניהם קשת. ואז מתקבל הרבה פעמים מבנה שנקרא connected components, תת גרפים שבהם הצמתים קשורים זה לזה.

היתרונות המיידיים של השיטה הזאת: אין צורך לפרט את K, הוא צומח הישר מהדאטא. אולי יש כאן קומפוננטה אחת, אולי חמש.

יתרון אחר הוא שאם יש תצפיות שלא שייכות לקומפוננטות הגדולות, הן אנומליות, הן אאוטליירז.

לבסוף כפי שתיכף נראה הגישה המאוד א-פרמטרית הזאת מביאה גם לאלגוריתמים שמזהים נכון קלאסטרים עם מבנים מסובכים יותר מ"גושים" של תצפיות, למשל טבעת בתוך טבעת שראינו.

:::

אז איך עובד DBSCAN.

דיביסקאן יחלק כל תצפית לאחד משלושה סוגים: core, ליבה, border, גבול, noise, רעש.

תצפיות קור הן תצפיות שנמצאות באיזור צפוף של נתונים. בצורה מדויקת יותר אם נגדיר איזשהו רדיוס של שכונה אפסילון, נשאל האם ברדיוס הזה סביב התצפית יש לפחות מינימום תצפיות, לפי פרמטר אחר שנגדיר מינפוינטס. אם כן נסווג תצפית כזאת כתצפית ליבה.

נחבר את כל תצפיות הקור שנמצאות בשכונה אחת לקלאסטר, וגם התצפיות שנמצאות בשכונה שלהן אבל הן לא סווגו כתצפיות ליבה. תצפיות כאלה הן תצפיות בורדר, גבול. לבסוף שאר התצפיות שלא שייכות לקלאסטר הן תצפיות נויז, רעש, אאוטליירז.

:::

כל מה שאמרתי עכשיו אפשר לקרוא כאלגוריתם פשוט מהמאמר המקורי (להדגים).

אפשר גם להביט בויזואליזציה נחמדה שממחישה זאת (להדגים).

:::

כדי לייעל את הפרוצדורה כמו שהמחברים כותבים וכמו שאולי שמתם לב בדמו, האלגוריתם שבאמת נמצא בשימוש הוא קצת יותר מורכב ויש אותו כאן בפסאודו קוד.

אין מה להיבהל מזה, אם תעברו עליו לאט לאט תראו שהוא דווקא די הגיוני. אנחנו עוברים על כל תצפית. אם כבר סיווגנו אותה לקלאסטר נמשיך לתצפית הבאה. אם לא -- צריך לסווג אותה. נסתכל על כל השכנים שלה, כלומר כל התצפיות שנמצאות במרחק עד אפסילון ממנה, נניח שזה כבר חושב עבורנו. אם מספר התצפיות לא עובר את פרמטר מינפוינטס התצפית כרגע רעש ועוברים לתצפית הבאה. אם כן יש מספיק שכנים נתחיל קלאסטר חדש c ונשייך את התצפית אליו, ונרצה לשייך את כל השכנים שלה אליו.

נעבור שכן שכן. אם השכן הוגדר כרעש ברור שצריך להיות מסווג לקלאסטר הנוכחי c. אם הוא כבר סווג לקלאסטר אחר לא נשנה את זה. ואם הוא לא מוגדר אז שוב ברור שהוא שייך לקלאסטר הנוכחי, אבל גם נבדוק את השכנים שלו. אם יש לו מעט אפשר להמשיך לשכן הבא, אבל אם יש לו הרבה אפשר פשוט להוסיף אותם לרשימת השכנים ולתת לחיפוש להמשיך לפעפע ברשימת השכנים החדשה.

באופן כזה עוברים על כל התצפיות כפי שראינו בדמו, כל פעם מתחילים קלאסטר חדש אם צריך וממצים אותו לעומק עד שנגמר החיפוש.

:::

כשמבצעים dbscan צריך לייבא את הקלאס המתאים, ולאתחל אותו עם איזשהו אפסילון רדיוס השכונה, ופרמטר מינפוינטס, כאן הוא נקרא min_samples ואפשר לראות שביקשתי 10 כלומר תצפית תהיה קור אם ברדיוס "1" ממנה יש לפחות 10 תצפיות אחרות. בדיביסקאן ניתן כמובן להחליף לכל מטריקת מרחק, כאן אוקלידית.

מה מקבלים כשמריצים fit על הנתונים של נטפליקס?

האינדקסים של נקודות הליבה בשדה core_sample_indices, כאן התקבלו כ300 מתוך 8000 נקודות במדגם הלמידה.

בשדה labels מופיע הקלאס של כל תצפית, כאשר אם היא לא שייכת לאף קלאסטר היא מקבלת מינוס 1. כאן אפשר לראות שכמעט כל התצפיות לא סווגו לקלאסטר, שבעת אלפים מתוך שמונת אלפים. נמצאו באופן טבעי על-ידי האלגוריתם רק עוד מספר קטן של קלאסטרים, הכי גדול בהם עם כמה מאות תצפיות. בריצות עם פרמטרים קצת אחרים תקבלו תוצאות די דומות.

אבל נקודה חשובה, האלגוריתם dbscan ללא איזה עיבוד פוסט הוק לא נותן לכם עוד מידע על הקלאסטרים שגילה כמו מרכז המסה שלהם נניח, כמו שלkmeans יש את הממוצע. לכן לא ניתן לסווג תצפיות חדשות בצורה טבעית ואין מתודה predict לאוביקט שמתקבל מdbscan, תקבלו שגיאה.

:::

ושוב קשה בקלאסטרינג באופן כללי להבין למה קיבלנו את החלוקה שקיבלנו, האם היא עושה שכל, במיוחד כשהמימד גבוה. אז אני שוב צובע את התצפיות כאן לפי קלאסטר במרחב במימד שאני כן יודע לצייר, המרחב שנוצר בPCA עם שני הPC הראשונים. הקלאסטר היחיד שעושה שכל זה הקלאסטר הכתום שאכן מתלכד עם ה"דיעה" של PCA שיש קבוצה של צופים שפשוט נותנים ציונים גבוהים לכל הסרטים. לא הצלחתי למצוא קלאסטרים מעניינים אחרים.

:::

=== 5. DBSCAN על הבעיות של Kmeans ===

בואו נחזור לדוגמאות שלנו של מצבים מאתגרים ונראה במה הועיל DBSCAN.

:::

אז במבנה הטבעות שלנו dbscan מוצא את פתרון ללא קושי, אין לפרוצדורה ביאס דווקא לגושים או כדורים בדאטא, ככל שהקלאסטרים מובחנים טוב יותר ככה יהיה לו קל יותר למצוא אותם, זה פחות קשור למבנה עצמו שלהם.

:::

על הדאטא בהתפלגות אחידה על הריבוע שלנו dbscan גם כאן מוצא שאין צורך ביותר מקלאסטר אחד באופן טבעי.

:::

ועל הדאטא שבו יש שני קלאסטרים מובחנים מאוד זה מזה ותצפית אאוטלייר שבבירור לא שייכת לאף אחד מהם, dbscan אכן מגדיר אותה כנקודת רעש, הוא לא רואה חובה לסווג כל תצפית ותצפית.

:::

איפה שdbscan לא הועיל, זה למשל האתגר של קלאסטרים עם צפיפויות שונות שעדיין קרובים זה לזה. עם צפיפות כל כך נמוכה אנחנו רואים שבפריפריה של הקלאסטר האמצעי dbscan נוטה למצוא הרבה רעש בתצפיות שהן חלק מהקלאסטר וצריך עוד קצת מאמץ בבחירת הפרמטרים שלו כדי שיגיע לסיווג שאנחנו יודעים שהוא נכון.

:::

נסכם בהשוואה בין שתי השיטות:

Kmeans הוא אלגוריתם פשוט מאוד וסקלבילי להרבה תצפיות ולהרבה משתנים, יש לו קשר הדוק לאלגוריתמים אחרים כמו PCA, EM, GMM שלא על כולם דיברנו.

אבל צריך לפרט מהו הK, יש לו העדפה למבנים של ספירות או גושים בדאטא, אין לו קונספט של אאוטליירז וכל תצפית שייכת לקלאסטר יחיד. קשה לו עם צפיפויות משתנות והוא בנוי על מרחק אוקלידי בילט אין.

DBSCAN יכול להבחין במבנים הרבה יותר מוזרים, הוא "מגלה" לבד את הK הנכון, אפשר להציב בו כל מטריקת מרחק והוא לא חייב לסווג את כל התצפיות לקלאסטרים, יכול גם להכריז על אאוטליירז.

מצד שני DBSCAN הרבה יותר איטי, יש כאן מעבר על כל נקודה וחיפוש עמוק ורקורסיבי על כל השכנים שלה, הוא רגיש מאוד לפרמטרים אחרים, האפסילון והמינפוינטס, וגם הוא מוצא קלאסטרים בצפיפות נמוכה כאתגר.

:::

=== 6. קלאסטרינג לאחר הורדת מימד ===

הזכרנו ביחידה הזאת את PCA. ואם אתם זוכרים גם כשדיברנו על PCA אזכרנו קלאסטרים, או אשכולות. אמרנו שהרבה פעמים אחרי הורדת מימד מתקבלים קלאסטרים מעניינים בדאטא, אז עכשיו זה הזמן לבדוק האם הורדת מימד באמת יכולה לשפר איכות של קלאסטרינג.

:::

כדי להדגים את הנושא אני רוצה להציג לכם דאטאסט חדש, הFashion MNIST, או FNIST בקיצור. הדאטא הזה לוקח השראה מדאטא מפורסם אחר שנקרא MNIST ובו יש אלפי תמונות שחור-לבן, כלומר שכבה אחת, של הספרות 0 עד 9 כתובות בכתב יד. כאן במקום ספרות יש לנו 10 סוגים של פריטי לבוש. כמו נעליים, חולצה, שמלה ועוד. יש לנו 60000 תמונות בטריינינג עם 6000 דוגמאות לכל פריט לבוש, ועוד 10000 דוגמאות בטסט סט.

המטרה בד"כ בדאטא כזה היא לבנות מודל קלסיפיקציה לחיזוי תמונות שהמודל לא ראה לאחד מעשרת פריטי הלבוש. כאן דווקא אבצע קלאסטרינג על הדאטא עם Kmeans, ואשתמש בפריטי הלבוש כground truth לראות אם הקלאסטרים קשורים אליהם, כלומר האם, במצב אידאלי, 10 קלאסטרים ייצגו עשרה פריטי לבוש.

:::

לפני שאני מבצע קלאסטרינג עם Kmeans ברור שהוא לא יכול לעבוד על תמונות כי אלה מערכים דו-מימדיים, אז אני משטח 28 על 28 פיקסלים לוקטור ארוך של 784 פיקסלים.

אני מבצע Kmeans ועושה קרוסטאב של הלייבלז שקיבלתי עם הground truth, עם y_train, פריטי הלבוש האמיתיים.

ברור שאין משמעות לסדר הלייבלז 0 עד 9, מעניין רק לראות אם קלאסטרים מסוימים הם אכן מוקדשים כולם או רובם לפריט לבוש ספציפי.

למשל כאן ניתן לראות שמכנסיים אכן מקבלים קלאסטר משלהן, וכן גם נעליים מסוג סניקרז. אבל בגדים עליונים כמו חולצה, מעיל וסוודר מתפזרים להם על פני קלאסטרים שונים.

יש אגב הרבה מטריקות לבדוק את הפתרון של קלאסטרינג כמו אלה שרשומות כאן. למשל האינדקס של ראנד בוחן את אחוז הזוגות של תצפיות מאותו קלאסטר אמיתי כמו חולצה וחולצה, שהסתווגו לאותו קלאסטר כמו 0 ו0.

:::

אז אמרנו שהורדת מימד עשויה לפעמים לגלות קלאסטרים בדאטא. כאן אנחנו עושים PCA עם שני PCs, כלומר מנסים להוריד מימד מ784 פיקסלים ל2 רכיבים בלבד. כשאנחנו צובעים את התצפיות במרחב ההטלה לפי סוגי הלבוש, אנחנו רואים שאכן סוגי לבוש שונים מתמפים קרוב זה לזה. במקרה של מכנסיים בורוד כאן זה מאוד בולט, אולי גם מגפיים. אבל פריטי לבוש אחרים נוטים להתערבב זה בזה, אז לא בטוח כמה Kmeans על המרחב הזה, על מטריצה T יהיה "טוב יותר" במובן שיפריד בין פריטי הלבוש.

:::

כשאנחנו עושים Kmeans על מטריצה T, מטריצה עם שני מימדים בלבד, אין ספק שהוא מהיר הרבה יותר, אבל אם תבחנו את הקרוסטאב תראו שלא ממש הועלנו, ואם תחשבו מדדים כמו rand index תראו שאולי אפילו הזקנו קצת.

:::

הסבר אחד יכול להיות שהשתמשנו במעט מדי PCs, בכל זאת הדאטא ממימד 784 ואנחנו מורידים אותו ל2 בלבד. אם משתמשים ב10 PCs למשל, אכן מקבלים תוצאה איכותית יותר, כעת גם לסנדלים יש אולי קלאסטר ברור יותר משלהם.

הסבר אחר יכול להיות שהורדת המימד שPCA עושה, שאמרנו בנפנופי ידיים שהיא "ליניארית" פשוט נוקשה מדי, לא מתוחכמת מספיק כדי להגיע לאיזשהו לייטנט ספייס בדרך לא-ליניארית. נסיים את היחידה בלדבר ולהדגים ממש בהיי לבל, דרך כזאת להורדת מימד.

:::

אוטואנקודרים הם ארכיטקטורה מאוד פופולרית של רשתות נוירונים להורדת מימד. באופן כללי משתמשים בהם למה שנקרא representation learning, איך אני לומד ייצוגים, כלומר וקטורים ממימד קטן, לעצמים שלי, באופן שיועיל להמשך למשימות כמו חיזוי, כמו קלאסטרינג. כלומר איך אני הופך אוביקט כמו מילה, כמו תמונה, מסמך לוקטור מספרים, שישפר לי ביצועים במשימות כאלה.

האוטואנקודר הכי פשוט הוא זה שאנחנו רואים כאן: אנחנו לוקחים וקטור של משתנים X ממימד p, ולומדים איזושהי רשת, אנקודר f לממד נמוך יותר q, ואז לומדים רשת אחרת ממימד q בחזרה למימד המקורי p, נקרא לה g, באופן שישחזר כמה שיותר טוב את הוקטור X המקורי, למשל על-ידי מינימזציה לשגיאה ריבועית. או נרשום זאת כך: שX יהיה כמה שיותר קרוב לG על F של X.

אם הייצוגים הם מה שמעניינים אותנו, הוקטורים ממימד נמוך, ניקח את האאוטפוט של רשת f האנקודר שלנו. הוא יחזיר לנו וקטור ממשי ממימד q לכל תצפית. ואנחנו מקווים שאם שגיאת השחזור הריבועית אכן קטנה, הדחיסה הזאת שווה משהו, הייצוגים הנלמדים אכן לוכדים במימד נמוך איזושהי משמעות בתצפיות. אפשר לחשוב על כל אחד מהאלמנטים בוקטור הייצוג כאיזשהו פיצ'ר מחוכם מאוד שהאוטואנקודר למד.

ובמה זה קשור לPCA? קשור מאוד. נסו לחשוב על ארכיטקטורה של הרשת שתבצע PCA ממש. אולי תתחילו עם PC אחד. מכל מקום אפשר לנסח אוטואנקודרים כהכללה של PCA, והם יכולים למצוא את המרחב הלטנטי המעניין הזה בצורה לא-ליניארית, בגלל שהאנקודר ודיקודר הם רשתות.

:::

כאן אנחנו רואים מימוש של אוטואנקודר הכי פשוט באמצעות קראס.

הרבה פעמים זה נקרא stacked autoencoder לכן ככה הוא נקרא כאן. כדי להגדיר את האנקודר אני משתמש ברשת שמקבלת את התמונות בגודל 28 על 28, עושה להם השטחה למימד 784 ומוסיפה שתי שכבות עם אקטיבציה רלו.

הדקודר הוא בארכיטקטורה זהה רק בהיפוך: מקבל את הייצוגים ממימד 30, ומוסיף שתי שכבות. השכבה האחרונה ממימד 784 והוא מחזיר אותה לצורת תמונה עם שכבת reshape שאין לה פרמטרים.

כעת האוטואנקודר עצמו הוא שרשור של האנקודר ודיקודר. אני עושה לו קומפליציה עם שגיאה ריבועית כלומר MSE ואיזשהו אופטימייזר, והאינפוט שלו שימו לב הוא X וגם האאוטפוט הוא X! אין כאן הרי y, אנחנו מנסים לקחת X ולבנות זוג רשתות שישחזרו אותו הכי טוב.

:::

כאן יש הדגמה שמראה שאכן על תמונות שהרשת לא ראתה היא מצליחה לשחזר לא רע. בשורה העליונה תמונות שהרשת לא ראתה, בשורה התחתונה השחזור שלהן, אחרי שהעברנו אותן באנקודר-ודיקודר הנלמדים. נראה לא רע, כלומר הרשת מצאה דרך לדחוס את התמונות לוקטורים באורך 30 שיש להם מספיק משמעות שנוכל גם לשחזר את התמונות המקוריות מהם.

:::

כעת כדי להוריד מימד אני מבקש predict על האנקודר בלבד, על הf הנלמד, ואני אכן מקבל מטריצה T של 60 אלף פריטי הלבוש שלי על 30 מימדים בלבד.

אני מבצע Kmeans במרחב הזה ורוצה לראות אם הועלתי. נראה שכן, ברור שיש כאן יותר אפסים, אבל קשה להגיד שזה "מובהק". המדדים שהזכרנו כמו rand index משתפרים קצת, אבל עדיין ניתן לראות את הבלבול בבגדים עליונים כמו מעיל ושמלה. צריך גם לזכור שעכשיו העלינו את המימד הלטנטי, את q ל30.

:::

ובכל זאת, כשאני מבצע שיטה שלא למדנו כמו TSNE לצייר את פריטי הלבוש שהם עכשיו ממימד 30 על מפה דו-מימדית, אני רואה הפרדה מרשימה בין סוגי הפריטים. ועדיין קשה מאוד להבדיל בתמונות בגודל 28 על 28 פיקסלים בין חולצות שהן כאן נקודות אדומות שמתפרשות על כל המרחב הקטן הזה, לבין מעילים לדוגמא שהם כאן נקודות סגולות.

נסיים בטעימה הזאת על אוטואנקודרים כנציג של למידת ייצוגים, representation learning. אני מקווה שזה יגרה אתכם ללמוד עוד על התחום המרתק הזה. כך אנחנו לומדים עם מילים למשל, עם גרפים ועם כל עצם שלא ברור איך הופכים אותו לוקטור ממימד נמוך של פיצ'רים כמותיים, רציפים.
:::
