---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Principal Components Analysis"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Principal Components Analysis - Class 4

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::
---

## The PCA Problem {.title-slide}

---

### Advanced exploration: dimensionality reduction

::: {.incremental}
- We have $n$ points in $p$ dimensions. In the Netflix Ddataset: $n=10^4, p=14$ (considering only the fully observed movies)
- We want to reduce the data to $q \ll p$ dimensions (typically $q=2$ or $q=3$), to: 
    - Identify **important dimensions** which summarize the data well
    - **Visualize** the data (2-d or 3-d visualizations)
    - Identify **structure** in the data, such as clusters

- Naive way: select $q$ out of the original $p$ dimensions
    - For $q=2$, we have been looking at pairwise plots of movies

- Less Naive way: Look for interesting "projections": linear combinations of the variables which expose interesting information and patterns
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Mathematical setup

::: {.incremental}
- We have $n$ vectors in ${\mathbb R}^p$: $\mathbf{x}_1, \dots, \mathbf{x}_n$

- We can desribe them thorugh a matrix $X_{n \times p}$ (each row is an observation)

- Assume for simplicity that the columns are centered: $\sum_i x_{ij} = 0,\;\forall j$, so our data is a cloud around $0$ in ${\mathbb R}^p$

- Total dispersion (squared distance of points from their center): $\sum_{ij} x_{ij}^2 = ||\mathbf{x}_1||_2^2+ \dots + ||\mathbf{x}_n||_2^2 = \text{tr}(X'X)$

- Possible goal: find $q \ll p$ good directions, such that **much of the dispersion** will be captured by these directions.

- A direction in ${\mathbb R}^p$ is a vector $\mathbf{v}\in {\mathbb R}^p$ with $||\mathbf{v}||_2^2=1$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The PCA Problem

::: {.incremental}

- Goal: Find the $q$ direction(s) with the most dispersion

- Projection is direction $\mathbf{v}$: $X\mathbf{v} \in \mathbb R^n.$ Examples: 
    - $\mathbf{v} = (1,0,...,0)'$: pick first coordinate from each observation
    - $\mathbf{v} = (1/\sqrt{p},1/\sqrt{p},...,1/\sqrt{p})'$: project on diagonal (average all coordinates)


- Dispersion in direction $\mathbf{v}$: $||X\mathbf{v}||^2 = \mathbf{v}'(X'X)\mathbf{v}.$

- Finding the best direction which maximizes dispersion: $\mathbf{v}_1 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1} \|X\mathbf{v}\|^2$

- $\mathbf{v}_1$ is the first Principal Component direction: the best direction to project on!

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How do we find the next principal component? 

::: {.incremental}
- Now we want a **different** direction $\mathbf{v}_2$ which maximizes direction after accounting for $\mathbf{v}_1$.

- Require orthogonality: $\mathbf{v}_2 = \arg\max_{||\mathbf{v}||^2 =1, \mathbf{v}'\mathbf{v}_1 = 0}||X\mathbf{v}||^2$

- This is the second principal direction

- Can keep going looking for new directions

- Assuming $p < n$, up to $p$ principal directions can be found this way, stack them into a $p \times p$ "loadings" matrix $W$

- Data with reduced dimensionality: $T_{n \times q} = X_{n \times p}W_{p \times q}$ taking only the first $q$ principal directions
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## PCA on Netflix Dataset {.title-slide}

---

### PCA on the Netflix data

::: {.incremental}
- Our Netflix dataset contains the 1-5 rankings made by 10,000 users to 99 movies.

- Let $X$ be the data matrix for the first 14 movies to which all users gave ranking.

- So: $n = 10000$ and $p = 14$. This isn't that "Big", however even with 14 variables it is almost impossible to see any latent structure hidden in the data.

- Let's perform PCA!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Remember the Data

```{python}
import numpy as np
import pandas as pd

ratings = pd.read_csv('../datasets/netflix/train_ratings_all.csv', header = None)
miss_cong = pd.read_csv('../datasets/netflix/train_y_rating.csv', header = None, names = ['score'])
movies = pd.read_csv('../datasets/netflix/movie_titles.csv', header = None, names = ['year', 'title'])
```

```{python}
#| echo: false
print()
```

::: {.fragment}
```{python}
X = ratings.values[:,:14]

print(X.shape)
```
:::

```{python}
#| echo: false
print()
```

::: {.fragment}
```{python}
print(X[:5, :5])
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Centering the Data

```{python}
# currently..
X.mean(axis=0)
```

::: {.fragment}
```{python}
# centering X: subtracting the mean from each column
X_centered = X - X.mean(axis=0)

print(X_centered.mean(axis=0))
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Performing PCA

::: {.fragment}
Performing PCA in 3 lines:
```{python}
#| eval: false
#| code-line-numbers: "|1|3-4|6-7|"

from sklearn.decomposition import PCA

# instantiating PCA object
pca = PCA()

# performing PCA
pca.fit(X_centered)
```
:::

```{python}
#| echo: false

from sklearn.decomposition import PCA

# instantiating PCA object
pca = PCA()

# performing PCA
_ = pca.fit(X_centered)
```

::: {.fragment}
What did we get?
:::

```{python}
#| echo: false

# Helper function for better pandas styling
def color_negative_red(val):
    """
    Takes a scalar and returns a string with
    the css property `'color: red'` for negative
    strings, black otherwise.
    """
    color = 'red' if val < 0 else 'black'
    return 'color: %s' % color
```

::: {.fragment}
The $W$ matrix, a.k.a the "loadings", each column is a principal direction:
```{python}
W = pca.components_.T

print(W.shape)
```

Let's see the W matrix first 2 principal directions alongside the movies: do you see anything interesting?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| code-fold: true
means = pd.DataFrame(X.mean(axis = 0))
means.columns = ['mean_rating']
loadings2 = pd.DataFrame(W[:, :2], columns = ['PC1', 'PC2'])
first_2_PCs = pd.concat([movies[:14]['title'], means, loadings2], axis = 1).set_index('title')

first_2_PCs.head(10).style.applymap(color_negative_red).format("{:.2f}")
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### PC directions in a plot

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

ax = sns.scatterplot(x='PC1', y='PC2', size='mean_rating', data=first_2_PCs)
for i, point in first_2_PCs.iterrows():
    ax.text(point['PC1'], point['PC2'], str(i))
plt.axhline(y=0, color='r', linestyle='--')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Projected Data

```{python}
T = X_centered @ W # make sure this is the same as pca.transform(X_centered)

print(T.shape)
```

More typically we would want to **reduce** dimensionality:

```{python}
T = X_centered @ W[:, :2]

print(T.shape)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
#| code-fold: true

ax = sns.jointplot(x=T[:,0], y=T[:,1], height=3.5)
ax.set_axis_labels('PC1: Popular Vote', 'PC2: Romance vs. Action', fontsize=10)
plt.show()
```

::: {.incremental}
- The first PC will indicate to what extent the user conforms with the general popular vote of movies
- The second PC will indicate if the user is a romance-comedy-drama or action person
- Are there interesting clusters or people worth pointing at? Would have been nice to see users' gender here!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### And now: Miss Congeniality!

What do you expect the scores to be for users that are high on 1st PC (hate all films)?

::: {.fragment}
```{python}
PC1_top_100 = T[:, 0].argsort()[-100:]
print('Top PC1:', miss_cong.iloc[PC1_top_100, :].groupby('score').size())
```
:::

What do you expect the scores to be for users that are low on 1st PC (love all films)

::: {.fragment}
```{python}
PC1_bottom_100 = T[:, 0].argsort()[:100]
print('Bottom PC1:', miss_cong.iloc[PC1_bottom_100, :].groupby('score').size())
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

What do you expect the scores to be for users that are high on 2nd PC (like romantic films)?

::: {.fragment}
```{python}
PC2_top_100 = T[:, 1].argsort()[-100:]
print('Top PC2:', miss_cong.iloc[PC2_top_100, :].groupby('score').size())
```
:::

What do you expect the scores to be for users that are low on 2nd PC (like action films)

::: {.fragment}
```{python}
PC2_bottom_100 = T[:, 1].argsort()[:100]
print('Bottom PC2:', miss_cong.iloc[PC2_bottom_100, :].groupby('score').size())
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## PCA via SVD {.title-slide}

---

### Calculating Principal Components: the SVD

::: {.incremental}
- We have the matrix $X_{n \times p}$ and want to find:
$$\mathbf{v}_1 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1} \|X\mathbf{v}\|^2$$ 
$$\mathbf{v}_2 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1, \mathbf{v}^T\mathbf{v}_1 = 0}\|X\mathbf{v}\|^2$$ 
Etc. 

- The key: the Singular value decomposition (SVD) $X = U D V'$, where: 
    - $U_{n\times p}$ is a matrix with orthonormal columns: $U'U = I_{p \times p}$
    - $D_{p\times p}$ is a diagonal matrix with non-negative diagonal elements  (called the Singular Values)
    - $V_{p\times p}$ is an orthogonal matrix: its columns are an orthonormal basis of $\mathbb R^p$, $V'V = V V' = I$. 
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### More on the SVD

- The key: the Singular value decomposition (SVD) $X = U D V'$, where: 
    - $U_{n\times p}$ is a matrix with orthonormal columns: $U'U = I_{p \times p}$
    - $D_{p\times p}$ is a diagonal matrix with non-negative diagonal elements  (called the Singular Values)
    - $V_{p\times p}$ is an orthogonal matrix: its columns are an orthonormal basis of $\mathbb R^p$, $V'V = V V' = I$.

::: {.incremental}
- Assuming $d_1 > d_2 > \dots, d_p$ on the diagonal of $D$ defines this decomposition uniquely.

- Now denote the columns of $V=[\mathbf{v}_1, \dots, \mathbf{v}_p]$, a basis of $\mathbb R^p$, and the columns of $U=[\mathbf{u}_1, \dots, \mathbf{u}_p]$.

- Key observation: $X\mathbf{v}_j = U D V' \mathbf{v}_j = UD \mathbf{e}_j = \mathbf{u}_j d_j$ because of orthogonality of $V$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Using the SVD to find the PCA

::: {.incremental}
- Given a vector $\mathbf{v} \in \mathbb R^p$ with $\|\mathbf{v}\|^2=1$, express it in the $V$ columns basis:
$\mathbf{v} = a_1\mathbf{v}_1 + \dots + a_p \mathbf{v}_p \mbox{ with } a_1^2 + \dots + a_p^2 = 1$

- Now we can calculate its dispersion using the SVD:
$X\mathbf{v} = a_1 d_1 \mathbf{u}_1 + \dots + a_p d_p \mathbf{u}_p \Rightarrow \|X\mathbf{v}\|^2 = a_1^2 d_1^2 + \dots + a_p^2 d_p ^2$

- We know $a_1^2 + \dots + a_p^2=1$, hence: $\|X\mathbf{v}\|^2 \leq d_1^2.$

- But we get equality when $\mathbf{v}=\mathbf{v}_1$, the first column of $V$, hence $\mathbf{v}_1$ is the first PC

- Similarly the second PC is defined by $\mathbf{v}_2$ in $V$, and so on
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## PCA via Eigendecomposition {.title-slide}

---

### Eigendecomposition

::: {.fragment}
A non-zero vector $\mathbf{v}$ is an eigenvector of a square $p \times p$ matrix $\mathbf{A}$ if it satisfies:
$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v},$$
for some scalar $\lambda$.
:::

::: {.incremental}
- Then $\lambda$ is called the eigenvalue corresponding to $\mathbf{v}$.

- Geometrically speaking, the eigenvectors of $\mathbf{A}$ are the vectors that $\mathbf{A}$ merely elongates or shrinks, and the amount that they elongate/shrink by is the eigenvalue

- An eigendecomposition of $\mathbf{A}$ is then: $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$,

- where $\mathbf{V}$ is the square $n \times n$ matrix whose $j$th column is the eigenvector $\mathbf{v}_j$ of $\mathbf{A}$, and $\mathbf{\Lambda}$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, $\mathbf{\Lambda}_{jj} = \lambda_j$

- If $\mathbf{A}$ is real and symmetric, $\mathbf{V}$ is orthogonal, $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}'$ and $\lambda_j$ are scalars

- If $\mathbf{A}$ is also positive definite (PSD), then $\lambda_j \ge 0$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Calculating Principal Components: the Eigendecomposition

::: {.incremental}
- Look again at the PCA problem:
$$\mathbf{v}_1 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1} \|X\mathbf{v}\|^2$$

- Using Lagrange multiplier $\lambda_1$: $\max_{\mathbf{v}_1}{\mathbf{v}_1'X'X\mathbf{v}_1} + \lambda_1(1 - \mathbf{v}_1'\mathbf{v}_1)$

- Take derivative with respect to $\mathbf{v}_1$, compare to 0:
$$2X'X\mathbf{v}_1 - 2\lambda_1\mathbf{v}_1 = 0 \\
X'X\mathbf{v}_1 = \lambda_1\mathbf{v}_1$$

- So $\mathbf{v}_1$ must be an eigenvector of the square, real, PSD $X'X$ matrix, and $\lambda_1$ its eigenvalue!

- Which eigenvalue and eigenvector?

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

- So we're looking for the set of $W_{p \times p}$ eigenvectors of $X'X$ with their corresponding eigenvalues $\lambda_1, \dots, \lambda_p$ ordered from largest to smallest.

- One can also show the $\lambda_j$ themselves are the variances of the PCs

::: {.incremental}
- What is the relation to SVD?
    - Let $X = UVD'$ as before
    - $X'X = VD'U'UDV = VD'DV = VD^2V$
    - $D^2$ is $p \times p$ diagonal with **squared** singular values on diagonal
    - Which means $V$ are eigenvectors of and they're also the required $W$ for PCA
    - Eigenvalues $\lambda_1, \dots, \lambda_p$ are squared singular values $d_1^2, \dots, d_p^2$
    - Either look at $\lambda_1, \dots, \lambda_p$ or $d_1^2, \dots, d_p^2$ for the PCs variance
:::