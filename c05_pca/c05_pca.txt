=== 1. בעיית הPCA ===
בשיעור זה נלמד על ניתוח גורמים ראשיים, או principal components analysis. אפשר לראות ב-PCA כשיטה מתקדמת לחקר של דאטא ולכן אנחנו לומדים אותה ממש כעת. אפשר גם לראות בה הרבה מעבר. PCA היא הבסיס להרבה מאוד אלגוריתמים מתקדמים בחזית המחקר, אלגוריתמים שעוסקים בלמידת ייצוגים ממימד נמוך, לדאטא ממימד עצום, על מנת שנוכל ללמוד מהדאטא הזה משהו.

:::

נתחיל בדוגמא פשוטה ונתקדם משם. יש לי דאטא עם n תצפיות ושני משתנים. אני רוצה לייצג כל תצפית לא עם שני משתנים אלא עם אחד. מספר אחד שימצה אותה, וזאת עם לאבד כמה שפחות מידע. כלומר: אני רוצה להוריד את המימד מ2 ל1.

אם אני משרטט את הנתונים, ברור לי אינטואיטיבית מה הכיוון בהם, מה הקו או הוקטור ששומר את מקסימום המידע. זה הקו הזה.

:::

PCA יעשה בדיוק את זה. הוא ימצא את הכיוון, או הוקטור שלאורכו נשמר מירב המידע בנתונים, וכשנמצא את הוקטור הזה, נוכל להטיל את הנתונים עליו, כלומר כל תצפית תקבל את הערך שלה על הקו החשוב הזה. וככה נקבל את מה שרצינו: דאטא במימד שני משתנים, הפך לדאטא במימד אחד, עם כמה שפחות איבוד מידע.

:::

באופן כללי יש לנו נתונים עם n תצפיות ו-p משתנים. זה מתרגם למטריצה X עם n שורות ו-p עמודות. בדוגמא של נטפליקס n יהיה 10 אלפים ו-p יהיה 14 אם ניקח רק את הסרטים לגביהם יש לנו את כל הנתונים.

נרצה להוריד את המימד של הנתונים להיות q, כשq בדרך כלל קטן בהרבה מ-p, בד"כ 2 או 3. נראה שבדרך זו נוכל לזהות כיוונים חשובים בדאטא שמתמצתים אותו היטב, נוכל לעשות תרשים לדאטא בדו או תלת מימד (אי אפשר לצייר נקודות ב-14 מימד!) ואם אנחנו ברי מזל נוכל גם לראות מבנים בנתונים שלא היינו רואים אחרת, נראה את הנתונים שלנו ממש מתחלקים לאשכולות, לקלאסטרים.

איך מבצעים PCA? הדרך הנאיבית היא לבצע בחירה של q משתנים מתוך p. כל פעם שבחרנו שני סרטים בדאטא ועשינו תרשים פיזור של סרט אחד מול אחר זה בדיוק מה שעשינו! אבל איבדנו ככה המון מידע!

הדרך הנאיבית פחות היא לעשות מה שעשינו בשקף הקודם רק על הרבה משתנים: לחפש הטלות, לחפש צירופים ליניאריים של המשתנים שיהוו כיוונים מעניינים בדאטא. בואו נפרמל את הבעיה הזאת.

:::

כל תצפית היא וקטור באורך p. יש לנו n תצפיות או וקטורים כאלה, x1 עד xn.

אפשר לסדר את הוקטורים האלה אחד מתחת לשני במטריצה n על p, כאשר כל שורה היא וקטור.

נניח לרגע שכל עמודה ממורכזת, כלומר הסכום שלה או הממוצע שלה הוא אפס. אם כל משתנה ממורכז סביב האפס, ניתן לראות בנתונים שלנו מאין "ענן" סביב הראשית במרחב p מימדי. שוב כל פעם שהמילים מרחב p מימדי מרתיעות אתכם, חשבו על p = 2 או p = 3, מימדים שניתן לדמיין.

דרך אחת לתאר כמה מידע יש במטריצה הזאת, מידע שנרצה לשמר, הוא הפיזור הריבועי של התצפיות מהמרכז שלהן. אם הנחנו שהמרכז שלהן באפס אז הכמות הזאת תהיה פשוט הוקטורים בריבוע, או הטרייס של X טרנספוז X, מטריצת הקווריאנס של המדגם הזה.

אז יש לנו כמות לשמר, ואנחנו רוצים למצוא q כיוונים שאם נטיל את המטריצה שלנו עליהם, נשמר כמה שיותר מהכמות הזאת, מהפיזור הזה, מהשונות של הדאטא.

וכשאנחנו אומרים כיוון, אנחנו מתכוונים לוקטור פי מימדי עם נורמה 1, כלומר וקטור מנורמל או וקטור יחידה. נסמן אותו ב-v.

:::

אז אנחנו רוצים למצוא את וקטור ההטלה ששומר על הכי הרבה פיזור. בואו ניתן דוגמאות:

אם v יהיה למשל וקטור הבסיס הטריוויאלי בR^p, שיש בו 1 בקואורדינטה הראשונה ואפס בכל השאר - כשנטיל את המטריצה עליו דבר זה יהיה שקול לבחירת המשתנה הראשון. כלומר המספר שייצג כל תצפית יהיה הערך שלה בעמודה הראשונה.

אם ניתן משקולת שווה לכל קוארדינטה, ומשקולת כזאת חייבת להיות 1 חלקי שורש p כדי שהוקטור יהיה מנורמל - הטלה של המטריצה שלנו עליו תהיה שקולה ללקחת את הממוצע של כל המשתנים עבור כל תצפית.

ובואו נגדיר מהו הפיזור של ההטלה Xv, מדובר בנורמה של הוקטור הזה, שאנו רואים בביטוי v'X'Xv.

לסיכום הנה הבעיה של PCA: נרצה לבחור את הכיוון v שימקסם את הפיזור של ההטלה v'X'Xv.

כשנמצא אותו, נקרא לכיוון הזה הprinicipal component direction הראשון. זה הכיוון הטוב ביותר להטיל עליו!

:::

אבל זה רק כיוון ראשון. אמרנו שאנחנו רוצים q  כיוונים. איך נמצא את הכיוונים הבאים?

הכיוון הבא v2  יהיה הכיוון שממקסם את פיזור ההטלה, אחרי שהתחשבנו כבר בכיוון v1.

אפשר לראות שהדבר שקול לדרישת אורתוגנוליות. הוקטור שאנחנו מחפשים v2 ממקסם את פיזור ההטלה, הנורמה שלה, כך שהוא מנורמל וגם הוא אורתוגנלי לוקטור הראשון.

זה יהיה הכיוון השני, ואפשר להמשיך לכיוון שלישי וכך עד שמשיגים q כיוונים מנורמלים אורתוגונליים זה לזה.

כשנשיג q וקטורים כאלה, כל אחד באורך p, נוכל להציב אותם זה לצד זה ולקבל מטריצה שנסמן כW. קוראים למטריצה כזאת הרבה פעמים loadings, כי מה שיש בכל וקטור ווקטור שמצאנו הוא בעצם המשקולות לכל משתנה ומשתנה מהדאטא המקורי. אם בדאטא המקורי לדוגמא יש 3 משתנים, והוקטור v שאנחנו מצאנו הוא (1/sqrt(3), sqrt(2)/sqrt(3), 0), שהוא בערך (0.57, 0.81, 0), זה אומר שהמשתנה הראשון קיבל משקולת 0.57, המשתנה השני 0.81, והשלישי משקולת 0, הוא לא חשוב לכיוון הזה שמצאנו.

ההטלה עצמה גם היא תהיה חשובה לנו: אם נכפול את הדאטא n על p שלנו במטריצת ההטלה W שבה יש את q הכיוונים הראשונים, נקבל דאטא ממימד נמוך יותר, מימד q. נקרא לו T.

=== 2. PCA על הנתונים של נטפליקס ===

נעשה רגע הפסקה בכל האלגברה הזאת לראות את השורה התחתונה כדי שתהיה לנו מוטיבציה לחזור שוב לחלק המתמטי ולראות איך מוצאים את פתרון ה-PCA.

:::

ניזכר שוב בדאטא של התחרות של נטפליקס, שמכיל דירוגים מ-1 עד 5 על 99 סרטים.

X יהיה המטריצה עם עשרת אלפים שורות על 14 הסרטים הראשונים אותם דירגו כל עשרת אלפים המשתמשים.

n שווה 10000, p שווה 14, זה לא כזה גדול אבל כאמור איך תעשו ויזואליזציה לדאטא 14 מימדי?!

נבצע PCA עם פייתון.

:::

אנחנו קוראים את הנתונים כפי שעשינו ביחידה הקודמת, ומוודאים שהמימדים הם כפי שאנחנו מצפים, 10000 על 14.

שוב נציץ לתוך הדאטא: יש כאן פשוט מספרים מ-1 עד 5.

:::

ניזכר שניתוח PCA אנחנו עושים על מטריצת נתונים אחרי שהיא עברה מרכוז או סנטרינג. כרגע היא כמובן לא ממורכזת, הממוצע של כל עמודה שונה מאפס.

כדי למרכז את הנתונים נחסר מכל עמודה את הממוצע שלה. כעת באוביקט X centered, יש את הנתונים שלנו אחרי מרכוז, אפשר לראות שהממוצע של כל עמודה הוא אפס מבחינה נומרית.

:::

איך עושים PCA על הנתונים שלנו? זאת בעצם הפעם הראשונה שאנחנו נתקלים בספריית sklearn, זאת ספריית אופן סורס או קוד פתוח, למימוש מודלים סטטיסטיים ומודלים של למידת מכונה שעוד נלמד עליהם.

כאן אני מייבא את הקלאס המתאים PCA ממודול של sklearn שנקרא decomposition.

אני מאתחל את הקלאס באמצעות אובייקט שאני קורא לו pca. בשלב הזה יכולתי למשל לפרט מהו q הרצוי באמצעות פרמטר הn_components, לדוגמא n_components=2.

פעולת הPCA עצמה קורית רק כשאני קורא למתודה פיט של האוביקט, כשאני מכניס לתוכה את הנתונים שלי, X_centered.

וזהו, עשינו PCA. אבל מה קיבלנו?

קיבלנו את המטריצה W, בשדה components קו תחתון. נשים לב שמאחר שלא פירטנו מהו q כמה כיוונים אנחנו רוצים, קיבלנו את מקסימום הכיוונים האפשרי שזהוא p, לכן המטריצה W שלנו היא בגודל p על p. נשים לב גם שsklearn מחזיר את המטריצה כפי שאנחנו סימנו אותו עם טרנספוז, בגלל זה צריך לעשות על הארגומנט קומפוננטס טרנספוז אם רוצים לקבל את W כפי שאנחנו סיכמנו.

אבל זה עדיין לא אומר מה קיבלנו. בדרך כלל מעניינים אותנו שניים-שלושה הכיוונים הראשונים, בואו נראה את המשקולות שלהם לכל סרט וסרט:

:::

בטבלה הזאת, מוצגים עשרת הסרטים הראשונים: השם שלהם, ממוצע הדירוג שלהם, והמשקולות שלהם במה שאנחנו קראנו v1 ו-v2, הם כיווני ה-PC הראשונים כלומר שתי העמודות הראשונות ב-W.

למי שמעוניין הקוד מצורף כאן במצגת.

הסתכלו רגע על הטבלה ונסו למצוא פרשנות כלשהי למשקולות של ה-PC הראשון, ולמשקולות של ה-PC השני.

קצת קשה לראות את זה. ובכל זאת שימו לב שהמשקולות ב-PC הראשון הן כולן לאותו כיוון. והגודל שלהן קשור מאוד בפופולריות הכללית של הסרט. ככל שהדירוג הממוצע של הסרט גדול יותר, כך קטנות המשקלות בערך מוחלט. כלומר נראה שהכיוון בנתונים ששומר על הכי הרבה פיזור, אם היינו צריכים לתת מספר אחד לכל תצפית שהיא במקרה שלנו צופה שראתה 14 סרטים, היינו מסתכלים כמה הצופה הזאת מסכימה עם הממוצע!

לגבי הכיוון השני, צריך קצת להכיר את הסרטים כדי לראות את זה. אבל בואו נראה את זה עם תרשים.

:::

בתרשים הפיזור שלפנינו כל סרט הוא נקודה, שהגודל שלה נקבע על-ידי כמה הסרט פופולרי. רכיב הX של הנקודה הוא המשקל שלה בPC הראשון, ורכיב הY שלה הוא המשקל שלה בPC השני.

את מה שראינו בטבלה אפשר לראות גם כאן, ככל שהסרט פופולרי יותר כך הנקודה קטנה בערך מוחלט.

אבל על "מדבר" הPC השני? אם אתם מכירים את הסרטים ברשימה זה די ברור: הPC השני מבדיל בין סרטים רומנטיים כמו אשה יפה, סוויט הום אלבמה, לבין סרטי אקשן, כמו קון איר וארמגדון.

כלומר אחרי שהתחשבנו בכיוון הראשון שמבטא כמה צופה מדרגת סרטים פשוט על-פי הפופולריות שלהם, הכיוון השני מבדיל כנראה בין צופים שאוהבים סרטים רומנטיים לבין צופים שמעדיפים סרטי אקשן.

חשוב להדגיש כבר עכשיו - זו פרשנות! כאן היא ממש קופצת מול העיניים, הרבה פעמים זה לא יהיה כל כך פשוט.

:::

אמרנו שגם הדאטא של ההטלה מעניין אותנו. אחרי שנכפול את X בW נקבל את T שהיא מטריצה מסדר גודל עשרת אלפים על 14. בעצם קיבלנו שוב מטריצת נתונים מסדר גודל 10000 על 14 אבל כזאת שהעמודות שלה אורתוגנוליות זו לזו, והיא מחלקת את הפיזור או השונות המקוריים בצורה שונה לגמרי. העמודה הראשונה תהיה עם הפיזור הגדול ביותר, לאחר מכן השניה וכולי.

בדרך כלל נתעניין בהורדת מימד, כלומר בהכפלה של X רק בעמודות הראשונות של W, כדי לקבל מטריצה T עם 2 עמודות בלבד.

עכשיו אפשר סוף סוף לעשות ויזואליזציה לנתונים שלנו, עם תרשים פיזור פשוט:

:::

על ציר האיקס הציון של כל צופה בPC הראשון, או עד כמה הצופה דומה בדירוגים שלו לדיעה הפופולרית.

על ציר הוואי הציון של כל צופה בPC השני, עד כמה הוא מעדיף סרטים רומנטיים על פני סרטי אקשן.

בשלב הזה פעמים רבות מתגלים אשכולות מעניינים בדאטא, או קלאסטרים. כאן קשה לראות אשכולות מעניינים אבל היה מעניין לצבוע את המגדר של הצופים כאן, ולראות האם אכן נכונה הסטיגמה שנשים אוהבות יותר סרטים רומנטיים וגברים אוהבים יותר סרטי אקשן. אם הסטיגמה נכונה נראה יותר נשים במעלה הגרף, ויותר גברים בתחתית הגרף.

עדיין חשוב להבין מה מייצגות תצפיות הקצה לראות שהבנו: כאן (תצפית הכי עליונה) יש צופה שנותן ציונים גבוהים לסרטים רומנטיים וציונים נמוכים יחסית לסרטי אקשן. כאן (נקודה הכי ימנית) יש צופה שפשוט מצביע הפוך לדעת הרוב, מאחר שהציונים הממוצעים די גבוהים, הצופה הזה כנראה שונא את כל הסרטים. ואילו כאן (נקודה הכי שמאלית) נראה צופה שנותנת ציונים בתיאום עם הדעה הפופולרית, כנראה ציונים גבוהים באופן כללי לכל הסרטים. 

:::

מה לגבי הסרט איזו מין שוטרת? אילו ציונים נצפה לראות מצופים שגבוהים בPC הראשון, כלומר שונאים את כל הסרטים?

אם ניקח את מאה הצופים עם ציון הכי גבוה בPC הראשון נראה שהם אכן נתנו ציון נמוך מאוד גם לאיזו מין שוטרת.

ומה לגבי 100 הצופים עם הציון הכי נמוך בPC הראשון, כלומר צופים שאוהבים הכל -- נראה שאכן הם אהבו גם את איזו מין שוטרת.

:::

בשביל לצפות מה יהיו הציונים של צופים גבוהים או נמוכים בPC השני, צריך קודם להחליט האם איזו מין שוטרת הוא סרט רומנטי או סרט אקשן. מי שראה את הסרט ידע להגיד שזה לא חד משמעי, אבל בגדול זה סרט שהצד הרך שלו, הקומי-רומנטי, דומיננטי יותר. ולכן זה לא מפתיע שצופים שגבוהים בPC השני נותנים ציונים גבוהים יחסית לאיזו מין שוטרת, וצופים שנמוכים בPC השני נותנים ציונים נמוכים יחסית לאיזו מין שוטרת.

לפני שנחזור לחלק המתמטי, אני מקווה שאתם מתרשמים איזה כלי מדהים זה PCA. מאוחר יותר נרצה אולי לחזות כיצד ידרג משתמש שלא ראינו בדאטא, את הסרט איזו מין שוטרת, על סמך שאר הדירוגים שלו. הכיוונים האלה שגילינו יכולים לעזור לנו מאוד, אפשר לומר שמצאנו משתנים חדשים בדאטא, משתנים שאנחנו יודעים שמשמרים הרבה מהשונות, ונוכל לבקש על הצופה הזה שאלה פשוטה שאולי לא היינו חושבים עליה בכלל - האם יש לך העדפה לסרטים רומנטיים על פני סרטי אקשן?

:::

עוד תרשים שעשוי לעניין אתכם נקרא סקרי-פלוט, שמשתמש בשדה explained_variance_ratio_ של אוביקט ה-PCA. הסקריפלוט מצייר את אחוז השונות המוסברת של כל PC. בדרך כלל נראה דפוס יורד בחדות, הPC הראשונים מסבירים אחוז ניכר מהשונות, ואלה שבאים אחריהם מסבירים מעט מאוד.

לפעמים נעזרים בתרשים כזה לדעת מה המימד הראוי לנתונים. ובכל מקרה אחוזי השונות המוסברת מסתכמים למאה.

=== 3. PCA דרך פירוק SVD ===

נצלול כעת לדרך שבה המחשב מבצע עבורינו PCA, דרך פירוק הSVD. בחלקים הבאים נשתמש בלא מעט ידע מאלגברה ליניארית, אם אתם מוצאים את עצמכם לא בטוחים בקשר לכמה מושגים, מומלץ לעצור ולחזור עליהם.

:::

נסכם שוב את הבעיה של PCA. אנחנו רוצים וקטור v1 מנורמל, כך שיביא למקסימום פיזור של ההטלה של X עליו.

כשנשיג אותו, נרצה את הוקטור הבא v2 שיהיה מנורמל, אורתגונלי לv1 וימקסם את הפיזור של ההטלה של X עליו, וכך הלאה, עד q וקטורים כאלה.

המפתח להשיג את הוקטורים האלה הוא פירוק הsingular value decomposition, או SVD. מטריצת הנתונים שלנו ממשית, ונניח לרגע תמיד שמספר התצפיות n גדול ממספר המשתנים p. מסתבר שאפשר לפרק תמיד את המטריצה לנו למכפלה של שלוש: UDV טרנספוז, 

כאשר: U היא מטריצה n על p עם עמודות אורתונורמליות, כך שאם נכפול U'U נקבל את מטריצת היחידה I.

D היא מטריצה אלכסונית מסדר p על p, על האלכסון שלה נמצאים ערכים אי-שליליים שנקראים גם ערכים סינוגלריים.

ו-V, מטריצה אורתוגונלית מסדר p על p. זה אומר שאם נכפול V'V וגם אם נכפול VV' נקבל את מטריצת היחידה. וזה גם אומר שהעמודות של V הן בסיס בR^p.

:::

כדי להגדיר את הפירוק באופן ייחודי, נחליט גם שהערכים הסינגולריים מסודרים מגדול לקטן. אם יש ביניהם חזרות זה מסבך מעט את הרישום לכן נניח שהם שונים.

כעת נסמן את העמודות של V כv1 עד vp, ואת העמודות של U  u1 עד up.

וכעת נשים לב מה קורה להטלה של X על אחד מוקטורי הבסיס של של V, vj:

ההטלה של X על vj, היא בעצם UDV'v_j, אבל העמודה v_j יוצרת אפס במכפלה עם כל העמודות ב-V כי V אורתגונולית, ואילו עם העמודה v_j היא יוצרת 1 כי v_j הוא וקטור יחידה, הנורמה שלו 1.
זה אומר, שבמכפלה הזאת התוצאה תהיה וקטור שכולו אפסים, ו-1 רק במיקום ה-j, וקטור שמסמנים אותו בד"כ כe_j, וקטור בסיס סטנדרטי.
כל מה שנשאר מהמכפלה הזאת זה רק הרכיב ה-j של המכפלה UD, כלומר הוקטור u_j, שמוכפל בסקלר d_j.

:::

כעת נחזור לבעיה שלנו. אחנו מעוניינים למצוא איזשהו וקטור מנורמל v. נביע את הוקטור הזה בבסיס של V, זה אומר כצירוף ליניארי בעמודה V, עם איזשהן משקלות a1 עד ap. קל לודא שסכום המשקולות האלה בריבוע חייב להיות 1.

אבל אם כך, מהי בעצם ההטלה של X על אותו v שאנחנו רוצים שהפיזור שלה יהיה כמה שיותר גדול? ההטלה היא בעצם צירוף ליניארי של עמודות U, שהמשקולות שלו הן a1 כפול d1, עד ap כפול dp. ושוב, מאחר שהעמודות של u הן וקטורי יחידה, כשנחשב את הפיזור, או הנורמה של ההטלה הזאת, נגיע למסקנה שגם הוא שווה לסכום המשקולות האלה בריבוע.

אבל סכום המשקולות של a בריבוע הוא 1. וה-d, הערכים הסינגולריים שלנו מסודרים בערך יורד. זה אומר שהערך הכי גבוה שהפיזור יכול לקבל הוא אם a1 יהיה 1 ואז הפיזור יהיה שווה ל-d1 בריבוע!

אבל מה המשמעות של a1 שווה לאחת? המשמעות היא ש-v, הוקטור שאנחנו מחפשים, הוא בעצם v1, וקטור העמודה הראשון של המטריצה V.
כלומר גילינו שv1 הוא הוקטור PC הראשון, שיביא למקסימום את פיזור ההטלה, וגילינו גם שפיזור ההטלה כזאת יהיה בהכרח הערך הסינגולרי d1 בריבוע!

באופן דומה תגלו שכיוון הPC הבא הוא v2, העמודה השניה של המטריצה V מפירוק הSVD, וכולי.

מה לא סיפרתי לכם? איך משיגים את פירוק SVD. אבל זה קצת יותר מדי בשביל הקורס שלנו. הדבר החשוב הוא שיש לנו אלגוריתמים מהירים בסקאלות מאוד מרשימות לעשות פירוק SVD, וזה אומר שאנחנו יכולים לבצע PCA על נתונים גדולים מאוד, מהר מאוד.

=== 4. PCA דרך פירוק ערכים עצמיים ===

נושא ה-PCA מרתק גם מהרבה כיוונים אחרים. אפשר גם להגיע לפיתרון על-ידי פתרון בעיית ערכים עצמיים של מטריצה.

:::

ניזכר בבעיית הערכים העצמיים בקצרה. נרצה למצוא וקטור עצמי, eigenvector, למטריצה A ריבועית, מסדר p על p.

כאשר המטריצה A כופלת וקטור כזה, הדבר שקול בעצם פשוט להכפלה של הוקטור הזה באיזשהו סקלר למדא. ולמדא הוא הערך העצמי.

מבחינה גיאומטרית, נראה שכל מה שעשתה המטריצה A לוקטור v, זה פשוט לכווץ או להאריך אותו. ומסתבר שלמציאת וקטור כזה יש שימושים רבים.

פירוק ערכים עצמיים של A הוא מכפלה של המטריצות V, למדא, V בהופכית.

כשV היא מטריצה ריבועית p על p, שהעמודות שלה הם הוקטורים העצמיים, ולמדא היא מטריצה אלכסונית שעל האלכסון של נמצאים הלמדות, הערכים העצמיים.

אם A היא מטריצה ממשית וסימטרית כמו שתיכף יהיה במקרה שלנו, V היא גם אורתוגונלית, וההופכי שלה הוא הטרנספוז של אז אפשר לרשום את הפירוק כך. יתרה מזאת, הערכים העצמיים שלה הם ממשיים.

ואם המטריצה היא חיובית, פוזיטיב-סמידפיניט, כמו שתיכף יהיה במקרה שלנו - הערכים העצמיים הם אפילו אי-שליליים.

:::

אז מה הקשר לבעיה שלנו? נכתוב אותה שוב.

אפשר לכתוב אותה כבעית אופטימיזציה, אם נשתמש בכופלי לגראנז'. אנחנו רוצים למקסם את הכמות v'X'Xv, עם אילוץ על v'v.

אם נגזור את הכמות הזאת לפי הרכיבים בv, נקבל את הביטוי שלפנינו, נשווה אותו לאפס ונגיע למסקנה שאנחנו מחפשים וקטור v שיקיים את המשוואה הזאת. זאת בדיוק משוואה שמגדירה וקטור וערך עצמי של המטריצה X'X, מטריצת הקווריאנס של מדגם הנתונים!

לכן v1 חייב להיות וקטור עצמי של מטריצת הקווריאנס, ולמדא1 הערך העצמי שלה. ומאחר שכל מטריצת קווריאנס היא ממשית, סימטרית וחיובית, למדא גם חייב להיות אי-שלילי.

איזה וקטור עצמי וערך עצמי ניקח? אם נכפול את הביטוי כאן ב v טרנספוז מצד שמאל נראה שהפיזור עצמו שווה לערך העצמי, ואנחנו רוצים פיזור כמה שיותר גדול, לכן ניקח את הוקטור העצמי שמתאים לערך העצמי הגדול ביותר. זכרו שהם אי-שליליים!

:::

נסכם: אנחנו מחפשים את המטריצה W שעמודותיה עם הוקטורים העצמיים של מטריצת הקווריאנס X'X, והערכים העצמיים שלה מסודרים מגדול עד קטן.

הערכים העצמיים עצמם שווים לפיזור או לנורמה של הוקטורים העצמיים או כיווני הPC.

ומה הקשר לSVD?

נזכור את פירוק הSVD. במקרה זה X'X הוא בעצם VD^2V.

D בריבוע היא מטריצה אלכסונית p על p שעל האלכסון שלה נמצאים הערכים הסינגולריים בריבוע.

אבל זה בדיוק פירוק ערכים עצמיים שראינו! כלומר המטריצה מפירוק SVD היא בדיוק המטריצה V מפירוק הערכים העצמיים והמטריצה שאנחנו מחפשים.

והערכים העצמיים הם הערכים הסינגולריים בריבוע, אפשר להסתכל על אלה או על אלה בשביל לראות את פיזור ההטלות.

=== 5. PCA לא ליניארי ===

בשקף אחד אחרון נזכיר שPCA במובן מסוים היא שיטה ליניארית. מה קורה אם הנתונים לא מסתדרים בצורה כזאת.

:::

קודם היה כיוון, וקטור ברור בנתונים שלנו X1 וX2. מה יקרה עם X1 וX2 יוצרים מעגל? עצרו וחשבו מה PCA יעשה?

(להדגים) הוא ימצא את הכיוון הזה? הזה? כל אחד מהכיוונים האלה טוב כמו האחר, ועם זאת ברור שיש כיוון אחד שלאורכו הנתונים משתנים, הוא פשוט לא ניתן לביטוי כצירוף ליניארי של X1 ושל X2, הוא לא ניתן לביטוי כוקטור במערכת הצירים הזאת. אני מדבר כמובן על כך שהנתונים נוצרו באמצעות סיבוב, אפשר לחשוב שמישהו "הלך" כמה צעדים במימד יחיד של זווית, כל פעם שינה אותה קצת ויצר לאט לאט עיגול. אבל PCA לא ימצא את הכיוון הזה.

יש שיטות לnon-linear PCA, אתם מוזמנים לקרוא בצורה עצמאית על שיטות כמו Kernel PCA, ולאחר שנלמד על רשתות נוירונים, לקרוא על autoencoders, שבמובן מסוים מכלילים את PCA ליחסים לא ליניאריים.

עד כאן PCA. אני יודע שזה הרבה לקלוט, אבל מדען נתונים שיש באמתחתו את הכלי הזה, ניתוח הנתונים שלו יהיה איכותי יותר, ויהיה לו קל הרבה יותר ללמוד שיטות מתקדמות להורדת מימד.
:::
