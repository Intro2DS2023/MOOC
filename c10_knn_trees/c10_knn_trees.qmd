---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Local Modeling: KNN and Decision Trees"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Local Modeling: KNN and Decision Trees - Class 10

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## K-Nearest Neighbour (KNN) {.title-slide}

---

### Global vs local modeling

- So far we have learned two predictive modeling techniques: OLS regression and logistic regression 

- Common theme: Global, parametric models (+ probabilistic model for inference) --- lots of assumptions!

::: {.fragment}
- A different approach: *Local* modeling: I am similar to my neighbors
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אילו מודלים לחיזוי למדנו עד כה? רגרסיה ליניארית ורגרסיה לוגיסטית. המודלים האלה הם מודלים גלובליים. באיזה מובן? במובן שיש לשניהם סט של פרמטרים, סימנו אותם בבטא, תוצר הרגרסיה הוא הסט הנאמד בטא-האט, ובכל נקודה במרחב אנחנו מכילים עליה את נוסחת הרגרסיה, שהיא משוואה, יחידה, גלובלית. לדוגמא כשמידלנו את הסיכוי לפציינטים לחלות במחלת לב לא התאמנו נוסחה שונה לחולים עם היסטוריה משפחתית וחולים בלי היסטוריה משפחתית של המחלה. שם נוסף למודלים כאלה הוא מודלים פרמטריים, והם באים עם לא מעט הנחות כמו קשר ליניארי, ואם אנחנו רוצים לבצע הסקה גם הנחות סטטיסטיות.

צורת מחשבה שונה לחלוטין, היא מידול לוקאלי. פציינט שאין לו היסטוריה משפחתית של מחלות לב, שהוא בן 50 ומעשן - נחפש במדגם הלמידה פציינטים אחרים שדומים לו ונראה אם להם יש מחלת לב או לא, או האחוז מהם שיש להם מחלת לב. בגישה הזאת אני לא מסתכל אפילו על חולים בני 70 שיש להם היסטוריה משפחתית והם לא מעשנים -- הם לא רלוונטיים לפציינט שלי, ולכן אנחנו קוראים לגישה הזאת לוקאלית.
:::
:::

---

### Simple example: 1-nearest neighbor

1. Define a distance over the $\cal{X}$ space. For $x\in \mathbb{R}^p$ can simply choose the Euclidean distance: 
$$d(x,u) = \|x-u\|^2$$
2. For a prediction point (say $x_0 \in Te$), find its nearest neighbor in the $Tr$
$$ i_0 = \arg\min_i d(x_0,x_i)$$
3. Predict $x_0$ as the response at the nearest neighbor $\hat{y}_0 = y_{i_0}$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הדוגמה הפשוטה ביותר למודל nearest neighbor היא 1-nearest-neighbor או 1NN.

צריך להגדיר קודם איזושהי מטריקה של מרחק בין שתי תצפיות, בין שני וקטורי X. הבחירה הפשוטה ביותר אף שהיא לאו דווקא הכי נכונה לנתונים שלכם היא מרחק אוקלידי.

וכשמגיעה תצפית חדשה, אין "מודל". אנחנו מחשבים מיהו השכן הקרוב ביותר שלה במדגם הלמידה, לפי מטריקת המרחק שקבענו. חשוב לשים לב שאין כאן שום התחשבות בY, המטריקה מחושבת רק על וקטורי הX.

ואז, נחזה עבור התצפית את הY של השכן הכי קרוב. אנחנו יודעים אותו כי הוא במדגם הלמידה. 
:::
:::

---

### K-nearest neighbor (KNN) methods

- Repeat the same steps, but instead of finding the nearest neighbor only, find the $k$ nearest points in $Tr$ to $x_0$. Assume their indexes are $i_{01},\dots,i_{0k}$

::: {.incremental}
- For regression predict the average: 
$$\hat{y}_0 = \frac{1}{k} \sum_{j=1}^k y_{i_{0j}}$$

- For classification predict the majority: 
$$\hat{y}_0 = \left\{\begin{array}{ll} 1 & \mbox{if } \frac{1}{k} \sum_{j=1}^k y_{i_{0j}} > 1/2\\
0 & \mbox{otherwise}\end{array} \right.$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ההכללה המתבקשת של 1NN היא KNN. למה שאסתכל רק אצל השכן הכי קרוב לתצפית, אולי אסתכל אצל עשרת השכנים הקרובים ביותר שלה, ואמצע את הY שלהם באופן מסוים, אולי הדעה של עשרה שכנים היא חיזוי טוב יותר מהדעה של שכן יחיד.

ברגרסיה, החיזוי של תצפית יהיה ממוצע הY של K השכנים.

בקלסיפיקציה לשני קלאסים, החיזוי של תצפית יהיה אם הרוב שייך לקלאס 1, כלומר אם ממוצע הYים הוא מעל חצי. בפועל זה אומר תחזה אחת אם למעלה מחמישה שכנים מתוך עשרה הם גם אחת. אפשר גם לחזות את ההסתברות להיות אחת בקרב השכנים לצרכי חישוב עקומת ROC וAUC.

נשאלת השאלה אבל: באיזה K להשתמש?
:::
:::

---

### Reminder: SAHeart Data

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
from sklearn.model_selection import train_test_split

saheart = pd.read_table("../datasets/SAheart.data", header = 0, sep=',', index_col=0)
saheart.index = saheart.index.rename('index')

saheart_X=pd.get_dummies(saheart.iloc[:, :9]).iloc[:, :9]
saheart_X.columns = [*saheart_X.columns[:-1], 'famhist']
saheart_y=saheart.iloc[:, 9]
```

```{python}
print(saheart_X.head())
```

```{python}
print(saheart_y.head())
```

```{python}
SA_Xtr, SA_Xte, SA_Ytr, SA_Yte = train_test_split(saheart_X, saheart_y, test_size=0.2, random_state=41)

print(f'No. of train rows: {SA_Xtr.shape[0]}, no. train of cols: {SA_Xtr.shape[1]}')
print(f'No. of test rows: {SA_Xte.shape[0]}, no. test of cols: {SA_Xte.shape[1]}')
print(f'no. of obs in train y: {SA_Ytr.shape[0]}')
print(f'no. of obs in test y: {SA_Yte.shape[0]}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר בדאטא על פציינטים מדרום אפריקה, שבו אנחנו מעניינים לחזות האם יחלו במחלת לב או לא, 0 או 1. אלה הם הנתונים שלנו, ובשורה התחתונה יש לנו במדגם הלמידה כ370 תצפיות  עם 9 משתנים, ובמדגם הטסט כ90 תצפיות.
:::
:::

---

### KNN for SAHeart (Classification)

```{python}
#| code-line-numbers: "|1|5-7|10-11|12-13|14-15|"
from sklearn.neighbors import KNeighborsClassifier

ntr = SA_Xtr.shape[0]
nte = SA_Xte.shape[0]
tr_err = []
te_err = []
kvals = [1, 3, 5, 10, 50, 100, 200]

for k in kvals:
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(SA_Xtr, SA_Ytr)
    yhat_tr = knn.predict(SA_Xtr) > 0.5
    yhat = knn.predict(SA_Xte) > 0.5
    tr_err.append(np.sum(yhat_tr != SA_Ytr) / ntr)
    te_err.append(np.sum(yhat != SA_Yte) / nte)
```

::: {.fragment}
::: {.callout-important}
Is Euclidean distance suited for this task?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו עושים סימולציה של KNN עם K שונים.

אנחנו מייבאים את הקלאס KNeighborsClassifier מsklearn, מאתחילים וקטורי טעות לטריין ולטסט, ואת הK שנרצה להשתמש בו. לכל K אנחנו מאתחלים את הקלאס עם הארגומנט n_neighbors שווה k, ומתאימים על מדגם הלמידה. נשים לב שאם לא ציינו אחרת, המטריקה בשימוש היא מרחק אוקלידי, ותיכף נגיד על זה משהו.

מה בעצם מתבצע כאן? זה תלוי במימוש אבל בגדול, לא הרבה. הרי עיקר העבודה בKNN תהיה בשביל החיזוי, כשמגיעה תצפית חדשה, ואנחנו צריכים לחשב את המרחק בינה לכל התצפיות כדי לקבל את K השכנים הקרובים ביותר. מכל מקום זה מה שאנחנו עושים בשלב הזה לטריין ולטסט, נשים לב שהחיזוי הסופי הוא בדיוק לפי מה שלמדנו, האם ממוצע התצפיות גדול מחצי, כן או לא.

לבסוף אנחנו מחשבים את טעות הmissclassification עבור הטריין והטסט ומוסיפים לרשימות.

לפני התוצאה: כאמור, נשאל אם מרחק אוקלידי ראוי לבעיה שלנו. לא בטוח, מרחק אוקלידי, כמו שונות, מושפע מאוד מערכים קיצוניים, ובמקרה שלנו למשתנים שונות יש סקאלות שונות, כמו שניתן לראות בשקף הקודם. אז המרחק עצמו עלול להיות מושפע יותר ממשתנים מסוימים ולא מאחרים. לכן מומלץ לעשות סטנדרטיזציה אם יש צורך לפני שמריצים KNN.
:::
:::

---

```{python}
#| output-location: fragment
#| code-line-numbers: "|2-3|"
plt.figure(figsize=(4, 4))
plt.plot(kvals, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(kvals, te_err, color='navy', lw=2, label='test')
plt.ylim([0.0, 0.5])
plt.xlabel('k')
plt.ylabel('Misclass. Err.')
plt.title('KNN on SAheart')
plt.legend(loc="lower right")
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו מסרטטים את טעות החיזוי עבור הטריין והטסט כפונקציה של K.

דבר ראשון שראוי לשים לב אליו הוא טעות החיזוי במדגם הטריין עבור K = 1. הטעות הזאת היא אפס, משום שבמדגם הלמידה, איזו תצפית היא השכן הקרוב ביותר לתצפית נתונה? התצפית עצמה! אם נחזה עבור תצפית את הערך שלה מובן שלא יהיו טעויות. הדרך היחידה שיכולות להיות טעויות במקרה כזה היא אם יש ties, אם יש עוד תצפיות שיש להן וקטור X זהה בנתונים, והמודל החליט לבחור לא את התצפית המקורית כשכן הקרוב ביותר, אלא אחרת, מסיבות כלשהן, וערך הY של התצפית הזאת הוא לא ערך הY של התצפית המקורית. זה מכל מקום מה שיכול לקרות במימוש של sklearn.

באופן כללי לכן הקו הכתום הוא פחות מעניין, הקו הכחול הוא המעניין כי הוא מייצג את השגיאה עבור נתונים שהמודל לא ראה. אנחנו רואים כאן דפוס שיחזור על עצמו, ועוד ננתח אותו: עבור K קטן שגיאת הטסט גדולה, מגיעים לאיזשהו מינימום טעות באמצע, כאן סביב 50 שכנים, ואז הטעות מתחילה לעלות. אפשר לחשוב כבר למה זה קורה בצורה אינטואיטיבית: להתחשב רק במעט שכנים, זו החלטה מאוד רועשת, סביר להניח שמעט מאוד שכנים יכולים לטעות מאוד בחיזוי. להתחשב בהרבה שכנים זה גם לא טוב, ככל שנגדיל את הרדיוס שאומר מה זאת שכונה נקבל שכנים רחוקים כבר שלא רלוונטיים ואולי לא צריך להתחשב הם. והK האופטימלי הוא כנראה איפשהו באמצע. 
:::
:::

---

### Reminder: Netflix Data

```{python}
#| echo: false

ratings = pd.read_csv('../datasets/netflix/train_ratings_all.csv', header = None)
miss_cong = pd.read_csv('../datasets/netflix/train_y_rating.csv', header = None, names = ['score'])
movies = pd.read_csv('../datasets/netflix/movie_titles.csv', header = None, names = ['year', 'title'])

ratings[np.isnan(ratings)] = 0

netflix_X = ratings.iloc[:, :14]
netflix_X.columns = movies['title'][:14]
netflix_Y = miss_cong.iloc[:, 0]
```

```{python}
print(netflix_X.iloc[:5, :3])
```

```{python}
print(netflix_Y.head())
```

```{python}
NE_Xtr, NE_Xte, NE_Ytr, NE_Yte = train_test_split(netflix_X, netflix_Y, test_size=0.2, random_state=42)

print(f'No. of train rows: {NE_Xtr.shape[0]}, no. train of cols: {NE_Xtr.shape[1]}')
print(f'No. of test rows: {NE_Xte.shape[0]}, no. test of cols: {NE_Xte.shape[1]}')
print(f'no. of obs in train y: {NE_Ytr.shape[0]}')
print(f'no. of obs in test y: {NE_Yte.shape[0]}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נעשה את אותו הדבר על הדאטא של נטפליקס, רק ברגרסיה, עם 14 הסרטים עבורם יש לנו דאטא מלא. ניזכר שבמדגם הלמידה יש 8000 צופים שדירגו מ1 עד 5 14 סרטים, ובמדגם הטסט יש 2000 צופים. וניזכר שהפעם הY שלנו הוא כמותי, לא קטגוריאלי, אנחנו ברגרסיה.

כאן, למשל, אפשר כבר לשער שמרחק אוקלידי יהיה מתאים יותר, כי כל הסרטים מדורגים באותה סקאלה.
:::
:::

---

### KNN for Netflix (Regression)

```{python}
#| code-line-numbers: "|1|7|10-11|12-13|14-15|"
from sklearn.neighbors import KNeighborsRegressor

ntr = NE_Xtr.shape[0]
nte = NE_Xte.shape[0]
tr_err = []
te_err = []
kvals = [1, 3, 5, 10, 50, 100, 200, 500]

for k in kvals:
    knn = KNeighborsRegressor(n_neighbors = k)
    knn.fit(NE_Xtr, NE_Ytr)
    yhat_tr = knn.predict(NE_Xtr)
    yhat = knn.predict(NE_Xte)
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat - NE_Yte)**2) / nte))
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
התהליך כאן הוא כמעט זהה, רק שאנחנו משתמשים בקלאס KNeighborsRegressor. כשאנחנו חוזים אנחנו מקבלים כמות ממוצעת על פני השכנים בyhat. לבסוף טעות החיזוי שלנו היא הRMSE שמחושב כאן.
:::
:::

---

```{python}
plt.figure(figsize=(4, 4))
plt.plot(kvals, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(kvals, te_err, color='navy', lw=2, label='test')
plt.ylim([0, 1])
plt.xlabel('k')
plt.ylabel('RMSE')
plt.title('KNN on Netflix')
plt.legend(loc="lower right")
plt.show() 
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
גם כאן קיבלנו דפוס דומה, שגיאת החיזוי, כאן הRMSE, מתקרבת לאפס אבל היא לא זהותית אפס במדגם הטריין, כי יש כנראה ties, משתמשים עם דפוס דירוג זהה.

והקו הכחול שמייצג את הטסט הוא המעניין, והוא מראה שבסביבות K = 100 שכנים קרובים ביותר מגיעים למינימום שגיאה, מעבר לזה זה כבר לא משנה, אז כנראה שנבחר כאן למודל סופי K = 100.
:::
:::

---

### The problems with KNN? 

1. What is the appropriate distance metric?

2. If the data are "sparse" in the space, nearest neighbors are far and the results can be very bad

- *Curse of dimensionality*: if the dimension $p$ is high, data are by definition sparse

- KNN fails in these settings

::: {.fragment}
Interesting solution to both problems: Adaptive local methods 
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
היתרון הכי גדול של KNN הוא ברור: אין הנחות. כל לקוח של המודל יכול להבין את זה, קח את השכנים הקרובים ביותר לתצפית נתונה, ותחזה את הממוצע שלהם בשבילה.

אילו בעיות בכל זאת יש עם המודל הזה שבפועל לא משיג חיזויים טובים על נתונים מסוימים כמו מודלים אחרים שנלמד?

קודם כל בחירת המרחק. אמרנו שאין הנחות, אבל כאן דווקא כן מסתתרות הנחות, איך נראים הנתונים שמתאים להם מרחק אוקלידי או דווקא מרחק אחר.

בעיה משמעותית היא כשאנחנו רוצים לבצע KNN על דאטא שיכול להיות דליל באיזורים מסוימים. אם נקודה רחוקה מאוד מהשכן הכי קרוב שלה, KNN לא מביא את זה לידי ביטוי, למרות שהשכן אולי כבר לא רלוונטי. כשהמימד של הבעיה, מספר המשתנים p גבוה מאוד, אנחנו יכולים להיות בטוחים שהתופעה הזאת תקרה, והיא נקראת קללת המימד, או curse of dimensionality.אנחנו רגילים לחשוב על מרחב דו-מימדי או תלת-מימדי, אבל מרחב ממימד גבוה מאוד -- למשל אם היינו לוקחים את כל 99 הסרטים של נטפליקס -- יהיה מאוכלס בצורה דלילה מאוד. כל נקודה חדשה שנרצה לחזות עבורה, השכן הכי קרוב לה יהיה כבר די רחוק. וכל שיטה מבוססת שכנים תיכשל במצב זה.

רעיון אחר שמטרתו לטפל בדיוק בבעיה הזאת של קללת המימד: נגדיר מה זו שכונה של תצפית, לא באמצעות איזושהי מטריקת מרחק כפי שעשינו, אלא בהסתכלות על הנתונים עצמם, בצורה אדפטיבית. ננסה לחלק את המרחב לחלקים שבתוכם Y, המשתנה שאנחנו רוצים לאמוד, משתנה כמה שפחות, ההתפלגות שלו כמה שיותר אחידה. כעת, כשתגיע תצפית חדשה נסווג אותה לשכונה שמתאימה לה, שכונה שאנחנו יודעים מה הY שם פחות או יותר, וזו יכולה להיות שכונה גדולה, קטנה, זה יוחלט בצורה אדפטיבית כלומר על-ידי הצצה על Y, מה שלא עשינו עם KNN, שם לא הבטנו בכלל בY.

:::
:::

---

## Decision Trees {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עצי החלטה הם דוגמא מצוינת לבחירת מודל בצורה אדפטיבית, על-ידי הצצה אל Y תוך כדי בניית המודל. נראה קודם איך נראה עץ החלטה בפועל, נראה מה הפרמטר הכי חשוב שמשפיע על הביצועים שלו, ורק אחר-כך נשאל איך לבנות עץ החלטה. יש הרבה סוגים של עצי החלטה, זה תחום נרחב, אנחנו נתמקד בגירסה אחת, הגירסה הקלאסית של עצי החלטה.
:::
:::

---

### Adaptive local methods: Trees

- The idea: split the space $\cal{X}$ into *neighborhoods* by recursive partioning

- Each time: pick a region and split it into two (or more) regions

- Can be described using a tree --- binary tree if all splits are in two. Titanic example: 

::: {.fragment}
![](images/CART_tree_titanic_survivors.png)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאמור, במבט על, נרצה לחלק את מרחב הנתונים, מרחב X, לשכונות, וכמו שרשום כאן נעשה זאת בצורה רקורסיבית או אדפטיבית. כל פעם נגיע לאיזור נתון, ונחלק אותו לשניים בצורה שתחלק את Y לשני איזורים ששונים זה מזה כמה שיותר. תיכף ניתן הגדרה מדויקת. נזכיר רק שאם מחלקים לשני איזורים מדובר בעץ בינארי, שראיתם אולי בקורסים אחרים, יש כמובן גירסאות של עצים עם חלוקה ליותר משני ענפים.

אז איך עץ נראה על נתוני הטיטאניק שראינו. השאלה הראשונה מסתבר שמחלקת את המרחב הכי טוב לפי הגדרה שעוד נראה, היא מה מין הנוסע. אם הנוסע ממין נקבה, נראה שאין צורך ביותר שאלות, 36 אחוזים מהמדגם היו נשים והם שרדו בסיכוי 0.73. אם הנוסע ממין זכר לעומת זאת, נראה שצריך לשאול עוד שאלות. השאלה הבאה החשובה ביותר היא מה גיל הנוסע האם הוא מעל 9.5. אם כן, כלומר הגענו ל"שכונה" של נערים ומבוגרים ממין זכר, אין צורך להמשיך יותר, 61% מהמדגם שייכים לשכונה הזאת והם שרדו בסיכוי 0.17. אם לא גדול, כלומר ילד קטן, נשאל עוד שאלה על המשתנה מספר אחים ורק אז נגיע לסוף העץ.

זהו עץ לקלסיפיקציה. מה נחזה בעץ כזה? נחזה את מה שראינו בעלים הסופיים של העץ. כמו בKNN, אם נרצה הסתברויות חזויות, נחזה את ההסתברות לשרוד בכל עלה, שהיא אחוז השורדים בעלה הזה, בשכונה הזאת. אם נרצה קלאס סופי נחזה אולי האם הסיכוי הזה לשרוד בעלה גדול מחצי או לא, כאן למשל עבור נשים הן שרדו בסיכוי 0.73 ולכן נחזה עבור תצפיות חדשות של נשים, שלא ראינו, שישרדו.

עוד מושג חשוב: עומק העץ. מה עומק העץ המקסימלי כאן? 3. כלומר כל תצפית תגיע לסוף העץ ותקבל חיזוי עם מקסימום 3 שאלות.

מה לא ראינו? איך העץ הזה מבצע על נוסעי טיטאניק שלא ראינו במדגם טסט. אולי צריך לשאול פחות שאלות? אולי יותר?
:::
:::

---

### Tree for SAHeart (Classification)


```{python}
#| code-line-numbers: "|1|3-4|5|"
#| output-location: fragment

from sklearn.tree import DecisionTreeClassifier, plot_tree

tree = DecisionTreeClassifier(max_depth = 2)
tree.fit(SA_Xtr, SA_Ytr)
plot_tree(tree, feature_names=SA_Xtr.columns)
plt.show() 
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה את המימוש של sklearn על הנתונים של חולי לב מדרום אפריקה, שמזכיר מאוד כל מימוש שראינו עד כה.

אני מייבא את הקלאס DecisionTreeClassifier, אני מאתחל אותו, ונשים לב שאני מבקש max_depth שווה ל2, כלומר לכל היותר נשאל שתי שאלות על כל פציינט כדי להגיע לחיזוי. גידול העץ עצמו נעשה עם המתודה fit, ועוד לא דיברנו על איך.

באוביקט tree קיים העץ ואני משתמש כאן במתודה plot_tree כדי לצייר אותו.

מה העץ שלנו אומר?

קודם כל הוא מחלק את המדגם לפציינטים בני למעלה מ50 ולפציינטים מתחת גיל 50. שאלה שניה עבור הפציינטים המבוגרים יותר היא מידת השימוש בטבק כלומר עישון. אם הפציינט מעשן מעל כמות מסויימת הגענו לעלה סופי, יש בו 95 פציינטים במדגם, שמתוכם 65 חלו במחלת לב, כלומר סיכוי של כשני שליש.

בצד האחר של העץ, עבור פציינטים צעירים יותר, השאלה היא לגבי ציון בשאלון אישיות שנועד לבחון אם אתה type A. טייפ A זה טיפוס לחוץ, קשה, הישגי. אם אתה ברמה נמוכה במדד הזה, אתה שייך לאיזור או שכונה סופיים שם הסיכוי לחלות במחלת לב קטן יחסית, ואם אתה ברמה גבוהה במדד הזה, גילינו תת-איזור שכזה עם 14 פציינטים, עם סיכוי גבוה מאוד לחלות, למעלה מ70 אחוז.

שימו-לב עוד לשני דברים: כמה העץ קל לפירוש, לא צריך להיות מדען נתונים כדי להבין את הלוגיקה שבאלגוריתם הזה מה שמסביר הרבה מהפופולריות שלו. וגם, מדובר באלגוריתם שמאוד קל לממש. גם אם העץ נבנה בפייתון, בסופו של דבר בכל שפת מחשב אפשר לממש אותו למשל עם פרוצדורות if else. זה שתי שאלות על תצפית ונותנים לה חיזוי, פשוט מאוד.
:::
:::

---

```{python}
ntr = SA_Xtr.shape[0]
nte = SA_Xte.shape[0]
tr_err = []
te_err = []
ds = [2, 3, 5, 7, 10, 15]

for depth in ds:
    tree = DecisionTreeClassifier(max_depth = depth)
    tree.fit(SA_Xtr, SA_Ytr)
    yhat_tr = tree.predict(SA_Xtr) > 0.5
    yhat = tree.predict(SA_Xte) > 0.5
    tr_err.append(np.sum(yhat_tr != SA_Ytr) / ntr)
    te_err.append(np.sum(yhat != SA_Yte) / nte)
```

```{python}
#| echo: false

plt.figure(figsize=(4, 4))
plt.plot(ds, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(ds, te_err, color='navy', lw=2, label='test')
plt.ylim([0.0, 0.5])
plt.xlabel('depth')
plt.ylabel('Misclass. Err.')
plt.title('Trees on SAheart')
plt.legend(loc="lower right")
plt.show() 
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל עדיין לא ענינו על איך בוחרים את הפרמטר העיקרי כאן של max_depth, נעשה תרגיל דומה למה שעשינו עבור ערכים שונים של K עם KNN. כאן אנחנו משתמשים בערכים שונים מ2 עד 15, מאמנים על הטריין, חוזים על הטסט, ושואלים מהי שגיאת החיזוי, על הטריין ועל הטסט.

התקבל דפוס שמזכיר מאוד את מה שראינו בKNN: עבור מדגם הלמידה עוד ועוד שאלות לא יכולות להזיק, בסוף נהיה נורא נורא ספציפיים, ונחזה עבור כל תצפית בדיוק את עצמה ונגיע ל0 אחוז שגיאת חיזוי. אבל זה לא מה שמעניין אותנו, מה שמעניין יותר הוא מדגם הטסט ושוב אנחנו רואים שעץ שטוח מדי הוא פשטני מדי, לא שואלים מספיק שאלות. ואילו עץ עמוק מדי הוא ספציפי מדי למדגם הטריין, קורה מה שעוד נכנה כoverfitting, ומקבלים גם כן שגיאה גדולה. איפשהו באמצע כאן בעומק 3 שאלות מתקבלת התוצאה הטובה ביותר.

עוד חשוב להדגיש שהעקומה שהתקבלה לא חלקה, כי אנחנו מאוד תלויים בחלוקה ספציפית של הנתונים לטריין ולטסט. סביר להניח שאם נמצע את העקומה הזאת על כמה חלוקות, כל פעם נחלק אחרת, נקבל עקומה מעט יותר חלקה עם תשובה ברורה יותר, עבור איזה עומק מגיעה שגיאת החיזוי של הטסט למינימום, על פני הרבה חלוקות.
:::
:::

---

### Tree for Netflix (Regression)

```{python}
from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(max_depth = 2)
tree.fit(NE_Xtr, NE_Ytr)
plt.figure(figsize=(10, 6))
plot_tree(tree, feature_names=NE_Xtr.columns)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עץ החלטה הוא כאמור גם עבור רגרסיה, למשל על הנתונים של נטפליקס. כאן אני עושה זאת עם הקלאס DecisionTreeRegressor, גם כן עם max_depth 2, ומתקבל העץ הבא:

שאלה ראשונה שנשאל כדי לחזות את הציון של איזו מין שוטרת היא האם אהבת או לא את סוויט הום אלבמה. אם כן נשאל האם אהבת גם את מה נשים רוצות, קומדיה רומנטית עם מל גיבסון והלן האנט. אם ענית כן לשתי השאלות נחזה שתאהבי מאוד את איזו מין שוטרת עם ציון 4.29. שכונה אחרת היא מי שתענה שלא אהבה את סוויט הום אלבמה, ולא אהבה את אישה יפה, עבורה נחזה ציון נמוך יחסית של 3.11.

נשים לב לשני דברים חשובים כאן: אחד שבאמת נראה שהמודל חילק את המרחב לארבע שכונות די זרות, הוא חוזה ציון שונה לחלוטין לכל אחת. הדבר השני הוא שהחיזוי לכל קבוצה כזאת ברגרסיה, הוא מספר אחד ויחיד, לכל מי שהגיע לעלה הזה, לדוגמא 3.11. זה נותן לכם איזשהו רמז למה מודל של עץ הוא בכל זאת מאוד לא גמיש ועל דרכים לשפר אותו.
:::
:::

---

```{python}
ntr = NE_Xtr.shape[0]
nte = NE_Xte.shape[0]
tr_err = []
te_err = []
ds = [2, 3, 5, 7, 10, 15]

for depth in ds:
    tree = DecisionTreeRegressor(max_depth = depth)
    tree.fit(NE_Xtr, NE_Ytr)
    yhat_tr = tree.predict(NE_Xtr)
    yhat = tree.predict(NE_Xte)
    tr_err.append(np.sqrt(np.sum((yhat_tr - NE_Ytr)**2) / ntr))
    te_err.append(np.sqrt(np.sum((yhat - NE_Yte)**2) / nte))
```

```{python}
#| echo: false

plt.figure(figsize=(4, 4))
plt.plot(ds, tr_err, color='darkorange', lw=2, label='train' )
plt.plot(ds, te_err, color='navy', lw=2, label='test')
plt.ylim([0.3,1.3])
plt.xlabel('depth')
plt.ylabel('RMSE')
plt.title('Trees on Netflix')
plt.legend(loc="upper left")
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו עושים את התרגיל של בחירת עומקים שונים וציור הטריין והטסט error עבורם אנחנו מקבלים את הדפוס המוכר, נשים לב רק שכאן הטעות היא הRMSE. בכל מקרה עבור מדגם הטסט הזה הגענו לRMSE הכי נמוך של 0.85 בערך. אם תיזכרו בתוצאה של רגרסיה ליניארית עבור חלוקה זו לטריין וטסט, זה קצת יותר גבוה, לא נראה ששיפרנו בהרבה.
:::
:::

---

### Rectangular Regions

![](images/recursive_partitioning.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מילה אחרונה על העץ שלנו. נשים לב שכל תצפית בסופו של דבר שייכת לאיזור אחד בלבד, כלומר הם זרים זה לזה, וכל המרחב "מתכסה". יותר מזה, אם נשרטט את המרחב של X כמו בדוגמה כאן שיש רק שני משתנים X1 וX2, נראה שאיזורי החלוקה הם מלבניים. לדוגמא אם X1 קטן מ1 נחזה איזושהי כמות בטא1, המשמעות היא שמשמאל ל-1 אנחנו כבר החלטנו, נוצר מלבן. ואת האיזור שהגענו אליו מימין ל1 נחלק אולי למלבנים נוספים. בתלת מימד האיזורים יהיו כמובן, תיבות. לחלוקה כזאת אנחנו קוראים partition.
:::
:::

---

## Building a CART {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה כעת איך נבנה עץ לרגרסיה. הדרך שבה נבנה עץ לקלאסיפיקציה מאוד דומה, נגיד גם על זה כמה מילים.
:::
:::

---

### Defining a decision tree algorithm

There are three main aspects to designing a decision tree algorithm for classification or regression:

1. How do we choose a split at each node of the tree?
2. How do we decide when to stop splitting?
3. How do we fit a value $\hat{y}$ for each terminal node (*leaf*)?

::: {.fragment}
Some well known decision tree algorithms: 

- ID3, C4.5, C5.0: for classification only, invented in the CS/machine learning community

- Classification and regression trees (CART): invented in the statistics community

- We are going to mostly describe CART, which is the basis for modern methods we discuss later
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה עושה בעצם העץ? הוא עושה חלוקה של המרחב של X לשכונות או איזורים זרים, בכל איזור ואיזור Y מתפלג כמה שפחות. זה הפלט שהיינו רוצים לקבל מאלגוריתם שבונה עץ.

אז הגדרנו כבר רעיון כללי ונשאר לנו לברר כמה פרטים ספציפיים:
 
 שאלה ראשונה, איך תתבצע החלוקה לשניים בכל צומת. שאלה אחרת היא מתי מפסיקים. בדוגמאות שלנו תמיד הגדרנו לעץ עומק מקסימלי אבל יכולות להיות דרכים אחרות, אולי איזשהו קריטריון שמסתכל על הדאטא עצמו ולא פרמטר קשוח. ועל השאלה השלישית ענינו גם כן, מה יהיה הערך החזוי עבור תצפית שמגיעה לעלה, אבל רמזנו שיכולות להיות אולי תשובות חכמות יותר.

 שלוש ההחלטות האלה מגדירות סוגים שונים של עצים, אני עושה כאן ניים דרופינג לכמה מהם. העץ הקלאסי שגם ממומש בsklearn הוא CART, ראשי תיבות של classification and regression tree, ואותו נלמד כעת.
:::
:::

---

### CART for regression: splitting process

::: {.incremental}
- Criterion: Minimize RSS on training. 

- Given set of $r$ observations in current node, define for a variable $j$ and possible split point $s$: 
$$L(j,s) = \{i\leq r: x_{ij} \leq s\}\;,\;\; R(j,s) = \{i\leq r: x_{ij} > s\}$$
$$\bar{y}_L =\frac{\sum_{i \in L(j,s)} y_i}{|L(j,s)|}\;,\; \bar{y}_R=\frac{\sum_{i \in R(j,s)} y_i}{|R(j,s)|}$$
$$RSS(j,s) = \sum_{i \in L(j,s)} (y_i - \bar{y}_L)^2 + \sum_{i \in R(j,s)} (y_i - \bar{y}_R)^2$$

- And find the pair $j, s$ which minimize this RSS among all possible pairs --- this is the split we do

- Split the node into two according to the chosen split and continue
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז השאלה הראשונה היא: מהו הקריטריון לחלוקה. בעץ רגרסיה CART הקריטריון הזה יהיה הRSS, בדומה לקריטריון ברגרסיה ליניארית שכבר נתקלנו בו. למה הכוונה.

הגענו לצומת מסוים בעץ (להדגים) ויש לנו קבוצה מסוימת של תצפיות שאנחנו רוצים לחלק. הצעת חלוקה מסוימת אומרת להסתכל על משתנה j ועל ערך מסוים של המשתנה הזה s. נשים את כל התצפיות שקטנות במשתנה הזה מS בLeft, שכונה שמאלית, ואת כל התצפיות שגדולות במשתנה הזה מS בRight, שכונה ימנית. כאן אנחנו מציצים בY ובודקים מה הממוצע שלו בשכונה השמאלית, ובשכונה הימנית. נרצה שכל שכונה תהיה הומוגנית כמה שיותר, כלומר קרובה לממוצע שלה כמה שיותר, לכן הקריטריון להביא למינימום הוא סכום הRSS של שתי השכונות, סכום השגיאות הריבועיות של התצפיות מהממוצע שלהן בשכונת שמאל, ועוד סכום השגיאות הריבועיות של התצפיות מהממוצע שלהן, בשכונה הימנית.

עכשיו זאת הצעת חלוקה אחת. כדי למצוא את החלוקה הטובה ביותר בנקודה שהגענו אליה, נצטרך לעבור על כל המשתנים האפשריים, ובכל משתנה על כל הערכים שאפשר לחלק בהם. כלומר בכל צומת האלגוריתם שלנו בזמן אימון עושה די הרבה, צריך לעבור על כל הזוגות האפשריים של j, s. הזוג שיביא למינימום את הRSS הוא הזוג הנבחר.

כעת נעשה את הפיצול, ונמשיך בצורה רקורסיבית בכל אחד משני הצמתים שיצרנו. לדוגמא בדאטא של נטפליקס, האלגוריתם הביט בכל 14 הסרטים, ובכל סרט בכל ציון, ומצא שהחלוקה שתביא למינימום את הRSS, כלומר היא מחלקת לשתי קבוצות ההומוגניות ביותר של אנשים שאוהבים את איזו מין שוטרת, ואנשים שלא אוהבים את איזו מין שוטרת -- היא החלוקה אם נתת ציונים גבוהים או נמוכים לסרט סוויט הום אלבמה.

נשים לב עוד, שאחרי החלוקה התצפיות בכל קבוצה כבר לא משפיעות יותר על החלוקה בקבוצה האחרת. זהו. אין להן יותר השפעה, לא נחזור אחורה בעץ קלאסה אולי היתה יכולה להיות חלוקה טובה יותר. כך שמדובר באלגוריתם greedy, חמדן.
:::
:::

---

### CART for regression: stopping criteria

- Why limit tree size?

::: {.incremental}
- Overfitting, computation,...

- In the examples above: *max_depth* of tree

- Other options: size of nodes not too small, improvement in RSS not too small,...

- Interesting approach of CART: grow a very big tree and *prune* it to smaller tree using test set performance (actually cross-validation, which we have not yet discussed)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ההחלטה השניה ברשימה שלנו היתה מתי להחליט שדי, אפשר לעצור לגדל את העץ. למה בכלל להפסיק לגדל את העץ? למה לא לתת לו לגדול ולגדול?

ראינו בנתונים למה לא, בדרך כלל העץ יעשה אוברפיטינג למדגם הלמידה, והחיזוי שלו יהיה באיכות נמוכה על נתונים שלא ראה. סיבה אחרת יכולה להיות חישובית, האימון וגם אחר כך החיזוי ייקחו יותר זמן ככל שנאפשר עצים עמוקים יותר.

מכל מקום, ראינו דרך אחת להחליט, והיא פרמטר גלובלי של max_depth מקסימום עומק לעץ. בצורה כזאת נקבל עצים שהם balanced, כל התצפיות יעברו דרך אותו מספר שאלות בטרם יקבלו חיזוי.

דרך אחרת יכולה להיות בכל צומת וצומת בעץ לשאול האם שווה להמשיך, כלומר להחליט בצורה מקומית. אפשר לחשוב גם כאן על עוד פרמטר כמו מספר מינימלי של תצפיות בעלה, לדוגמא 10. אני לא רוצה לעשות עצים שבסוף מגיעים להחלטה על פחות מ10 תצפיות. אפשר לחשוב על להביט בקריטריון RSS למשל, ולראות שהוא משתפר מעל איזשהו ערך סף. אבל שימו לב מה אמרתי, האם הוא משתפר מעבר לאיזשהו סף, לא משתפר באופן כללי כי מה תמיד מובטח לנו, כמו ברגרסיה ליניארית? הRSS על מדגם הלמידה עם עוד שאלות, או עוד משתנים, יכול רק להשתפר. אז נקפיד לקבוע איזשהו ערך שרק אם הוא משתפר יותר ממנו שווה להמשיך לפצל. בכל מקרה בדרך זאת יכולים להיווצר עצים לא מאוזנים כמו העץ של הטיטאניק שראינו. אם הנוסע ממין נקבה ראינו שאין צורך לשאול יותר שאלות, ואם הוא ממין זכר שאלנו עוד שאלות.

בCART יש רעיון נוסף, והוא לגדל את העץ כמה שיותר, ואז בשלב נוסף לבצע לו קטימה או pruning. לא ניכנס לעומק הpruning, אבל בגדול מדובר עכשיו להסתכל שוב על כל הצמתים של העץ ולהסתכל האם ההפחתה שקיבלנו בRSS הייתה שווה את זה, כשאנחנו נותנים עוד איזשהו עונש של מורכבות, complexity על מספר העלים הסופיים במודל. ניקח את העץ הקטום שמביא למינימום את הקריטריון החדש הזה אחרי התחשבות בעונש הזה על מספר גדול מדי של איזורים.
:::
:::

---

### CART for regression: fits at leaves

- Similar to OLS, we want to estimate $\hat{y}(x) \approx E(y|x)$ 

- We interpret the splitting as finding *homogeneous areas* with similar $y$ values in our data, hence hopefully similar $E(y|x).$

- Consequently, given a leaf (terminal node) with set of observations $Q \subseteq \{1,\dots,n\}$, we estimate: 
$$\hat{y} = \bar{y}_Q = \frac{\sum_{i \in Q} y_i}{|Q|}$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ההחלטה השלישית והאחרונה ברשימה שלנו היתה מה בסופו של דבר אנחנו חוזים. ראינו כבר שבעץ רגרסיה נחזה את הממוצע. אבל זאת לא החלטה שרירותית, כמו ברגרסיה ליניארית אנחנו בעצם ממדלים את התוחלת המותנית של Y אחרי שראינו את X.

ואם Y משתנה כמה שפחות כי העלה שלנו מאוד הומוגני, התוחלת הזאת היא קבועה פחות או יותר.

ואנחנו יודעים שבכל עלה ועלה, האומד הכי טוב תחת הפסד ריבועי כמו שלנו לתוחלת, הוא ממוצע המדגם שהגיע לעלה.
:::
:::

---

### CART and others for classification

- Various splitting criteria: Gini, information gain, log-likelihood, all give similar trees

- Not a good idea: using misclassification rate as splitting criterion

- Stopping criteria: similar ideas to regression

- Fits at leaves: usually empirical % of classes (or majority if need hard classification)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך בונים עץ לקלסיפיקציה?

יש הרבה אפשרויות לקריטריונים של חלוקה, כאן הם ייקראו אולי מדדים לimpurity, עד כמה שתי השכונות השמאלית והימנית הן pure או טהורות, כל התצפיות מקבלות בהן אותו קלאס. יש מדדים שונים שלא נעבור עליהם, כפי שראיתם המדד הדיפולטיבי בsklearn הוא כרגע הג'יני.

גם מבחינת קריטריון עצירה אין הרבה שינוי מעץ לרגרסיה, ראינו שגם כאן אפשר להגדיר פרמטר גלובלי של max_depth, ואפשר גם להחליט לוקלאית כמו מינימום מספר תצפיות לעלה.

וכשמגיעים לעלה, כמו שראינו, אם רוצים סיווג קשיח חוזים פשוט את הרוב --ושימו לב שכאן קל מאוד להכליל להרבה קלאסים. אם רוצים הסתברות אפשר להתאים את אחוז התצפיות שהן קלאס "1", ואז יש לזה גם פרשנות יפה, זה בדיוק התוחלת המותנית של Y בהינתן X כי Y הוא משתנה ברנולי שמקבל 0 או 1, והתוחלת שלו היא ההסתברות עצמה.
:::
:::

---

## Closing Remarks {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסיים את הדיון בעצים במספר יתרונות וחסרונות שלהם.
:::
:::

---

### Important properties of trees

#### 1. categoraical features

- Real life data often includes categorical features that have many values but are important for prediction, like: 
  1. City of residence
  2. University/department
  3. Customer class

::: {.fragment}
- CART always does binary splits. For a categorical variable with $K$ values ${\cal G} = \{g_1,\dots,g_K\}$  it divides $\cal G$ into two groups $\cal G_1, \cal G_2$ so that:
$$L(j) = \{i : x_{ij} \in \cal G_1\}\;,\;\;R(j) = \{i : x_{ij} \in \cal G_2\}.$$

- C4.5/C5.0 do multi-way non-binary splits

- Presents interesting computational and statistical challenges
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עצים טובים מאוד כשיש בנתונים משתנים קטגוריאליים, שיש בהם הרבה קטגוריות אבל הם לא מספרים, לדוגמא העיר שבה גר פציינט או סוג ההשכלה שלו, איזה תואר למד. זה בניגוד לשיטות שלמדנו עד עכשיו כמו רגרסיה או KNN, שלא טבעי להן לקבל משתנים קטגוריאליים.

לגבי עצים, אנחנו יכולים רק לעשות רק פיצול בינארי. אנחנו לא יכולים לפצל משתנה קטגוריאלי לפי איזשהו סף, אבל אנחנו כן יכולים לחלק אותו לשתי קבוצות של קטגוריות G1 ו-G2. כל תצפיות שיש להן קטגוריה ששייכת לG1 תלך ימינה, וכל תצפיות שיש להן קטגוריה ששייכת לG2 תלך שמאלה.

יש אלגוריתמים אחרים למשל שמחלקים את הצומת ל5 ענפים אם יש 5 קטגוריות כמו C5, שלא דיברנו עליו.

מה לא סיפרתי לכם? בעיקר איך העץ מחלק לשתי חלוקות של קטגוריות, הרי אם יעבור על כל החלוקות האפשריות של שתי קבוצות של קטגוריות זה יהיה סדר גודל של 2 בחזקת K קטגוריות, וזה צריך לעשות בכל צומת וצומת. אז זה באמת לא מה שCART עושה, מי שרוצה יכול לקרוא עוד על זה בספרים מתאימים, באופן כללי משתנים קטגוריאליים ואיך להתמודד איתם זה נושא מרתק סטטיסטית.
:::
:::

---

### Important properties of trees:

#### 2. missing data

- Many methods struggle dealing with missing data, trees have nice solutions

- Solution 1 (CART): in addition to the split I want to do, find similar *surrogate splits* on other variables, and if I don't see $x_{ij}$ I can use surrogate split on $x_{ik}$

- Solution 2 (C4.5): if I want to split on feature $j$ and I don't know $x_{ij}$, send observation $i$ both left and right


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
יתרון חשוב נוסף לעצים הוא כמה טבעי להזין להם משתנים עם ערכים חסרים. כבר אמרנו שבקורס הזה לא ניכנס לעובי הקורה של תצפיות חסרות, אבל בשביל הרבה מדעני נתונים תצפיות חסרות הן מציאות כאובה שיש להתמודד איתה. ועצים מתמודדים איתה ללא קושי.

יש כל מיני אסטרטגיות של עצים לטפל במשתנים עם ערכים חסרים, CART למשל, בכל פיצול לא מחשב רק את הפיצול הטוב ביותר, אלא גם את הבאים אחריו, פיצולים שנקראים surrogate. באופן זה אם המשתנה הטוב ביותר לחלק על פיו הוא משתנה שיש לו גם ערכים חסרים, ותגיע לחיזוי תצפית עם ערך חסר במשתנה הזה, עדיין היא תוכל להתמיין ימינה או שמאלה בהתאם למשתנים אחרים בהם אין לה ערכים חסרים.

דרך אחרת של עץ מסוג C4.5, היא לשלוח במורד העץ תצפית עם ערך חסר בפיצול שלנו, גם ימינה וגם שמאלה. ואז כמובן האלגוריתם צריך להתמודד עם מצב שבו אותה תצפית הגיעה לעלים שונים, אבל לא ניכנס לזה.
:::
:::

---

### Summary: Decision Trees

Advantages:

1. Intuitive and appealing: divide the space into *flexible* neighborhoods
2. Flexible: categorical variables, missing data, regression or classification, big or small,...
3. Big trees are a very rich class of models: can describe well many true models for $E(y|x)$.

Disadvantages:

1. Intuitive appeal is misleading: very unstable and high variance 
2. Not a good prediction model: a single tree is usually not competitive!

::: { .fragment}
Conclusions and next steps:

1. We do not really want to use trees as our prediction models
2. Can we take advantage of their good properties and mitigate the problems?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם עצים. מהם היתרונות?

קודם כל התוצר עצמו של האלגוריתם מאוד אינטואיטיבי, כל לקוח יכול להבין אותו, מדובר בתרשים זרימה, אוסף של שאלות שבסופן מתקבלת תשובה.

מבחינת דאטא סיינס עץ גמיש מאוד, מתאים לנתונים בפרקטיקה, נתונים עם משתנים קטגוריאליים, נתונים חסרים, נורא בקלות אפשר לעבור עם אותה פרדיגמה בין קלאסיפיקציה לרגרסיה. אימון וחיזוי על עץ גם נחשבים למהירים מאוד, כך שהוא מהיר יחסית לנתונים גדולים.

לא פחות חשוב מזה, אם תיזכרו ברגרסיה ליניארית או לוגיסטית, אלה מודלים פרמטריים, שמניחים יחס בין מרחב X לY מאוד נוקשה, יחס ליניארי. עץ לא נשאר בטווח הליניארי של יחסים, הוא יכול למדל יחסים מאוד לא ליניאריים אם הוא עמוק מספיק. אם תחשבו על עץ עם max_depth 10 למשל, לכמה שכונות הוא מחלק את הנתונים שלנו? 2 בחזקת 10, כלומר כאלף שכונות שונות, מה שיכול לתת מודל עשיר למדי.

מה בכל זאת בעייתי בעץ?

קודם כל, מבנה העץ אולי אינטואיטיבי, אבל עץ הוא גם נורא נוקשה, וסובל משונות גבוהה. הכוונה לתופעה שאם אשנה כמה תצפיות, אעביר אותן ממדגם הלמידה למדגם הטסט למשל, אני הרבה פעמים אראה שינוי גדול בעץ שנוצר, יכול להיות שזה לא ישנה בהרבה את טיב האמידה אבל לקוח שיביט בעץ יראה תרשים זרימה שונה לגמרי. אתם מוזמנים לנסות את זה בעצמכם על הנתונים שלנו, לעשות חלוקות קצת אחרות של הדאטא ולראות שעלולים להתקבל עצים שונים לחלוטין, לא משנה כמה העצים שראינו היום היו אינטואיטיביים והם היו.

בעיה חמורה יותר היא שעץ הוא פשוט לא מודל חיזוי טוב. בפועל, אנחנו רואים בהשוואה למתודות אחרות, שאם שגיאת החיזוי על נתונים שהמודל לא ראה היא מה שמעניין אותנו, נדיר שעץ יהיה המודל הטוב ביותר. הסיבה נעוצה בכך שלמרות כל מה שאמרנו עץ אחד מגיע ממרחב עצום של עצים שאנחנו מחפשים בו בצורה גרידית, ובגלל זה מקבלים מודל אחד כנראה לא אופטימלי, עם שונות גבוהה מאוד.

ואז נשאלת השאלה למה בכלל למדנו על עצים אם ככה?!

אנחנו נראה שעץ בודד הוא לא מודל טוב אבל כסאב-רוטינה, כמודל אחד מתוך אוסף של מודלים או יער, עץ הוא מודל מצוין. בחלק הבא ננסה לשמור על היתרונות של העץ ונראה איך להתמודד עם החסרונות שלו, בעיקר השונות הגבוהה, באמצעות מיצוע או קומבינציה, של מספר עצים.

:::
:::
