=== 1. מבוא למודלים לחיזוי ===

אנחנו מתחילים ביחידה היום בעצם את החלק השני של הקורס. אחרי שעסקנו בהבנת נתונים, אקספלורטורי דאטא אנליסיס ובדיקת השערות, אנחנו רוצים להשתמש בנתונים האלה, לבניית מודלים לחיזוי. נתחיל היום במודלים ליניאריים שיש מאחוריהם לא מעט הנחות, נמשיך למודלים של מאשין לרנינג מודרניים יותר שמאפשרים גם מידול של יחסים לא ליניאריים בכלל, ונגיע למודלים של רשתות נוירונים או למידה עמוקה, מהשנים האחרונות ממש.

חשוב לי להדגיש שוב שאנחנו מסתכלים על כל שיטה ושיטה בהיי לבל, על מנת לתת מבט רחב על התחום. ברור שמי שרוצה ממש לעסוק בתחום, אני ממליץ לו לקרוא עוד הרבה יותר ולקחת עוד קורסים בנושא.
:::

נתחיל במבט על על מה אנחנו מנסים לעשות כאן - מה זה מודלים לחיזוי או predictive modeling?

:::

הרעיון הבסיסי: יש לנו תצפית לחיזוי שנסמן אותה בX, וX הוא וקטור ממימד p, כלומר יש לנו p משתנים על התצפית שלנו. וY הוא סקלאר.

אנחנו מניחים שY הוא פונקציה של X והמטרה שלנו היא לבנות מודל שיעשה קירוב ליחס הזה.

נזכיר שמות שונות לX כדי שנראה שאנחנו מדברים על אותו דבר: X יכול להיות משתנים, מנבאים, פיצ'רים, משתנים אקסוגניים.

באופן דומה Y נקרא לפעמים משתנה התגובה, המשתנה התלוי, המשתנה האנדוגני -- בקורס שלנו, זה בסדר שיהיו שמות שונים.

:::

אם נצליח לבנות מודל איכותי, זה ימלא שתי מטרות:

מטרת החיזוי, אם תגיע תצפית חדשה X נוכל לחזות לה את ערך הY המתאים לה אף על פי שהמודל לא ראה אותה, נקרא לו y_hat.

ומטרת ההסקה או ההבנה: מודל טוב יאפשר לנו להבין את הקשר בין X לY. אילו משתנים בX הם חשובים כדי להסביר או לחזות את Y, ואופי התלות של Y בX, האם הקשר הוא ליניארי? האם הוא מורכב יותר?

אנחנו נדבר על שני סוגים של מודלים לחיזוי על פי הY: כשY הוא כמותי, ממשי, כמו גובה של תינוק שעומד להיוולד, זה ייקרא רגרסיה.

כשY הוא קטגוריאלי, הוא קטגוריה מתוך איזושהי קבוצה G, ואנחנו רוצים לסווג X חדש לאיזו קטגוריה הוא שייך, נקרא לזה קלסיפיקציה.

:::

בדוגמא של הציורים מאתר wikiart, הבעיה שלנו תהיה בעית קלסיפיקציה. X יהיה הפיקסלים של התמונה של הציור, וראינו שאם התמונה היא בגודל K על K
 יש K כפול K כפול 3 שכבות פיקסלים או משתנים. וY יהיה אחת משתי קטגוריות, ציור ריאליסטי או אימפרסיוניסטי.

 דוגמא מורכבת יותר: סיפאר-טן, סט של 60 אלף תמונות עם 10 קלאסים שונים, כמו תמונות של מטוסים, מכוניות וציפורים.

 מודל טוב במקרה של הציורים יינתן ערך קרוב ל-1 לציורים אימפרסיוניסטים למשל, וערך קרוב ל-0 לציורים ריאליסטיים.

 למשל מודל שבודק מה הרמה הממוצעת של פיקסל אדום בתמונה ומסתכל על איזשהו סף, עד ערך מסוים יחזה 0 ומעליו יחזה 1.

 סביר להניח שזה מודל לא טוב, הוא לא יפריד היטב בין ציורים אימפרסיוניסטים לציורים ריאליסטיים.

:::

בעית רגרסיה אפשר לראות בדוגמא של נטפליקס: X הוא וקטור ממימד 99 סרטים, וY הסקלר הוא הציון של הסרט איזו מין שוטרת.

הערכים של X במקרה הזה לא באמת נמצאים על כל הישר הממשי, הם דירוגים במין סולם אורדינלי כזה של 1 עד 5 וראינו שיש בהם גם ערכים חסרים.

גם Y הנתון לנו לא מקבל כל ערך, רק מספרים בין 1 ל5.

בכל זאת מודל טוב יקבל את הציונים הקיימים של משתמש קיים, כולל הסרטים שהוא לא ראה, וייתן חיזוי לדירוג של איזו מין שוטרת הכי קירוב לדירוג Y של אותו משתמש.

:::

בואו נחשוב על עוד דוגמאות מעניינות:

Genome Wide Association Studies
או GWAS, בהם אנחנו מנסים לבודד גנים שאחראים על מחלות למשל.

Y כאן הוא האם האדם בריא או חולה, X הוא כל הגנום שלו. זה יכול להגיע למיליון מיקומים על הכרומוזום, שבהם יכול להיות לאדם 0, 1 או 2 עותקים של מוטציות בגן. וזו דוגמא טובה למצב שמעניין אותנו חיזוי על אדם חדש בהתאם לגנום שלו, האם הוא בסיכון למחלה, אבל גם מעניין אותנו לראות אילו גנים אחראים למחלה, כלומר אילו משתנים בX משפיעים על Y.

דוגמא קלאסית נוספת היא חיזוי האם מייל הוא ספאם או לא. X יכול להיות הרבה סוגים של משתנים: מי השולח, מי הנמען, מתי נשלח המייל וכמובן תוכן המייל, המילים השונות שמרכיבות אותו. שימו לב שוקטור שמתאר מילים הוא וקטור ממימד עצום, של כל אוצר המילים בשפה אחת או יותר, בעצם. בכל אופן המטרה היא לחזות האם מייל הוא ספאם וישר להעביר אותו לתיקיית הספאם.

:::

דוגמא אחרונה היא מעולם הפרסום: גולשת מגיעה לאתר וצריך להחליט האם להראות לה פרסומת, באנר, ואיזו פרסומת להראות לה.
Y יהיה השורה התחתונה, כמה כסף היא תבזבז על המוצר בפרסומת.
X כאן יכול להיות מגוון ככל שיד הדמיון טובה עליכם: היסטורית הגלישה של הגולשת, המיקום שלה, מתי היא גולשת, ואפילו דאטא מקוקיז וממסדי נתונים חיצוניים.

:::

נחזור לדוגמא של נטפליקס ונחשוב על מודל פשוט: נחזה את הדירוג של צופה באיזו מין שוטרת באמצעות סרט שדומה לו, למשל ראינו שהציונים של סוויט הום אלבמה מאוד דומים לציונים של איזו מין שוטרת, שני הסרטים הם קומדיות רומנטיות. כאן אנחנו רואים את גרף הפיזור שממחיש את הקשר החזק יחסית בין שני הסרטים, מי שמדרג גבוה את סוויט הום אלבמה יש לו נטייה לדרג גבוה את איזו מין שוטרת, והמתאם ביניהם חיובי ורחוק יחסית מאפס.

:::

מודל פשוט אחר שכבר רמזנו עליו כשדיברנו על PCA: אולי הscore של צופה בPC הראשון, הPC שמסביר הכי הרבה שונות, הוא מנבא טוב לציון של הסרט איזו מין שוטרת.

ניזכר שבPC הזה הציון של כל צופה גבוה יותר ככל שהוא פחות מסכים עם הדירוג הממוצע של הסרטים. והדירוגים בממוצע הם די גבוהים, לכן נצפה שככל שהציון של צופה גבוה בPC הראשון ככה הוא "שונא" יותר סרטים וגם ייתן ציון נמוך לאיזו מין שוטרת. וזה אכן מה שמתקבל, יחס יורד ומתאם שלילי לא מבוטל.

:::

בכל בעיה של מודל לחיזוי יהיה לנו מדגם למידה או training, שנסמן בTR, המדגם יהיה בגודל N, כלומר N זוגות של X ו-Y. אפשר לכתוב אותם גם כמטריצה X עם N שורות וP עמודות, וY כוקטור באורך N.

ואנחנו נניח את הנחת הIID בקורס שלנו, זוגות התצפיות הם בלתי תלויים, הם נדגמים בצורה בלתי תלויה מאיזושהי התפלגות משותפת PXY. אם התצפיות לא בלתי תלויות אגב זה מצב מעניין מאוד שמביא לפיתוחים מרתקים אבל לא נעסוק בזה בקורס שלנו.

בסופו של דבר הפלט שלנו יהיה מודל לחיזוי, f_hat שמבוסס על מדגם הלמידה, והמטרה האולטימטיבית היא, כשתגיע תצפית חדשה X_0, נחזה לזה y_hat_0 באמצעות המודל שלמדנו f_hat.

איך נמדוד את הביצועים של המודל שלנו? במצב אידאלי נגדיר איזושהי פונקצית הפסד L, שמקבלת ממדגם הלמידה תצפית Y אמיתית ותצפית חזויה y_hat. תוחלת הכמות הזאת, תחת תצפיות שהמודל לא ראה, היא היא הכמות שהיינו רוצים לעשות לה מינימיזציה.

:::

מה יכולה להיות פונקצית ההפסד L?

זה תלוי בנתונים. היינו רוצים שהיא תבטא כמה אנחנו "מפסידים" כשהתצפית האמיתית היא Y ואנחנו חוזים y_hat.

בקלאסיפיקציה אפשר למשל לחשוב על שיעור התחזיות השגויות: אם y_hat שווה לקטגוריה Y הנכונה, אז שילמנו מחיר 0. אם לא נספור את זה כטעות, מחיר 1.

אפשר לחשוב גם על משקול הטעויות שלנו, הרי לא כל טעות "עולה לנו" באותה מידה. אז על טעות בכיוון אחד ההפסד יוגדר כ1, וטעות בכיוון אחר ההפסד יהיה פי 10. נסו לחשוב על דוגמאות בהן הטעות היא לא סימטרית. למשל באבחון מחלה בה הטיפול אינו בעל תופעות לוואי חמורות, ניתן להגיד שאם נאבחן אדם בריא כחולה אולי לא נסב הרבה נזק. אבל אם נאבחן אדם חולה כבריא נפספס אותו והוא עלול לא לקבל טיפול ותחול הידרדרות במצבו עד כדי סכנה ממשית.

בבעיות רגרסיה, מדד מקובל הוא הטעות הריבועית כפונקצית הפסד. ואפשר גם לדבר על שגיאה בערך מוחלט ועוד המון פונקציות הפסד אחרות שמדגישות מה שחשוב לבעיה הספציפית שלפנינו.

:::

מכל מקום, היינו רוצים כאמור את התוחלת של ההפסד תחת התפלגות תצפיות שהמודל לא ראה, אבל אנחנו לא מסוגלים באמת לחשב את זה בלי הערכה טובה של ההתפלגות של הדאטה.

לכן מה שנהוג לעשות זה לחשב את ההפסד האמפירי, על סט נתונים נפרד, בגודל M נאמר, זהו סט המבחן או הטסטינג סט, ונסמן אותו כTE. ואז, הטעות שנרצה למזער היא ממוצע ההפסד על פני הטסט סט הזה, שהמודל לא ראה. אנחנו מקרבים תוחלת על-ידי ממוצע.

למשל ברגרסיה כך יראה הממוצע של פונקצית ההפסד הריבועי, זה נקרא mean squared error או MSE, ונהוג לקחת שורש ולקבל את הroot mean squared error או הRMSE.

בדרך כלל נקבל סט אחד של נתונים, בשאיפה עם מספיק תצפיות, כדי לחלק אותם בצורה אקראית, למדגם למידה (למשל 80 אחוז), ומדגם טסט (20 אחוז).

:::

נעשה את החלוקה הזאת בדאטא של נטפליקס בצורה ידנית, מאוחר יותר נראה שיש לנו פונקציה שתבצע לנו את העבודה בצורה אוטומטית.

X הוא הדירוגים על 99 סרטים בדאטאפריים של הרייטינגז, Y הוא הציונים של miss congeniality.

כאן אני מחשב את גודל מדגם הלמידה, 80 אחוז מהN, וגודל מדגם הטסט, כל השאר. ואני דוגם מספר אינדקסים של תצפיות בהתאם באוביקט tr_ind.

עכשיו מתבצעת החלוקה לXtr ו-Xte, ולYtr ו Yte.

בסופו של דבר בהתאם לציפיות יש לנו 8000 תצפיות על 99 סרטים במדגם הלמידה, ו-2000 תצפיות על 99 סרטים במדגם הטסט.

עכשיו אפשר להתחיל לדבר על מודלים לחיזוי ספציפיים לנתונים של נטפליקס.

=== 2. רגרסיה ליניארית ===

נתחיל ברגרסיה ליניארית. רגרסיה ליניארית היא נושא שנכתבו עליו אינספור ספרים עבי כרס החל מאמצע המאה העשרים, ובחוגים מסוימים מקדישים למודל היחיד הזה סמסטר שלם, כמו למשל אקונומטריקה בחוג לכלכלה. כאן ננסה לתת מבט-על על המודל כמו על מודלים אחרים, ונפנה את מי שמעוניין לקורסים מתקדמים יותר.

:::

המודל f_hat שקושר בין וקטור תצפיות X לסקלאר הוא מודל ליניארי: הכמות הנחזית תהיה צירוף ליניארי של המשתנים בX כשהמשקולות מסומנות בבטא, ויש מעליהן סימן האט כי זאת המשימה שלנו לשערך את משקולות הרגרסיה.

איך נמצא את מקדמי הרגרסיה ממדגם הלמידה?

ננסה למצוא מקדמים שמביאים את המודל הכי קרוב לY האמיתי.

למשל, נרצה למזער את סכום השגיאות הריבועיות מהf_hat החזוי לy האמיתי, על פני מדגם הלמידה. אנחנו קוראים להפסד הזה RSS או residual sum of squares, ומסמנים אותו כפונקציה של הבטאות. נשים לב שניתן גם לכתוב אותו בכתיב מטריציוני מה שיכול להאץ את המימוש: הוא בעצם הנורמה הריבועית של הוקטור Y פחות מטריצה X מוכפלת  בוקטור המקדמים בטא.

נשים לב שנוספה עמודה למטריצת הדאטא X כאן, ויש לה כבר p + 1 עמודות, וזאת על מנת לאפשר את החותך בטא-אפס שיש לנו במודל. העמודה שתתווסף לX תהיה בעצם וקטור שכולו אחד בצורה הזאת (להדגים).

המודל הזה הוא מודל הרגרסיה הליניארית הקלאסית הרגיל, OLS או ordinary least squares, כי אנחנו רוצים להביא למינימום את השגיאות הריבועיות.

:::

נראה איך מתאימים את מודל הרגרסיה הליניארית דווקא בפייתון קודם, כדי לראות את השורה התחתונה. אחר כך נחזור למתמטיקה של הפרוצדורה.

כאן נתחיל עם משתנה יחיד בX וחותך. ניקח סרט אחד והוא יהיה sweet home alabama.

נראה מימוש בשתי ספריות, נתחיל בספריה שהמחברים שלה יותר דאגו למשתמשים עם אורינטציה סטטיסטית, ובאמת הפלט שהיא נותנת עשיר יותר כמו פלטים של תוכנות סטטיסטיות כמו SPSS או R.

אני קורא את הסרט sweet home alabama לתוך מטריצת X_sweet_tr, שתהיה לה עמודה אחת בשלב זה. נשים לב שאני מקפיד להשתמש במדגם הלמידה שלי, ומדגם הטסט כלל לא מופיע!

כעת אני מייבא מספריית statsmodels את המודול api.

בספרייה הזאת החותך לא מתווסף באופן אוטומטי, אנחנו צריכים לבקש על מטריצת הX שלנו add_constant וזה מה שמוסיף לה עמודה של אחדות, נקרא לה עכשיו X_sweet_tr1.

כעת מאתחלים קלאס שנקרא OLS, מזינים לתוכו את Y ואז את X, נקרא לאוביקט הזה model.

ורק כשמבקשים את המתודה fit קורית בעצם הרגרסיה הליניארית, ובאוביקט הmodel שלנו יש את כל מה שאנחנו צריכים.

בפרט יש לאוביקט שלנו שדה שנקרא params ובתוכו נמצאות הבטאות. במקרה שלנו יש שתי בטאות, בטא-אפס לחותך, ובטא-אחת לשיפוע של המשתנה היחיד, ואני מדפיס את המודל הסופי בצורה כזאת:

מסתבר שהמודל חיזוי הליניארי הכי פשוט עם סוויט הום אלבמה כסרט מנבא למיס קונג'יניאליטי, הוא לתת לכולם את ציון הבסיס 2.14, ואז על כל עליה בדירוג של סוויט הום אלבמה להוסיף 0.40.

ספרייה אחרת שכבר ראינו היא sklearn, והיא ספריה שאין לה אוריינטציה סטטיסטית אבל יש לה הרבה יתרונות אחרים כמו הממשק האחוד שלה לכל מודל מאשין לרנינג.

כאן אני מייבא את הקלאס לינאר רגרשן, מאתחל אותו לתוך אוביקט ששוב נקרא model. רק כשאני מבקש על model את המתודה פיט, לתוכה אני מכניס את X וY מתבצעת הרגרסיה הליניארית. נשים לב שכאן ברירת המחדל היא להוסיף חותך למודל והX שמכניסים למתודה פיט לא כולל את העמודה של אחדות, sklearn יעשה את זה בשבילכם.

כאן לחותך יש שדה משלו, אינטרספט, ושאר הבטאות יופיעו בשדה coef מהמילה coefficients. נקבל כמובן את אותו מודל.

:::

למקרה שזה לא היה ברור, המודל שקיבלנו הוא משוות קו ישר פשוט. בואו נראה את זה:

אני בונה X מלאכותי שיש בו פשוט את הדירוגים 1 עד 5. ואני מבקש מהמודל לחזות את הדירוג של Y בהתאם לדירוגים האלה של 1 עד 5.

עכשיו אני מצייר שוב את הסקאטרפלוט שלנו של דירוגי מיס קונג'יניאליטי מול סוויט הום אלבמה, ומוסיף עליהם את הקו הישר שחוזה המודל. הקו נראה די מתאים לנתונים גם אם הוא פשטני.

:::

אז המודל מובן. קו ישר. אמרנו שכדי להעריך עד כמה המודל טוב, צריך לראות את הביצועים שלו על נתונים שהמודל לא ראה, הטסט סט, ומקובל לעשות את זה עם מדד הRMSE. שורש השגיאה הריבועית הממוצעת.

כאן אני מחלץ את סוויט הום אלבמה גם מהטסט סט, והוואי-הט שלי מתקבל באמצעות קריאה לmodel.predict.

כעת אני מדווח על שני RMSE. RMSE נאל, הוא RMSE שיתקבל אם אני פשוט אחזה את הממוצע של Y שראיתי במדגם הלמידה. זה ייתן לנו איזשהו בייסליין של האם המודל שלנו שיפר במשהו, ואני מאוד מאוד ממליץ לעבוד עם בייסליינים פשוטים במיוחד ברגרסיה, כי הRMSE עצמו לא אומר הרבה כפי שנראה.

RMSE נוסף שאני מחשב הוא הRMSE של המודל שלנו עם סרט אחד בלבד, נשים לב שאני מממש כאן את הנוסחה בדיוק כפי שהיא כתובה, ברור שלsklearn יש פונקציות גם בשביל זה.

והתוצאה, במודל הכי פשטני שהוא בעצם קו ישר של הממוצע, נקבל RMSE של כמעט 1, ועם סוויט הום אלבמה כבר נקבל הפחתה בכעשרה אחוזים לקצת פחות מ0.9.

:::

בואו נתקדם להשתמש בכל 14 הסרטים שלגביהם אין לנו תצפיות חסרות. כלומר p יהיה 14. כאן אני עוטף אותם כדאטאפריים של פנדאז בשביל פלט נוח יותר. ואז חוזר על הצעדים שלי, מוסיף חותך, קורא לOLS, ומבקש fit לריצת הרגרסיה.

עכשיו אני מבקש model.summary כדי לקבל פלט מקצועי של רגרסיה ליניארית. ברור שאין בסקופ של הקורס שלנו פנאי לעבור על כל מדד ומדד שמודפס כאן. כרגע אני רק רוצה שתשימו לב לעמודה של coef, אנחנו מקבלים על החותך ועל כל סרט את המקדם בטא שלו. המודל הזה כבר נותן ציון בסיס נמוך של 0.4 למיס קונג'יניאליטי, ואז מעניין לראות את המקדמים של הסרטים השונים. אפשר לראות שסרטים כמו אישה יפה, סוויט הום אלבמה ומה נשים רוצות מקבלים מקדמים חיוביים ודי גדולים, פורסט גאמפ למשל מקבל מקדם שלילי.

:::

ושוב, נרצה לבדוק את הRMSE של המודל על מדגם הטסט שהוא לא ראה.  אנחנו מחשבים אותו בדיוק כמו קודם, ומקבלים עוד הפחתה משמעותית, אנחנו כבר ב0.81.

ועוד נקודה חשובה.

כשהמודל היה עם סרט אחד היה ברור שהמודל הוא בעצם קו ישר. איך נראה המודל שלנו עכשיו? אם תכלילו לשני משתנים המודל יהיה בעצם מישור (להדגים), ויותר משני משתנים נקרא לזה מישור פי-מימדי. 

:::

אבל יש לנו 99 סרטים. בחלקם יש פשוט ערכים חסרים.

בואו נציב לצורך התרגיל במקום ערכים חסרים אפס. כאילו מי שלא דרג סרט מסוים ממש לא אהב אותו. ברור שזה לא הדבר הכי חכם שניתן לעשות כאן ועל טיפול בערכים חסרים גם כן ניתן לבלות סמסטר שלם. ובכל זאת, אולי זה יועיל לנו כפי שמתבטא מהtest RMSE.

ומסתבר שכן, בדוגמא הזאת, אם אנחנו משתמשים בכל 99 הסרטים בהצבת אפס במקום ערכים חסרים הRMSE כבר יורד מתחת ל0.8.

כעת כשראיתם כבר את השורה התחתונה נדבר על איך בעצם מוצאים את המקדמים של הרגרסיה ליניארית, את הבטאות.

=== 3. רגרסיה ליניארית - התאמת המודל ===

אז איך מתאימים מודל רגרסיה ליניארית. נראה את זה קודם מההיבט האלגברי, טכני. ניתן גם פירוש גיאומטרי. לבסוף נדבר על רגרסיה ליניארית מההיבט הסטטיסטי.

:::

נתחיל במקרה הפשוט של משתנה אחד בX, למשל הסרט סוויט הום אלבמה, ועוד חותך. כך נראה הקריטריון למינימיזציה, הRSS, ואנחנו רוצים לעשות לו מינימום לפי המקדמים בטא-אפס, ובטא-אחת.

זאת בעיה יחסית פשוטה מחשבון דיפרנציאלי ואינטגרלי, אתם יכולים לגזור את הכמות הזאת לפי בטא אחת, להשוות לאפס, ולמצוא את הפתרון שלפנינו. לאחר מכן תגזרו לפי בטא-אפס, תשוו לאפס ותמצאו את הביטוי שיש לנו כאן: ממוצע הY פחות ממוצע הX כפול האומד לבטא-אחת. לבסוף כדי לוודא שזאת אכן נקודת מינימום תצטרכו כמובן להביט על כל מטריצת הנגזרות השניות ולוודא שהיא חיובית, פוזיטיב דפיניט.

ואם האומד לבטא-אחת נראה לכם מוכר, אתם לא טועים, מדובר בנוסחה קרובה מאוד למקדם המתאם של פירסון המוכר. בתרגול תראו את הקשר בין השניים ובאילו מקרים הם יהיו זהים, כלומר שיפוע המודל הליניארי יהיה ממש מקדם המתאם בין X לY.

:::

אם אנחנו רוצים להכליל לp משתנים מומלץ לכתוב את הRSS בכתיב וקטורי.

אם נגזור ונשווה לאפס נקבל את מה שמכונה המשוואות הנורמליות.

ושוב, לאלו שפחות מתחברים לכתיבה של סט משוואות בכתיב וקטורי, אפשר להראות שבסופו של דבר מדובר בסט של משוואות מהצורה הזאת, גזירה של כמות לפי אחד המקדמים בוקטור בטא, והשוואה לאפס. p + 1 משוואות כאלה.

:::

אנחנו פותחים את הסוגריים, מחלקים פי 2, ומגיעים לפתרון הריבועים הפחותים, נוסחה שכדאי להכיר בעל-פה. ושוב, מטריצת הנגזרות השניות היא פוזיטיב דפיניט, לכן אנחנו יודעים שזוהי נקודת מינימום.

תרגיל פשוט למי שרוצה אתגר: עבור p = 1, הראו שפתרון הריבועים הפחותים הוא בדיוק הנוסחאות הספציפיות לבטא-אפס ולבטא-אחת שראינו.

:::

חשוב להכיר גם את הפרשנות הגיאומטרית של הפתרון שלנו: הרי כרגע במרחב הלמידה העמודות של X פורסות איזשהו מרחב, וY הוא גם וקטור, נוסף.

אנחנו רוצים למצוא צירוף ליניארי של העמודות האלה של X, זו המשמעות של הביטוי Xbeta

לכן כשאנחנו עושים מינימיזציה לRSS, לכמות הזאת, אנחנו בעצם מחפשים את הצירוף הליניארי של עמודות X שהוא הקירוב הטוב ביותר לY. הפתרון שלנו הוא הוקטור הכי קרוב במרחב הנפרש על-ידי עמודות X, לוקטור Y שכנראה לא נמצא במרחב הזה. נדגים זאת על שני משתנים:

(הדגמה על הלוח)

איך קוראים לוקטור y_hat הזה? הוקטור הזה הוא ההטלה האורתוגונלית של הוקטור Y אל המרחב הנפרש על-ידי עמודות X!

=== 4. רגרסיה ליניארית - ההיבט הסטטיסטי ===

איפה סטטיסטיקה נכנסת לפעולה? עד כאן היתה לנו רק אלגברה ליניארית וחשבון דיפרנציאלי. ומאיפה באים ערכי הטי והpvalue שראינו בפלט הרגרסיה?

:::

נשים לב עד עכשיו שלא הנחנו שום מודל שהוא הקשר האמיתי בין X לY. לא היתה שום התפלגות, לא היו כמעט שום תנאים על פתרון הרגרסיה הOLS, ואכן רבים שאין להם נגיעה לסטטיסטיקה בכלל לומדים על רגרסיה ליניארית במסגרת קורסים באלגברה ליניארית.

כעת כן נניח שיש מודל, הנחה די מחמירה: שY הוא צירוף ליניארי של האיקסים, ועוד איזשהו רעש שנסמן באפסילון, משתנה מקרי, ועל הרעש הזה נניח התפלגות נורמלית. נניח שהתוספת הזאת ממורכזת באפס, ויש לה שונות, כמובן לא ידועה, של סיגמא בריבוע.

תיכף ניתן הערה שלא נוכיח, על מדוע התייחסות כזאת לא תשנה את פתרון הריבועים הפחותים, בטא-האט עדיין יהיה (X'X)-1X'y. אבל בואו נגיד מה התווסף לנו:

כעת Y הוא משתנה מקרי. ויש לו התפלגות ותוחלת, בהינתן האיקסים. התוחלת המותנית של Y בהינתן שהאיקסים קבועים על ערך מסוים, היא x'beta, היא פונקציה ליניארית של X.

הטעות שמתקבלת אם אחסר מY את התוחלת המותנית שלו היא גם משתנה מקרי, זה בעצם האפסילון, אז היא מתפלגת נורמלית, עם תוחלת אפס, ובאופן בלתי תלוי בטעויות האחרות.

ותחת הנחת המודל הסטטיסטי גם בטא-האט יהיה וקטור של משתנים מקריים, עם התפלגות! אנחנו כבר יכולים לא רק להגיד מהו הבטא-האט אלא גם לבצע עליו הסקה סטטיסטית. נוכל להגיד האם בטא מסוים שונה בצורה מובהקת סטטיסטית מאפס, כלומר האם הוא באמת חשוב במידול של Y או לא.

:::

אנחנו רוצים למצוא את ההתפלגות של האומד בטא האט. בטא-האט הוא צירוף לינארי של משתנים נורמליים ולכן גם הוא, מתפלג נורמלית, עם איזשהו וקטור תוחלות ומטריצת שונויות (להדגים). השאלה היחידה היא מהו וקטור התוחלות ומטריצת השונויות האלה. 

מה ידוע לנו עד כה? התוחלת המותנית של Y היא X בטא. הYים בלתי תלויים, מטריצת השונויות שלהם היא אלכסונית עם סיגמא בריבוע על האלכסון, ניתן לסמן זאת כך. והאומד לבטא-האט נראה כך, הוא לא משתנה כאמור בעקבות ההנחה הסטטיסטית.

התוחלת של בטא-האט: האיקסים הם קבועים, ומליניאריות התוחלת הם יוצאים החוצה ונשארת רק התוחלת של Y, שהיא כידוע X בטא, וכך אנחנו מגיעים לעבודה שהתוחלת של בטא-האט היא בטא עצמו, בטא-האט נקרא אומד חסר הטיה לבטא.

ומטריצת השונות או הקווריאנס של וקטור בטא-האט: כשמחשבים שונות של סקלאר כפול משתנה הסקלאר יוצא בריבוע. כשמחשבים מטריצת שונות של מטריצת קבועים כפול הוקטור שלנו, היא יוצאת בהכפלה משמאל ומימין. אבל מטריצת השונות של Y היא כאמור אלכסונית, וכל מה שנשאר זה הכפלה של הביטוי הזה בסיגמא בריבוע. דברים מצטמצמים ומגיעים לביטוי סופי, סיגמא בריבוע כפול המטריצה ההופכית של X'X.

נסכם: בטא-האט מתפלג נורמלית עם תוחלת בטא האמיתית, ושונות סיגמא בריבוע כפול מטריצת X'X.

:::

אז איך מבצעים הסקה סטטיסטית על הבטאות?

באופן שולי ניתן לראות שכל בטא-האט-ג'י בוקטור מתפלג נורמלית עם תוחלת בטא-ג'יי ושונות שהיא סיגמא בריבוע, כפול האיבר האלכסון במטריצה ההופכית של X'X.

זה אומר שעכשיו אנחנו יכולים לבצע בדיקת השערות על כל בטא-ג'יי: השערת האפס תהיה שבטא-ג'יי שווה לאפס. ההשערה האלטרנטיבית, תהיה כמעט תמיד דו-צדדית, שבטא-ג'יי שונה מאפס, כלומר המשתנה הזה חשוב במודל.

תחת השערת האפס ההתפלגות של בטא-האט-ג'יי היא נורמלית עם אותה שונות אבל עם תוחלת אפס,

אם היינו יודעים את השונות סיגמא בריבוע זה כל מה שאנחנו צריכים בשביל מבחן Z. אנחנו לא יודעים את השונות סיגמא בריבוע ולכן אנחנו עושים מבחן T.

:::

לא ניכנס לפרטים, בקורס לנו לא תצטרכו לעשות מבחן טי ידני על הבטאות, אבל אתם כן מבינים כעת הרבה יותר מהפלט רגרסיה של statsmodels שראינו קודם.

(הדגמה על הפלט)

:::

נסכם, ואז ניתן שתי הערות:

כדי לקבל את אומד הריבועים הפחותים אנחנו עושים מינימיזציה של הRSS, סכום השגיאות הריבועיות על מדגם הטריין, וזה יביא לנו את הצירוף הליניארי הטוב ביותר של X כדי להתקרב לוקטור Y.

הפתרון הוא אלגברי לחלוטין, והפירוש הגיאומטרי שלו הוא בעצם איך נראית ההטלה של הוקטור Y על המרחב שנפרש על-ידי X.

המודל הסטטיסטי בעצם מאפשר לנו לבצע הסקה סטטיסטית על המקדמים בבטא ולתת אמירות כמו המשתנה חשוב או לא חשוב למודל, כי הוא שונה או לא שונה מאפס ברמת מובהקות מסוימת.

זה הכלי אולי החשוב ביותר מבחינה מסורתית שיש למדען נתונים, כי המון מודלים מתחילים מהנקודה שבה הוא נגמר והוא תמיד ישמש בייסליין פשוט שבאופן מפתיע לא תמיד קל לנצח.

ומי שרוצה להעמיק עוד, מוזמן לקחת קורס כמו מודלים סטטיסטיים בחוג לסטטיסטיקה, ו/או לקרוא מגוון עצום של ספרים שנכתבו בנושא.

:::

שתי הערות חשובות:

כפי שראינו, תחת המודל הסטטיסטי, האומד בטא-האט, הוא בעצם אומד חסר הטיה לבטא האמיתי. אבל זה אומר, שגם y_hat בהינתן X, התחזיות שלנו הסופיות, הן אומד חסר הטיה למודל הליניארי, לx'beta, כלומר לתוחלת של Y בהינתן X.

כלומר, מה שאנחנו בעצם עושים ברגרסיה ליניארית זאת אמידה של תוחלת מותנית, אפילו אם המודל לא נכון.

זו פרשנות נורא יפה של מה שאנחנו מנסים לעשות בבניית מודלים לחיזוי, להעריך את מה שאנחנו יודעים על Y, הסיכום שלו, בהינתן שראינו את כל המשתנים באיקס.

אנחנו ניקח את התוחלת המותנית של Y בהינתן X איתנו הלאה, זה מה שננסה לחזות גם במודלים אחרים, אנחנו פשוט לא נהיה מחויבים ליחס הליניארי הנוקשה הזה.

:::

ועוד הערה ששווה לתת:

אפשר היה להגיע לפתרון הריבועים הפחותים גם בדרך אחרת. להתחיל בכלל במודל הסטטיסטי כפי שרשמנו אותו: Y הוא צירוף ליניארי של המשתנים ועוד רעש נורמלי בלתי תלוי. תחת ההנחה הזאת Y הוא כאמור משתנה מקרי נורמלי, ויש לו פונקצית צפיפות מוכרת. אפשר היה לנסות למקסם כמות אחרת מהRSS, שנקראת נראות או likelihood, והיא מכפלת הצפיפויות.

הדבר המדהים הוא, שזה היה מביא אותנו לאותו פתרון ריבועים פחותים לבטא-האט! אנחנו נראה שוב את הנראות בקרוב, ובאופן כללי אמידת נראות מקסימלית היא שיטה סטטיסטית מקובלת ומצוינת לקבל אומדים כשמניחים התפלגות מסוימת, לאו דווקא נורמלית, כמו כאן.

=== 5. רגרסיה לוגיסטית ===

התחלנו ברגרסיה, כשY הוא ממשי, ונעבור עכשיו לקלאסיפיקציה, כשY קטגוריאלי.

:::

נתמקד בבעיה הפשוטה שY הוא אחת משתי קטגוריות אפשריות, כלומר הוא בינארי, ובתרגול תדברו קצת יותר על Y שיש לו יותר משתי קטגוריות.

דוגמאות לY בינארי יכולות להיות כאמור למדל האם יש לנו ציור אימפרסיוניסטי או ריאליסטי, האם האדם נשא של נגיף או לא, האם צרכנית תקנה או לא תקנה מוצר כלשהו.

אנחנו עדיין ממדלים באמצעות הטריין סט, מדגם הלמידה בגודל n שמורכב מזוגות תצפיות של X ושל Y. ונניח גם שהאיקסים שלנו ממשיים, בתרגול תיתקלו גם באיקסים קטגוריאליים.

אז נשאלת השאלה: האם ניתן להשתמש בכל התוצאות שלנו מרגרסיה ליניארית עבור Y שהוא קטגוריאלי עם שתי קטגוריות?

התשובה היא שברור שכן, פשוט צריך להפוך אותו לנומרי, נגדיר שאימפרסיוניסטי זה 1, וריאליסטי זה 0, והנה יש לנו Y ממשי וכל הפונקציות הרלוונטיות בפייתון יעבדו. לא נקבל שגיאה.

:::

אז איפה פה בכל זאת השגיאה, למה זה לא פתרון טבעי לבעיה?

נזכור שמה אנחנו ממדלים בעצם ברגרסיה ליניארית? את התוחלת המותנית של Y בהינתן X, את מה שלמדנו על Y בממוצע בעקבות המידע על המשתנים האחרים. ומה זו התוחלת של משתנה שיש לו שתי תוצאות, אפס או 1? כמו משתנה ברנולי, זו ההסתברות שהוא יקבל 1 או ההסתברות שהציור הוא ריאליסטי בדוגמא שלנו בהינתן שראינו את הפיקסלים בתמונה.

זה מצוין. אבל הסתברות היא כמות בין אפס לאחת! ואין שום אילוץ ברגרסיה ליניארית לקבל תחזיות בין אפס לאחת. נוכל לקבל חיזויים קטנים מאפס, גדולים מאחת, ונוכל לקבל אפילו סט של תחזיות שכולן מתחת לאפס או כולן מעל 1. ואז מה נעשה?

ניתקל בעוד בעיה כשנרצה לבצע הסקה סטטיסטית: האם סביר להניח שY הוא צירוף ליניארי של משתנים ועוד רעש נורמלי, אפסילון, בלתי תלוי? לא, כי וואי מקבל ערכים אפס או אחת.

אז אנחנו צריכים גישה אחרת. גישה שכן תתחשב בערכים האפשריים שY יכול לקבל ושתתאים מודל סטטיסטי הולם לבעיה.

:::

גישה כזאת היא רגרסיה לוגיסטית.

אנחנו לא ממדלים את Y עצמו כמודל ליניארי. אלא את הכמות הבאה: לוג, של ההסתברות שY שווה 1, חלקי ההסתברות שY שווה אפס. הכמות הזאת היא תהיה צירוף ליניארי של המשתנים באיקס. ניתן גם לרשום את זה כלוג של ההסתברות p חלקי 1 פחות p, כך שאנחנו רואים שבמקום למדל את התוחלת של Y בהינתן X אנחנו ממדלים איזושהי פונקציה G שלה. הפונקציה הזאת די מפורסמת וקוראים לה גם לוג'יט.

מכל מקום כעת הכמות שאנחנו ממדלים היא בין מינוס אינסוף לאינסוף ולכן כל חיזוי שלנו יהיה כמות לגיטימית.

אם יש לנו את המקדמים ואנחנו רוצים לקבל בחזרה את ההסתברות החזויה, את הכמות בין אפס לאחת, אפשר לראות שהפונקציה ההופכית נראית כך: e בחזקת הצירוף הליניארי, חלקי 1 ועוד e בחזקת הצירוף הליניארי. או ההסתברות המשלימה עם נוסחה קצת יותר נעימה שמופיעה כאן.

:::

אז אנחנו מוצאים את המקדמים במקרה של רגרסיה לוגיסטית?

כאן אנחנו חייבים לעבור דרך פונקצית הנראות, הlikelihood ולבצע אמידת נראות מקסימלית. פונקצית הנראות שלנו היא פונקציה של בטא בהינתן הדאטא בטריינינג ומסומנת בדרך כלל בL. במקרה הבדיד כמו לפנינו שY הוא בעצם משתנה ברנולי, מדובר במכפלת ההסתברויות במדגם תחת המודל.

מאחר שY מקבל ערכים אפס או אחת, ניתן לרשום את הביטוי בצורה כזאת. נציב את הביטויים של מודל הרגרסיה הלוגיסטית עבור ההסתברות שוואי שווה אחת ועבור ההסתברות שוואי שווה אפס. ונגיע לביטוי לא סימפטי אבל מפורש, שצריך למקסם כדי לקבל את בטא-האט.

כשנקבל את בטא-האט, ותגיע תצפית חדשה ממדגם הטסט, נוכל לחזות את ההסתברות שהיא אחת באמצעות הצבה בנוסחה של הפונקציה ההופכית. ואם אנחנו רוצים חיזוי סופי, האם Y הוא 1 או 0, אפשר להשוות את ההסתברות הנחזית לאיזשהו קאטאוף, לדוגמא חצי. אם היא גדולה מחצי נחזה 1, ואם לא נחזה 0.

הערה לפני שממשיכים: זה לא סתם שלא רשמנו כאן ביטוי מפורש לבטא-האט, הסיבה שאין כזה. פונקצית הנראות אמנם מפורשת וקמורה אבל אין לה פתרון סגור, לכן משתמשים בשיטות אופטימיזציה כמו ניוטון רפסון למי שמכיר, כדי למצוא את המקדמים.

:::

ועוד מילה על המודל שלנו. יחס הסיכויים שאנחנו ממדלים הוא בעצם הodds. בשפה יומיומיות, אם מאורע יכול לקרות בסיכוי שליש, או לא לקרות בסיכוי שני שליש, אנחנו אומרים "הוא יקרה בסיכוי של 1 ל-2". כלומר מה שאנחנו ממדלים כצירוף ליניארי הוא הלוג-אודז, לוג יחס הסיכויים.

הפרשנות של מקדמי הבטא ברגרסיה לוגיסטית הרבה פחות פשוטה לעומת רגרסיה ליניארית, ועלולה לבלבל, אז נשים לב:

אז מה המשמעות של מקדם בטא-ג'יי? עלייה של יחידה אחת בXj, פירושה עלייה של בטא-ג'יי בלוג אודז.

ואם זה לא אומר הרבה, קחו את האקספוננט של בטא-ג'יי ותראו מה הוא עושה לאודז עצמו. עלייה של בטא-ג'יי בגודל 1 היא עלייה פי e של האודז עצמו.

=== 6. רגרסיה לוגיסטית לדוגמא: דאטא על חולי לב ===

נראה כעת דוגמא של רגרסיה לוגיסטית על דאטא קטן שמתאים לבעיה.

:::

בנתונים שלפנינו יש 462 גברים מדרום אפריקה. יש לנו מידע רפואי עליהם כמו לחץ דם, מידת העישון, צריכת אלכוהול וגיל, והמשתנה שיעניין אותנו הוא הchd, coronary heart disease, האם הם לקו במחלת לב או לא.

:::

נגדיר את מטריצת הX שלנו ואת הY. יש לנו כאן משתנה קטגוריאלי שמצריך טיפול מיוחד עם הפונקציה get_dummies, ולא ניכנס לזה כרגע.

מכל מקום אנחנו רוצים לחלק את הדאטאט לטריין וטסט. עשינו את זה ידנית עם הדאטא של נטפליקס, בואו נשתמש כאן בפונקציה מקובלת מספריית sklearn, הפונקציה train_test_split, שמקבלת את מטריצת הX, את הוקטור Y, את גודל מדגם הטסט כמספר עשרוני, כאן עשרים אחוז, וגם איזשהו סיד כדי שההקצאה האקראית תישמר בריצות חוזרות. מה שמתקבל הוא X טריין, X טסט, Y טריין, Y טסט.

והנה אנחנו מדפיסים את גודל הדאטא, בשורה התחתונה יש לנו p = 9 משתנים.

:::

ברגע שיודעים לבצע רגרסיה ליניארית בstatsmodels או בsklearn, רגרסיה לוגיסטית זה קל.

פשוט מאתחלים את הקלאס Logit, מזינים לתוכו את Y טריין ואז את X טריין בתוספת חותך, וקוראים לmodel.fit.

אם נבקש summary נקבל פלט סטטיסטי עשיר ומסודר של המקדמים ומבחנים סטטיסטיים עליהם.

נשים לב שהמבחנים כאן הם מבחני Z אבל לא ניכנס בקורס הזה לסיבה לכך. מכל מקום לכל מקדם יש טעות תקן שונה, אז כדי להשוות ביניהם נסתכל על הציוני תקן, על ערכי הZ. משתנים עם ערכים גבוהים הם age למשל, הגיל. לכל שנת חיים נוספת, המודל מוסיף איזושהי כמות ללוג-אודז שהפציינט יחלה במחלת לב. מקדם שלילי גדול הוא הfamhist_Absent, שמשמעותו האם אין לך היסטוריה משפחתית של מחלת לב. אם אין, יורדת לך כמות של 0.8 מהלוג-אודז.

:::

בsklearn הקלאס שאנחנו רוצים נקרא LogisticRegression. כאן אנחנו יכולים לפרט איזשהו סולבר ומספר איטרציות, כי כאמור רגרסיה לוגיסטית מצריכה שיטות נומריות לאופטימיזציה.

אחרי שהגדרנו את המודל נקרא לfit, ונדפיס את המקדמים שהתקבלו. כמו ברגרסיה ליניארית, sklearn הרבה יותר מוכוון מאשין-לרנינג ולא סטטיסטיקה, אז הפלט שלו מוגבל.

נשים לב גם שקיבלנו מקדמים דומים ל-statsmodels אך לא זהים, כי לא מדובר בנוסחאות סגורות.

:::

אבל איך אנחנו מכמתים את הביצועים של המודל על מדגם הטסט, על זה לא דיברנו. אפשר לדווח את הנראות שאותה מיקסמנו, אבל זה לא יגיד הרבה.

מקובל יותר למשל לחזות את ההסתברויות p_hat על מדגם הטסט, באמצעות הנוסחה שראינו. בsklearn זה נעשה עם הפונקציה predict_proba.

ואז להשוות את ההסתברויות החזויות לאיזשהו קאטאוף דיפולטי של חצי. ואם אתם מודאגים מזה אתם צודקים, ותיכף נדבר על זה. זה יביא אותנו לחיזוי סופי של Y, האם הוא 0 או 1.

את החיזוי הזה אפשר להכניס לתוך קונפיוז'ן מטריקס או "מטריצת בלבול", מטריצה 2 על 2 שתראה לנו, מתוך הפציינטים שהם לא חולים, כמה המודל חזה שהם כן חולים, וכמה לא. ומתוך הפציינטים שהם כן חולים, כמה המודל חזה שהם חולים וכמה לא.

אפשר גם לחשב מדדים אינטואיטיביים של אחוז דיוק, accuracy, ואחוז שגיאה, error. מדובר באופן כללי באחוז התצפיות מהטסט סט שהמודל צדק לגביהן, והאחוז שהוא טעה. כאן על נתונים שהמודל לא ראה הוא צודק ב76 אחוז וטועה ב24 אחוז.

בחלק האחרון של השיעור נרחיב על אווליואציה של מודלים לקלסיפיקציה, מסתבר שהמדדים שהסתכלנו עליהם כרגע יכולים להיות בעייתיים.

לדוגמא (להדגים), נניח ש99 אחוז מהפציינטים היו בריאים ורק אחוז אחד היו חולים. מה אם אתן לכם מודל שחוזה שכל הפציינטים הם בריאים? זה מודל נהדר, הוא יקבל אקיורסי של 99 אחוז! חייב להיות אם כן מדד טוב יותר שיעיד על כך שזה לא מודל נהדר ואפילו די גרוע.

=== 7. אווליואציה של מודל קלסיפיקציה ===

אז אני טוען שאחוז הaccuracy לא תמיד משקף נכון את הביצועים של המודל, וראינו שהתופעה חמורה במיוחד כשהנתונים הם מה שקרוי imbalanced, אין באוכלוסיה מספר שווה של דוגמאות חיוביות ושליליות. ואנחנו זקוקים למדדים טובים יותר.

:::

רמזנו בתחילת היחידה, שלטעויות שונות יכול להיות משקל שונה. לפספס חולה ולהגיד לו שהוא בריא, יכולה להיות לזה משמעות אחרת לגמרי מלהגיד לאדם בריא שהוא חולה.

נסמן את הכמויות בטבלת הקונפיוז'ן מטריקס בצורה כזו:

מספר הדוגמאות החיוביות, או חולים, באופן שולי נסמן כP. מספר הדוגמאות השליליות נסמן כN. באופן דומה, מספר החיזויים החיוביים נסמן כP_hat, ומספר החיזויים השליליים נסמן כN_hat.

אם המציאות היא חולה והמודל חזה חולה, זה true positive או TP. אם המציאות היא חולה והמודל חזה לא-חולה, כלומר בריא, זה false negative או FN.

באופן דומה מגדירים false positive וtrue negative.

ומי שצריך להגדיר את זה עם נוסחאות יותר פורמליות, אפשר להגדיר את כל הכמויות האלה עם y וy_hat בצורה כזאת.

:::

הטבלה הזאת מאפשרת לנו להסתכל על מדדים הרבה יותר ספציפיים למה שמעניין אותנו בבעיה הנתונה. נפרט אותם כעת:

האקיורסי כעת היא סכום התאים על האלכסון של הקונפיוז'ן מטריקס, הTP והTN, חלקי m גודל מדגם הטסט.

טעות הניבוי היא בדיוק סכום התאים האחרים לא על האלכסון חלקי m.

הפרסיז'ן הוא הסיכוי להיות חולה בהינתן שחזיתי חולה. נקרא גם positive predictive value או PPV. באופן דומה אפשר להגדיר את הפרסיז'ן לקלאס האחר, הסיכוי להיות לא-חולה אם חזיתי לא-חולה, או הnegative predictive value. בכל מקרה נרצה שהפרסיז'ן יהיה כמה שיותר גדול.

מדד אחר הוא הריקול, או sensitivity או true positive rate, TPR. זה הסיכוי שבהינתן חולה המודל אכן חוזה חולה, או שיעור החולים שהמודל אכן מחלץ. גם כאן ניתן לחשוב על הריקול של הקלאס האחר, אחוז הבריאים שהמודל מחלץ נכון. וגם כאן ברור שנרצה שהמדד הזה יהיה גדול ככל שניתן.

מדד שנרצה שיהיה קטן ככל האפשר הוא הfalse positive rate, הFPR, בהינתן לא-חולה הסיכוי לחזות חולה.

ומאחר שהמדדים פרסיז'ן וריקול מודדים דברים שונים והיינו רוצים ששניהם יהיו גבוהים מסתכלים לפעמים על הממוצע ההרמוני שלהם, 2 כפול המכפלה שלהם חלקי הסכום שלהם.

למה צריך את כל זה? בדיוק בגלל מה שאמרנו, יכול להיות לקוח של המודל שלנו, שפשוט לא מסוגל לפספס אף חולה, לקוח כזה ירצה ריקול כמה שיותר גבוה. ויכולה להיות לקוחה שאין לה בעיה לפספס חולים, אבל כשהמודל מסמן לה חולים הוא חייב להיות צודק, היא לא יכולה להעניק טיפול קשה כזה לבריאים -- בשביל לקוחה כזאת נרצה אולי למקסם את הפרסיז'ן. וברור שיהיה כאן טריידאוף בין השניים במודל לא מושלם.

:::

בנתונים שלנו כך נראית הקונפיוז'ן מטריקס:

כדי לקבל את כל המדדים שדיברנו עליהם אפשר לבקש classification_report מsklearn. נדגים לראות שהבנו.

למשל, הפרסיז'ן של החולים: אם חזיתי חולה או 1, מה הסיכוי שהפציינט באמת חולה: מתוך 26 חזויים כחולים, 19 אכן חולים, דיוק של 73 אחוז.

או, הריקול של החולים, אם אתה חולה מה הסיכוי שהמודל יחזה נכון. מתוך 34 חולים המודל צדק רק לגבי 19, שזה 56 אחוז, לא גבוה מאוד.

נשים לב אגב שהכל מבוסס על ערך הסף הזה שהשווינו אליו את ההסתברויות החזויות כדי לקבל את y_hat_te, הקאטאוף של חצי -- יכול להיות שעם קאטאוף אחר היינו מקבלים מדדים טובים יותר?

:::

באופן כללי יכולות להיות לדאטא סיינטיסט מטרות שונות במודל קלסיפיקציה:

האם אני רוצה פשוט לחזות נכון.

האם מעניינות אותי דווקא ההסתברויות הנחזות ולדאוג שהן יהיו מכוילות כמו שיותר עם ההסתברויות המקוריות.

ואולי, כל מה שמעניין אותי זה הדירוג של תצפיות, מבחינת הסיכוי שהן 1 או שהפציינט חולה. לא חשוב לי אפילו אם הכמויות שאני חוזה הן הסתברויות, אלא אני מתייחס אליהן כאיזשהו סקור כללי שאני רוצה שידרג נכון את התצפיות שלי.

המטרה שלי משפיעה על המדד שלי לטיב המודל:

אם המטרה שלי היא פשוט לחזות נכון אני מסתכל על מדדים כמו אלה שראינו: אחוז דיוק, פרסיז'ן, ריקול.

אם המטרה שלי היא הסתברויות מכוילת כמה שיותר, אני באמת אדווח אולי על הלוס שהשתמשנו בו, הנראות.

אבל אם, כמו שקורה פעמים רבות, המדד שמעניין אותי זה הראנקינג, רק הדירוג של התצפיות, איך כדאי להסתכל על הביצועים של המודל?

:::

אם מה שמעניין אותנו הוא ראנקינג, נהוג להסתכל בעקומה שנקראת receiver operating characteristic או ROC בקיצור.

הרעיון הוא לא להסתפק בקאטאוף דיפולטי של חצי כפי שעשינו, כי יכול להיות שהסקור שהוצאנו איננו בדיוק הסתברות. נשנה את הקאטאוף בצעדים קבועים, ובכל צעד נמדוד את ה: טרו פוזיטיב רייט, זה בעצם הריקול, ואת הפולס פוזיטיב רייט, אחוז הדוגמאות השליליות שעוברות את הקאטאוף הנוכחי ונחזות כחיוביות.

עקומת הROC תצייר את הTPR לעומת הFPR לכל קאטאוף אפשרי. מודל מושלם ימצא קאטאוף שעבורו הFPR קרוב ל0 והTPR קרוב ל1.

ושוב נדגיש שגם אם ההסתברות הנחזית מגיעה ממודל שבכלל לא אמור להוציא הסתברויות והיא לא מכוילת או אפילו היא לא הסתברות, היא איזשהו סקור, הדירוג עצמו עדיין יכול להיות מצוין. וזה היתרון הגדול של גישה כזאת. התחשבות בקאטאופים שונים והעובדה שהיא פרקטית לכל סקור.

:::

כך נראית עקומת הROC על מדגם הטסט שלנו, עבור מודל הרגרסיה הלוגיסטית שמנסה לחזות אם פציינט יחלה במחלת לב או לא.

הקוד בפייתון לבצע את זה נמצא כאן למי שרוצה.

אנחנו לא רואים את הספים עצמם שהקוד משנה, אבל כן נוצרת עקומה יפה של TPR מול FPR, ואנחנו רואים שעבור ספים נמוכים מתקבל TPR גבוה, ועבור ספים גבוהים מתקבל FPR נמוך. אפשר גם לראות את הטריידאוף בין שני המדדים בצורה יפה.

ואם רוצים לחזור לניבוי סופי, אפשר לבחור את הקאטאוף שיביא אותנו לנקודה האופטימלית מבחינתנו. אם אין לנו העדפה מסוימת, כמו שהדגמנו קודם, הנקודה האופטימלית שומרת על TPR כמו שיותר גבוה וFPR כמה שיותר נמוך אז זו תהיה הנקודה הכי קרובה לקצה השמאלי העליון של הגרף, נאמר זאת.

וברור שבמודל מושלם יימצא סף שמגיע עד הנקודה בקצה השמאלי עליון של הגרף.

עכשיו, היינו רוצים לתמצת את העקומה הזאת לאיזשהו מדד יחיד, שיבטא את טיב המודל שלנו. עקומה זה מרשים אבל צריך לסכם אותה איכשהו. נראה שבמודל מושלם השטח תחת העקומה יהיה השטח של כל הריבוע, 1 כפול 1, זאת אומרת 1, וכאן כפי שמודפס השטח הוא רק 0.8, או 80 אחוז.

:::

השטח תחת עקומת הROC , הarea under curve או AUC, הוא מדד מקובל מאוד, והוא מצוין כי הוא לא תלוי בקאטאוף ספציפי, הוא מתחשב בכולם, מעין ממוצע או אינטגרציה של טיב המודל על פני ספים שונים.

מודל אקראי לחלוטין, שככל שאנחנו מעלים את הסף כך יורד הTPR ועולה הFPR באופן שווה, העקומה תהיה בעצם הקו האלכסוני בתוף הריבוע, והשטח תחתיה, הAUC יהיה חצי.

מודל מושלם כאמור, יגיע לAUC של 1, כל שטח הריבוע.

פרשנות יפה של AUC היא, מתוך כל זוגות התצפיות האפשריות כך שאחת חיובית ואחת שלילית, כמה מדורגות "נכון". ומה זה נכון? אם הסקור הנאמד לתצפית החיובית בזוג, גדול מהסקור לתצפית השלילית. אז אחוז הזוגות שמדורגים נכון, במודל אקראי יהיה -- חמישים אחוז, חצי. כמו שאמרנו שAUC למודל אקראי יהיה. ואחוז הזוגות שמדורגים נכון במודל מושלם יהיה -- מאה אחוז.

בצורה פורמלית אפשר לסמן זאת כך, אם יש לנו m0 תצפיות שליליות וm1 תצפיות חיוביות, אז הAUC שווה לאחוז הזוגות שעבורם p_hat לתצפית החיובית גבוה מp_hat לתצפית השלילית. ונשים לב שהמדד הזה גם לא סובל במרכאות אם יש לנו הרבה יותר תצפיות חיוביות משליליות במדגם, או להיפך, כמו שראינו עבור מדד הaccuracy. זו פרופורציה שבכל מקרה נרצה שתהיה כמה שיותר קרובה לאחת.

:::

נקודה אחרונה למחשבה: למה לא הזכרנו בשיעור הזה את הציורים שלנו הריאליסטיים והאימפרסיוניסטיים? למה לא השתמשנו ברגרסיה לוגיסטית כדי לבנות מודל שיחזה האם ציור הוא אימפרסיוניסטי?

ברור שאפשר, ואתם תבנו מודל כזה בשיעורי הבית. ואז אתם תגלו שמודל כזה לא יהיה מאוד איכותי. נסו לחשוב למה!

ביחידות הבאות נמשיך עם בניית מודלים לחיזוי, אבל נוותר על המודל הליניארי הנוקשה. יחד עם זאת, כל המושגים שלמדנו היום ישמשו אותנו היטב.

:::
