---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2DS_logo_white.jpg"
pagetitle: "Collecting, Exploring and Cleaning Data"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Data Science](https://intro2ds2023.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Data Science {.title-slide}

### Collecting, Exploring and Cleaning Data - Class 3

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2ds` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::
---

## Common Data Formats in Data Science {.title-slide}

---

### CSV: Comma Separated Values

::: {.fragment}
<img src = "images/csv.png" style="width: 70%">
:::
::: {.fragment}
```{python}
#| output-location: fragment

import pandas as pd

df = pd.read_csv('../datasets/drugs.csv')

print(df.head())
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### JSON: JavaScript Object Notation

::: {.fragment}
<img src = "images/json.png" width = "50%">
:::

::: {.fragment}
```{python}
#| output-location: fragment

import json

data = dict()

with open('../datasets/test.json') as f:
    data=json.load(f)

data
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Plain Text

::: {.fragment}
<img src = "images/txt.png">
:::

::: {.fragment}
```{python}
#| output-location: fragment

with open('../datasets/test.txt') as f:
    lines = f.readlines(1000)

lines
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### HTML

::: {.r-stack}
![](images/black_friday_website.png){.fragment .fade-out}

![](images/black_friday_html.png){.fragment width=80%}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### HTML

::: {.fragment}
```{python}
#| output-location: fragment

from bs4 import BeautifulSoup

with open('../datasets/test.html') as f:
    soup = BeautifulSoup(f, 'html.parser')

print(soup.prettify())
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Collecting Data {.title-slide}

---

### Where do(es) data come from?

- Then: Manual Curation
- Now: Automatic Curation

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Then: Manual Curation (I)

If your parents have not taken note, anywhere, of how tall you were at the age of 1 - we may never be able to extract this information.

<img src = "images/height_marks.jpg" width = "70%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Then: Manual Curation (II)

If the US government had not seen fit to estimate and record the level of alcohol consumption of its citizens, we would never have known.

<img src = "images/alcohol-consumption-per-person-us.png" width = "70%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Now: Automatic Curation (I)

Have you ever opened up an Internet browser, searched for "Amazon", clicked on [amazon.com](amazon.com) and scrolled around to check the price of a T-shirt? You don't have to be logged in. You don't have to buy. You are data.

<img src = "images/you_are_data.png">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Web Scraping

- Public APIs
- Beautiful Soup

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Public APIs (I)

You also have the power to automatically curate data, yourself.

<img src = "images/google_trends01.png">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Beautiful Soup

You may not even need an API.  The following code scrapes the Wikipedia page for the [Beatles discography](https://en.wikipedia.org/wiki/The_Beatles_discography){target="_blank"} and creates a table, out of "thin air". See more advanced examples in recitation.

```{python}
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd

url = 'https://en.wikipedia.org/wiki/The_Beatles_discography'
r = requests.get(url)
soup = BeautifulSoup(r.content, 'html.parser')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
def get_release_details(release_col):
    release_date = None
    release_label = None
    if release_col is not None:
        release_list = release_col.find('ul')
        if release_list is not None:
            release_list_elements = release_list.find_all('li')
            for element in release_list_elements:
                element_text = element.get_text()
                if element_text.startswith('Released: '):
                    release_date = re.search('Released: ([0-9a-zA-Z ]+)',\
                                             element_text).group(1)
                if element_text.startswith('Label: '):
                    release_label = re.search('Label: ([0-9a-zA-Z,\(\) ]+)', \
                                              element_text).group(1)
    return release_date, release_label
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
albums = dict()
id = 0
albums[id] = dict()
tables = soup.find_all('table')
for table in tables:
    caption = table.find('caption')
    if caption is not None:
        header = caption.get_text()
        if re.match(re.compile('^List of(.+?)albums'), header):
            rows = table.find_all('tr')
            for row in rows:
                title_col = row.find('th')
                if title_col is not None and 'scope' in title_col.attrs and\
                title_col.attrs['scope'] == 'row':            
                    title_cell = title_col.find('a')
                    if title_cell is not None and title_cell.attrs is not None and\
                    'title' in title_cell.attrs:
                        albums[id]['name'] = title_cell.attrs['title']
                        release_col = row.find('td')
                        release_date, release_label = get_release_details(release_col)
                        if release_date is not None or release_label is not None:
                            albums[id]['release_date'] = release_date
                            albums[id]['release_label'] = release_label
                            id += 1
                            albums[id] = dict()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
albums_df = pd.DataFrame.from_dict(albums, orient ='index')
albums_df.head(5)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Small Data, Big Data {.title-slide}

---

### What's in a name?

::: {.incremental}
These definitions are constantly changing.

* "Everything processed in Excel is small data." ([Rufus Pollock, The Guardian](https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution))
* "[Big Data] is data so large it does not fit in main memory" (Leskovec et al., Mining of Massive Datasets)

Or maybe we should define the size of our data according how easy it is to process and understand it?


* "[Small Data is] data that has small enough size for human comprehension." ([jWork.ORG](jWork.ORG))
* "data sets that are too large or complex for traditional data-processing application software to adequately deal with" ([Wikipedia](https://en.wikipedia.org/wiki/Big_data))
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What's in a name? (II)

::: {.incremental}
The actual definition should probably merge both of the above.

* Excel can fit 1M rows, 16K columns of double numbers. Try loading a matrix such as this into Matlab, Python or R, and invert it - you can't. So isn't that Big?
* Facebook generates 4 Petabytes of data, daily. That's 4K Terabytes or 4M Gigabytes. ([Brandwatch.com](https://www.brandwatch.com/blog/47-facebook-statistics/)) But a Facebook Data Scientist in daily life typically needs only a copy of some of these data, which fits in her PC. Isn't that small?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Web data is Big Data

We can all agree *this* is big: ([Domo.com](https://www.domo.com/data-never-sleeps))
    
<img src = "images/domo.png" width = "40%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Exploring Data: Basic Plots {.title-slide}

---

### Boxplot

```{python}
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sns.set()
X = np.random.chisquare(5, 1000)
```

```{python}
sns.boxplot(y = X)
plt.ylabel('X')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Swarmplot

```{python}
sns.swarmplot(y = X)
plt.ylabel('X')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Histogram

```{python}
sns.distplot(X, kde = False)
plt.ylabel('Frequency')
plt.xlabel('X')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Histogram

```{python}
sns.distplot(X, hist = False)
plt.ylabel('Density')
plt.xlabel('X')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Density plot: kernel density estimation / convolution

::: {.fragment}
The way to get a smooth estimate of the distribution in density plot is by defining a kernel which "smoothes" the data. Mathematically we define a kernel weight function $w: \mathbb{R} \to \mathbb{R}^+$ as:

::: {.incremental}
1. Non-negative and symmetric: $w(x) = w(-x)$
2. Integrates to 1: $\int_{\mathbb{R}} w(x)dx = 1$
:::
:::

::: {.fragment}
And then the density kernel estimate is: $J(x) = \frac{1}{n} \cdot \sum_{i=1}^n w(x_i - x)$.
:::

::: {.fragment}
Nice property: $\int_{\mathbb{R}} J(x)dx = 1$

- Wide $w$: smooth estimate, but it may not reflect the real data
- Narrow $w$: very non-smooth description

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

For example smoothing this same dataset with too narrow or too wide window:

```{python}
#| code-fold: true

plt.figure(figsize =(14, 5))
plt.subplot(1, 3, 1)
sns.kdeplot(X, bw=0.1)
plt.ylabel('Density')
plt.xlabel('X')
plt.title('Width=0.1: Too narrow')
plt.subplot(1, 3, 2)
sns.kdeplot(X, bw=1)
plt.ylabel('Density')
plt.xlabel('X')
plt.title('Width=1: About right')
plt.subplot(1, 3, 3)
sns.kdeplot(X, bw=10)
plt.ylabel('Density')
plt.xlabel('X')
plt.title('Width=10: Too wide')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Scatterplot

```{python}
b0 = 2
b1 = 3
Y = X * b1 + b0 + np.random.normal(0, 10, 1000)
sns.scatterplot(X, Y)
plt.ylabel('Density')
plt.xlabel('X')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What can we learn from simple plots?

::: {.fragment}
Look at outliers:

![](images/Outliers.jpg){width=30%}
:::

::: {.fragment}
See the shape and tail direction:

![Age at heart attack (left) and cost of hospitalization (right)](images/Skew.jpg){width=50%}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Exploring Data: Summary Statistics {.title-slide}

---

### Location

"Where is this X located? Where is the central mass?"

- Mean  of empirical distribution (=average): 
$$Mean(X) = \frac{1}{N}\sum\limits_{i=1}^N X_i$$
- Median:
$$Med(X) = m\space s.t. \space P(X \leq m) = P(X \geq m) = 0.5$$
- Mode:
$$Mode(X) = Most \space frequent \space value \space in \space a \space dataset$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

```{python}
mean = np.mean(X)
median = np.median(X)
hist, _ = np.histogram(X, bins=range(20))
mode = list(range(20))[hist.argsort()[::-1][0]]
sns.distplot(X, bins = range(20), kde = False)
plt.plot([mean, mean], [0, 160], linewidth=2, color='r')
plt.plot([median, median], [0, 160], linewidth=2, color='g')
plt.plot([mode, mode], [0, 160], linewidth=2, color='b')
plt.legend({'Mean':mean,'Median':median,'Mode':mode})
plt.xlabel('X')
plt.ylabel('Frequency')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Dispersion

"Is X widely spread out? Does it concentrate narrowly around the mean?"

- Quantiles/Percentiles:
$$Q(X, q) = v\space s.t. \space P(X \leq v) = 1-P(X \geq v) = q$$
- Range:
$$Range(X) = Max(X) - Min(X)$$
- Inter-Quartile-Range:
$$IQR(X) = Q(X, 0.75) - Q(X, 0.25)$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Dispersion

- (Empirical) Variance:
$$Var(X) = \frac{1}{N}\sum\limits_{i=1}^N (X_i - Mean(X))^2$$
- Standard Deviation:
$$STD(X) = \sqrt{Var(X)}$$

::: {.fragment}
```{python}
print(f'90th percentile: {np.percentile(X, 90) :.2f}')
print(f'Range: {np.max(X) - np.min(X) :.2f}')
print(f'IQR: {np.percentile(X, 75) - np.percentile(X, 25) :.2f}')
print(f'Variance: {np.var(X) :.2f}')
print(f'Standard Deviation: {np.std(X) :.2f}')
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Shape

"Is X symmetric or not? How 'tailed' is it?"

- Skewness:
$$Skew(X) = \frac{1}{N}\frac{\sum\limits_{i=1}^N (X_i - Mean(X))^3}{STD(X)^3}$$

::: {.fragment}
```{python}
from scipy import stats

print(f'Skewness: {stats.skew(X) :.2f}')
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Advanced Visualization {.title-slide}

---

### Minard's Napoleon March

<img src = "images/minard.png">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Heatmaps

[source](https://towardsdatascience.com/exploring-infections-through-data-interactive-visualisation-of-measles-in-the-usa-6ae8d5949538)

<img src = "images/heatmap.png">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Spotify: Total Eclipse of the Heart

[source](https://insights.spotify.com/us/2017/08/24/data-viz-total-eclipse-of-the-heart-follows-the-eclipse/)

<img src = "images/total_eclipse.gif">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Chernoff Faces

[source](https://www.axios.com/the-emoji-states-of-america-1513302318-0ca61705-de75-4c8f-8521-5cbab12a45f2.html)

<img src = "images/chernoff.png">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge plot (a.k.a Joy plot)

<img src = "images/ridge.png" width = "80%" height = "80%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Flowing Data: A Day in the Life of Americans

[source](https://flowingdata.com/2015/12/15/a-day-in-the-life-of-americans/)

<iframe width="800" height="500" src="https://www.youtube.com/embed/k88d_fn3G-I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Pudding: Women's Pockets (and every single post on their site!)

[source](https://pudding.cool/2018/08/pockets/)

<img src ="images/pockets.png" width = "60%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Gapminder story

The history of the world encapsulated in a simple visualization:

[source](https://www.gapminder.org/world)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Dangers of Dirty Data {.title-slide}

---

### What could be dirty about data?

- The data itself
- The data's structure

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The data itself: Outliers

::: {.fragment}
- Numerical Outliers: This is a histogram of random ~2.3 million transactions on ebay US website in over a few weeks in 2013 ([source](https://users.soe.ucsc.edu/~draper/Reading-2015-Day-5.html)):

<img src = "images/ebay_dist.png" width="35%">

:::

::: {.fragment}
- Textual Outliers: The [Blog Authorship Corpus](http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf) consists of 19K posts by bloggers from blogger.com in 2004. These are actual words used in the 10-20 age group:

>aaaaaaaaaaaaaaaaaaaaaargh, lolzi, jfjgfjhgjhfjgfjf, roflmfao, duuuuuuh, walang, dunno

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The data itself: Missing data

[source](https://www.themarker.com/news/1.2593452)

<img src = "images/elections.png" width = "80%" height = "80%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The data's structure

In a word: Excel.

[source](https://medium.com/@miles.mcbain/tidying-the-australian-same-sex-marriage-postal-survey-data-with-r-5d35cea07962)

<img src = "images/messy_excel.png" width = "60%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Spreadsheet Blunder

[source](https://www.bbc.com/news/technology-54423988)

<img src = "images/covid_uk_excel.png" width = "60%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Some advice on cleaning data

::: {.fragment}
Plot first:

```{python}
tips = sns.load_dataset('tips')
sns.pairplot(tips, height=1.5)
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Some advice on cleaning data

::: {.fragment}
Apply common transformations:

Here's how ebay's 2.3 million transactions look with a log transformation:

::: {layout-ncol=2}
![](images/ebay_dist.png){width=50%}

![](images/ebay_log.png){width=50%}
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Some advice on cleaning data

Use robust statistics (an entire field in Statistics):

For example the Median is much more robust to extreme values than the mean:

<img src = "images/median_mean_simulation.gif">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Some advice on cleaning data

Tidy your data:

> Each variable is a column, each observation is a row, and each type of observational unit is a table. ([Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.html))

Untidy (wide):

<img src = "images/untidy_data.png" width="40%">

Tidy (long):

<img src = "images/tidy_data.png" width="40%">

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
